<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ravi Kalia">
<meta name="dcterms.date" content="2025-02-06">
<meta name="description" content="An investigation into Reinforcement Learning (RL), its methods, state-of-the-art techniques, and how to get started.">

<title>Reinforcement Learning – Blog Directory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-9923a10789e6ee87d406762b4ff79bf7.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Blog Directory</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/project-delphi"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/ravi-kalia-phd"> <i class="bi bi-linkedin" role="img" aria-label="LinkedIn">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://project-delphi.github.io"> <i class="bi bi-house-fill" role="img" aria-label="LinkedIn">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Reinforcement Learning</h1>
                  <div>
        <div class="description">
          An investigation into Reinforcement Learning (RL), its methods, state-of-the-art techniques, and how to get started.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Reinforcement Learning</div>
                <div class="quarto-category">Machine Learning</div>
                <div class="quarto-category">AI</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Ravi Kalia </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 6, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Reinforcement Learning (RL) is a branch of machine learning where an agent learns to make decisions by interacting with an environment. Unlike supervised learning, RL does not require labeled data; instead, it optimizes a reward function through exploration and exploitation. It’s lost some steam as a application and research area with Generative AI dominating these days.</p>
<p>I’ve worked on Reinforcement Learning a few times for course exercises, but never had time to really dig into. I worked on making use on Meta’s <a href="https://research.facebook.com/publications/horizon-facebooks-open-source-applied-reinforcement-learning-platform/">Horizon</a> platform for RL. The idea was to modify the UI and notifications in experiments conducted with users - that is each user receive a different version of the app, different UI and notifications. We didn’t get the go ahead to follow through from early stage investigation though.</p>
<section id="core-components-of-rl" class="level2">
<h2 class="anchored" data-anchor-id="core-components-of-rl">Core Components of RL</h2>
<p>RL problems are typically modeled as <strong>Markov Decision Processes (MDPs)</strong>, consisting of:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 48%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>State (s)</strong></td>
<td>The environment’s representation at a given time.</td>
</tr>
<tr class="even">
<td><strong>Action (a)</strong></td>
<td>The choice made by the agent.</td>
</tr>
<tr class="odd">
<td><strong>Reward (r)</strong></td>
<td>A scalar feedback signal received after taking an action.</td>
</tr>
<tr class="even">
<td><strong>Policy (π)</strong></td>
<td>A strategy that maps states to actions.</td>
</tr>
<tr class="odd">
<td><strong>Value Function (V or Q)</strong></td>
<td>Measures the expected return of being in a state or taking an action.</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="value-based-methods" class="level1">
<h1>1. Value-Based Methods</h1>
<p>Value-based methods estimate the expected return of states or actions and use it to make optimal decisions.</p>
<section id="q-learning" class="level2">
<h2 class="anchored" data-anchor-id="q-learning">Q-Learning</h2>
<p>Q-learning estimates the action-value function <strong>Q(s, a)</strong> and updates it using the Bellman equation:</p>
<p><span class="math display">\[
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\alpha\)</span> is the learning rate.</li>
<li><span class="math inline">\(\gamma\)</span> is the discount factor.</li>
<li><span class="math inline">\(s'\)</span> and <span class="math inline">\(a'\)</span> are the next state and action.</li>
</ul>
<section id="what-does-q-in-q-learning-stand-for" class="level3">
<h3 class="anchored" data-anchor-id="what-does-q-in-q-learning-stand-for">What Does “Q” in Q-Learning Stand For?</h3>
<p>The “Q” in Q-learning stands for <strong>“Quality”</strong>, representing the <strong>quality</strong> or <strong>value</strong> of taking a particular action ( a ) in a given state ( s ).</p>
<section id="definition-of-q-value" class="level5">
<h5 class="anchored" data-anchor-id="definition-of-q-value">Definition of Q-Value</h5>
<p>The Q-value function, ( Q(s, a) ), estimates the <strong>expected cumulative reward</strong> an agent will receive if it takes action ( a ) in state ( s ) and then follows an optimal policy thereafter. The function satisfies the <strong>Bellman equation</strong>:</p>
<p><span class="math display">\[
Q(s, a) = \mathbb{E} \left[ r + \gamma \max_{a'} Q(s', a') \right]
\]</span></p>
<p>where: - <span class="math inline">\(r\)</span> is the immediate reward, - <span class="math inline">\(\gamma\)</span> is the discount factor (determining how much future rewards are valued), - <span class="math inline">\(s'\)</span> is the next state, - <span class="math inline">\(a'\)</span> is the next action.</p>
</section>
<section id="q-learning-update-rule" class="level5">
<h5 class="anchored" data-anchor-id="q-learning-update-rule">Q-Learning Update Rule</h5>
<p>Q-learning iteratively updates ( Q(s, a) ) estimates using the Bellman equation:</p>
<p><span class="math display">\[
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the learning rate.</p>
</section>
<section id="optimal-policy-from-q-values" class="level5">
<h5 class="anchored" data-anchor-id="optimal-policy-from-q-values">Optimal Policy from Q-Values</h5>
<p>The optimal policy is derived by selecting the action with the highest Q-value:</p>
<p><span class="math display">\[ \pi^*(s) = \arg\max_a Q(s, a) \]</span></p>
<p>This means the agent <strong>chooses the action that maximizes the expected future reward</strong>.</p>
</section>
<section id="example-code-in-python" class="level5">
<h5 class="anchored" data-anchor-id="example-code-in-python">Example Code in Python</h5>
<p>Below is a simple Q-learning update step in Python:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize Q-table</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> np.zeros((<span class="dv">5</span>, <span class="dv">2</span>))  <span class="co"># 5 states, 2 actions</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.1</span>   <span class="co"># Learning rate</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.9</span>   <span class="co"># Discount factor</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample experience (state, action, reward, next_state)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>s, a, r, s_next <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">2</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Q-learning update</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>Q[s, a] <span class="op">=</span> Q[s, a] <span class="op">+</span> alpha <span class="op">*</span> (r <span class="op">+</span> gamma <span class="op">*</span> np.<span class="bu">max</span>(Q[s_next, :]) <span class="op">-</span> Q[s, a])</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(Q)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="example-q-learning-in-python" class="level6">
<h6 class="anchored" data-anchor-id="example-q-learning-in-python">Example: Q-Learning in Python</h6>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gym</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.99</span>  <span class="co"># Discount factor</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.1</span>   <span class="co"># Learning rate</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># Exploration rate</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"FrozenLake-v1"</span>, is_slippery<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> np.zeros((env.observation_space.n, env.action_space.n))</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> env.reset()[<span class="dv">0</span>]</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        action <span class="op">=</span> np.argmax(Q[state]) <span class="cf">if</span> np.random.rand() <span class="op">&gt;</span> epsilon <span class="cf">else</span> env.action_space.sample()</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        next_state, reward, done, _, _ <span class="op">=</span> env.step(action)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        Q[state, action] <span class="op">+=</span> alpha <span class="op">*</span> (reward <span class="op">+</span> gamma <span class="op">*</span> np.<span class="bu">max</span>(Q[next_state]) <span class="op">-</span> Q[state, action])</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> next_state</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="temporal-difference-td-learning" class="level5">
<h5 class="anchored" data-anchor-id="temporal-difference-td-learning">Temporal Difference (TD) Learning</h5>
<p>TD methods update value estimates using a one-step lookahead:</p>
<p><span class="math display">\[
V(s) \leftarrow V(s) + \alpha \left[ r + \gamma V(s') - V(s) \right]
\]</span></p>
<p>This balances immediate and future rewards without requiring a full trajectory.</p>
</section>
</section>
</section>
</section>
<section id="policy-based-methods" class="level1">
<h1>2. Policy-Based Methods</h1>
<p>Instead of learning <strong>Q(s, a)</strong>, policy-based methods directly optimize the policy <strong>π_θ(a | s)</strong>, parameterized by θ.</p>
<section id="policy-gradient-pg" class="level2">
<h2 class="anchored" data-anchor-id="policy-gradient-pg">Policy Gradient (PG)</h2>
<p>Policy gradient methods use gradient ascent to maximize expected rewards:</p>
<p><span class="math display">\[
\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)
\]</span></p>
<p>where <strong>J(θ)</strong> is the expected return under the policy <strong>π_θ</strong>.</p>
<section id="derivation-of-jtheta-in-policy-gradients" class="level3">
<h3 class="anchored" data-anchor-id="derivation-of-jtheta-in-policy-gradients">Derivation of <span class="math inline">\(J(\theta)\)</span> in Policy Gradients</h3>
<section id="defining-the-objective-function" class="level4">
<h4 class="anchored" data-anchor-id="defining-the-objective-function">1. Defining the Objective Function</h4>
<p>In <strong>policy gradient methods</strong>, the objective function <span class="math inline">\(J(\theta)\)</span> represents the <strong>expected return</strong> (total reward) under the parameterized policy <span class="math inline">\(\pi_{\theta}\)</span>. It is derived from the expected cumulative reward an agent receives when following <span class="math inline">\(\pi_{\theta}\)</span>.</p>
<p>We define <span class="math inline">\(J(\theta)\)</span> as the expected return over all possible trajectories:</p>
<p><span class="math display">\[
J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} \left[ R(\tau) \right]
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\tau = (s_0, a_0, s_1, a_1, \dots)\)</span> is a trajectory (a full sequence of states and actions),</li>
<li><span class="math inline">\(p_{\theta}(\tau)\)</span> is the probability of a trajectory under policy <span class="math inline">\(\pi_{\theta}\)</span>,</li>
<li><span class="math inline">\(R(\tau)\)</span> is the total reward for a trajectory, usually defined as:</li>
</ul>
<p><span class="math display">\[
R(\tau) = \sum_{t=0}^{T} \gamma^t r_t
\]</span></p>
<p>where <span class="math inline">\(r_t\)</span> is the reward at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(\gamma\)</span> is the discount factor.</p>
<p>Using the chain rule, the trajectory probability can be written as:</p>
<p><span class="math display">\[ p_{\theta}(\tau) = p(s_0) \prod_{t=0}^{T} \pi_{\theta}(a_t | s_t) p(s_{t+1} | s_t, a_t) \]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(p(s_0)\)</span> is the initial state distribution,</li>
<li><span class="math inline">\(\pi_{\theta}(a_t | s_t)\)</span> is the policy,</li>
<li><span class="math inline">\(p(s_{t+1} | s_t, a_t)\)</span> is the environment transition probability.</li>
</ul>
</section>
<section id="policy-gradient-theorem" class="level4">
<h4 class="anchored" data-anchor-id="policy-gradient-theorem">2. Policy Gradient Theorem</h4>
<p>We take the gradient of <span class="math inline">\(J(\theta)\)</span> to optimize the policy:</p>
<p><span class="math display">\[
\nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} \left[ R(\tau) \nabla_{\theta} \log p_{\theta}(\tau) \right]
\]</span></p>
<p>Using the log-derivative trick:</p>
<p><span class="math display">\[
\nabla_{\theta} \log p_{\theta}(\tau) = \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t)
\]</span></p>
<p>we obtain the final form:</p>
<p><span class="math display">\[ \nabla_{\theta} J(\theta) = \mathbb{E}_{\tau} \left[ R(\tau) \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \right] \]</span></p>
<p>This means that <strong>we update the policy in the direction that increases the probability of high-reward actions</strong>.</p>
</section>
<section id="practical-policy-gradient-update-rule" class="level4">
<h4 class="anchored" data-anchor-id="practical-policy-gradient-update-rule">3. Practical Policy Gradient Update Rule</h4>
<p>The policy parameters are updated using stochastic gradient ascent:</p>
<p><span class="math display">\[
\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\theta)
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the learning rate.</p>
</section>
<section id="example-code-in-python-reinforce-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="example-code-in-python-reinforce-algorithm">4. Example Code in Python (REINFORCE Algorithm)</h4>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple policy network</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PolicyNetwork(nn.Module):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, state_dim, action_dim):</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(state_dim, action_dim)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.softmax <span class="op">=</span> nn.Softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, state):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.softmax(<span class="va">self</span>.fc(state))</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize policy, optimizer</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>policy <span class="op">=</span> PolicyNetwork(state_dim<span class="op">=</span><span class="dv">4</span>, action_dim<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(policy.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample data</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>log_probs <span class="op">=</span> []  <span class="co"># Store log π(a|s)</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>rewards <span class="op">=</span> []  <span class="co"># Store rewards</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute policy gradient loss</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> <span class="bu">sum</span>(rewards)  <span class="co"># Total return</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>policy_loss <span class="op">=</span> <span class="op">-</span><span class="bu">sum</span>(log_prob <span class="op">*</span> R <span class="cf">for</span> log_prob <span class="kw">in</span> log_probs)  <span class="co"># Gradient ascent</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Backpropagate</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>optimizer.zero_grad()</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>policy_loss.backward()</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>optimizer.step()</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a><span class="co">### Example: Policy Gradient with REINFORCE</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>```python</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gym</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"CartPole-v1"</span>)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>policy <span class="op">=</span> nn.Sequential(nn.Linear(<span class="dv">4</span>, <span class="dv">128</span>), nn.ReLU(), nn.Linear(<span class="dv">128</span>, <span class="dv">2</span>), nn.Softmax(dim<span class="op">=-</span><span class="dv">1</span>))</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(policy.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> policy_gradient():</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> env.reset()[<span class="dv">0</span>]</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>    rewards, log_probs <span class="op">=</span> [], []</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>    done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> torch.tensor(state, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>        action_probs <span class="op">=</span> policy(state)</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>        action <span class="op">=</span> torch.multinomial(action_probs, <span class="dv">1</span>).item()</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>        log_prob <span class="op">=</span> torch.log(action_probs[action])</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>        state, reward, done, _, _ <span class="op">=</span> env.step(action)</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>        rewards.append(reward)</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>        log_probs.append(log_prob)</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> rewards, log_probs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
</section>
<section id="actor-critic" class="level2">
<h2 class="anchored" data-anchor-id="actor-critic">Actor-Critic</h2>
<p>Actor-Critic methods combine value-based and policy-based approaches: - <strong>Actor</strong> updates the policy <strong>π_θ</strong>. - <strong>Critic</strong> estimates the value function <strong>V(s)</strong> to reduce variance.</p>
</section>
</section>
<section id="state-of-the-art-rl-methods" class="level1">
<h1>3. State-of-the-Art RL Methods</h1>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 22%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>DQN (Deep Q-Networks)</strong></td>
<td>Value-Based</td>
<td>Uses deep learning to approximate Q-values.</td>
</tr>
<tr class="even">
<td><strong>PPO (Proximal Policy Optimization)</strong></td>
<td>Policy-Based</td>
<td>A stable policy optimization algorithm.</td>
</tr>
<tr class="odd">
<td><strong>SAC (Soft Actor-Critic)</strong></td>
<td>Actor-Critic</td>
<td>Introduces entropy regularization for exploration.</td>
</tr>
<tr class="even">
<td><strong>MuZero</strong></td>
<td>Model-Based</td>
<td>Learns a model of the environment while optimizing actions.</td>
</tr>
</tbody>
</table>
</section>
<section id="learning-path-for-rl" class="level1">
<h1>4. Learning Path for RL</h1>
<section id="estimated-time-to-competency" class="level3">
<h3 class="anchored" data-anchor-id="estimated-time-to-competency">Estimated Time to Competency</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 33%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Stage</th>
<th>Topics</th>
<th>Duration</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Foundations</strong></td>
<td>MDPs, Q-learning, TD learning</td>
<td>1-2 months</td>
</tr>
<tr class="even">
<td><strong>Deep RL</strong></td>
<td>DQN, PPO, Actor-Critic</td>
<td>2-3 months</td>
</tr>
<tr class="odd">
<td><strong>Advanced Topics</strong></td>
<td>Multi-Agent RL, Meta-RL</td>
<td>3-4 months</td>
</tr>
<tr class="even">
<td><strong>Research &amp; Specialization</strong></td>
<td>Real-world applications, new RL methods</td>
<td>Ongoing</td>
</tr>
</tbody>
</table>
</section>
<section id="key-resources" class="level3">
<h3 class="anchored" data-anchor-id="key-resources">Key Resources</h3>
<ul>
<li><strong>Books</strong>: Sutton &amp; Barto’s <em>Reinforcement Learning: An Introduction</em></li>
<li><strong>Courses</strong>: David Silver’s RL course</li>
<li><strong>Libraries</strong>: Stable-Baselines3, RLlib, Gym</li>
</ul>
</section>
</section>
<section id="criticisms-of-rl" class="level1">
<h1>Criticisms of RL</h1>
<p>Yann LeCun has been critical of reinforcement learning (RL) as a general approach to intelligence. Some key points from his statements include: 1. Inefficiency of RL • He often points out that RL is sample inefficient, requiring millions or even billions of interactions to learn effective policies. This makes it impractical for real-world applications outside of simulated environments. 2. Limited Applicability • LeCun argues that RL is useful in specific cases, such as game playing (e.g., AlphaGo, Atari games) or robotic control, but it does not scale well to more general AI tasks. 3. Need for More Efficient Learning • He believes that future AI should rely more on self-supervised learning (SSL) rather than RL. SSL allows models to learn from vast amounts of unlabeled data efficiently, whereas RL often relies on sparse rewards. 4. RL and Common Sense • LeCun has stated that RL alone cannot give AI systems “common sense” because it lacks the ability to learn structured world models. He advocates for model-based approaches that integrate predictive learning. 5. Hybrid Approaches • While critical of RL’s limitations, he acknowledges that combining RL with other paradigms, such as self-supervised learning and energy-based models, may be key to building more intelligent systems.</p>
<p>A famous quote from LeCun:</p>
<pre><code>&gt; “Reinforcement learning is like training a dog with a clicker: it takes forever, and the dog never learns to play chess.”</code></pre>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Reinforcement learning is a powerful framework for sequential decision-making. Mastering it requires understanding both theory and practical implementation.</p>
<hr>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next Steps</h2>
<p>If and when I get time, I would like to read more about <strong>Meta-RL</strong> and explore its applications. It seems like a fascinating area, although less popular these days, with generative models taking over. (Although RLHF on Large Language Models is a special application of RL).</p>
<p>Here are some canonical papers that have shaped the field of reinforcement learning (RL) and are often considered foundational:</p>
<ol type="1">
<li><p>Temporal Difference Learning (TD) • Title: Learning to Predict by the Methods of Temporal Differences • Authors: Richard S. Sutton • Year: 1988 • Summary: This paper introduces Temporal Difference (TD) learning, a core method that combines Monte Carlo methods and dynamic programming to estimate value functions for reinforcement learning tasks.</p></li>
<li><p>Q-Learning • Title: Learning from Delayed Rewards • Authors: Christopher J.C.H. Watkins • Year: 1989 • Summary: Q-learning is introduced in this work, a model-free algorithm for learning the value of action-state pairs without needing a model of the environment. It’s a foundational algorithm for value-based RL methods.</p></li>
<li><p>Policy Gradient Methods • Title: Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning • Authors: Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour • Year: 2000 • Summary: This paper introduces policy gradient methods, which optimize policies directly by updating the policy parameters using the gradient of expected return, paving the way for better solutions in continuous action spaces.</p></li>
<li><p>Deep Q-Networks (DQN) • Title: Human-level control through deep reinforcement learning • Authors: Volodymyr Mnih, Koray Kavukcuoglu, David Silver, et al. • Year: 2015 • Summary: DQN combines deep learning with Q-learning, using a neural network to approximate the Q-value function. This paper demonstrated human-level performance on several Atari games and marked a significant breakthrough for deep RL.</p></li>
<li><p>Asynchronous Advantage Actor-Critic (A3C) • Title: Asynchronous Methods for Deep Reinforcement Learning • Authors: Volodymyr Mnih, Adrià Puigdomènech Badia, David Silver, et al. • Year: 2016 • Summary: A3C introduces a parallelized framework where multiple agents interact with the environment asynchronously, using actor-critic methods to stabilize learning and improve performance on complex tasks.</p></li>
<li><p>Proximal Policy Optimization (PPO) • Title: Proximal Policy Optimization Algorithms • Authors: John Schulman, Filip Wolski, Prafulla Dhariwal, et al. • Year: 2017 • Summary: PPO is introduced as an efficient and simpler alternative to TRPO (Trust Region Policy Optimization). It aims to balance exploration and exploitation by constraining how much the policy changes at each step, leading to stable training.</p></li>
<li><p>AlphaGo • Title: Mastering the game of Go with deep neural networks and tree search • Authors: Silver, Hubert, Schrittwieser, et al. • Year: 2016 • Summary: This paper introduces AlphaGo, a system that combines deep learning, Monte Carlo tree search, and reinforcement learning to play Go at a superhuman level. It demonstrates the power of RL in combining planning and learning.</p></li>
<li><p>MuZero • Title: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model • Authors: Silver, Singh, Precup, et al. • Year: 2020 • Summary: MuZero is a model-based approach that learns a model of the environment (dynamics) and uses it to plan actions. Unlike prior methods, MuZero learns the model itself rather than relying on a predefined one, achieving state-of-the-art performance in multiple domains.</p></li>
<li><p>AlphaStar • Title: Grandmaster level in StarCraft II using multi-agent reinforcement learning • Authors: Vinyals, Babuschkin, et al. • Year: 2019 • Summary: AlphaStar is an RL agent trained to play the complex real-time strategy game StarCraft II. This paper outlines the challenges and methods used to train AlphaStar, including multi-agent learning and hierarchical reinforcement learning.</p></li>
<li><p>Model-Based Reinforcement Learning • Title: World Models • Authors: David Ha, Jürgen Schmidhuber • Year: 2018 • Summary: World Models are introduced here, where an RL agent learns an internal model of the environment that allows it to perform well even with a limited number of real-world interactions, showing how model-based methods can significantly improve sample efficiency.</p></li>
<li><p>Trust Region Policy Optimization (TRPO) • Title: Trust Region Policy Optimization • Authors: John Schulman, Sergey Levine, Philipp Moritz, et al. • Year: 2015 • Summary: TRPO addresses the challenge of large policy updates by introducing a constraint on the size of each update, thus ensuring the policy remains close to the old policy, which improves the stability of training in policy-based methods.</p></li>
<li><p>A Survey of Deep Reinforcement Learning • Title: Deep Reinforcement Learning: An Overview • Authors: Yuxi Li • Year: 2017 • Summary: This is a comprehensive review paper on deep reinforcement learning, covering the evolution of deep RL algorithms, key ideas, and the challenges they aim to solve. It provides a useful resource for understanding the broad scope of deep RL research.</p></li>
</ol>
<p>These papers cover the evolution of RL, from value-based methods like Q-learning to policy-based methods, and the integration of deep learning into RL, all the way to model-based approaches like MuZero. Together, they form the backbone of modern RL research.</p>
<p><em>Have thoughts or questions? Contact me</em></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions"><ul><li><a href="https://github.com/project-delphi/ml-blog/edit/main/posts/reinforcement-learning/index.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div></div></footer></body></html>