[
  {
    "objectID": "posts/train-dev-test-splits/index.html",
    "href": "posts/train-dev-test-splits/index.html",
    "title": "Train Dev Test Data Splits",
    "section": "",
    "text": "Made with ❤️ and GitHub Copilot"
  },
  {
    "objectID": "posts/train-dev-test-splits/index.html#introduction",
    "href": "posts/train-dev-test-splits/index.html#introduction",
    "title": "Train Dev Test Data Splits",
    "section": "Introduction",
    "text": "Introduction\nFor good machine learning, selecting the right model and properly splitting your data are crucial steps for success. We explore the process of model selection, drawing parallels between machine learning models and biological speciation, and delves into the details of data splitting and hyperparameter tuning."
  },
  {
    "objectID": "posts/train-dev-test-splits/index.html#model-selection",
    "href": "posts/train-dev-test-splits/index.html#model-selection",
    "title": "Train Dev Test Data Splits",
    "section": "Model Selection",
    "text": "Model Selection\nTo understand model selection better, let’s recap an analogy to biological evolution:\n\nModels are like different species\nData represents the environment\nThe score function (sometimes called evaluation metric or just metric depending on framework) is akin to survival fitness\n\n\nThe Learning Process\n\nModels start in an “newborn state” - they begin with random or preset parameters.\nThey mature and learn by adjusting their parameters based on the training data and available compute resources.\nThe learning process follows a predefined path, often using first-order approximations (like Taylor series) and techniques such as stochastic gradient descent (SGD).\nThe goal is to optimize an appropriate loss function, which guides the model’s adaptation to the data.\n\n\n\nHyperparameters\nIf models are species, then different hyperparameter configurations can be thought of as subspecies:\n\nEvaluation depends on the score function, similar to how a species’ success depends on its ability to survive in a specific environment.The score function is chosen to reflect the “survival task”. It could be the same as the loss function, but often is not.\nWell-chosen hyperparameters allow the model to adapt effectively to the given data and task.\nThe scoring “environment” is the validation data, and the fitness” is determined by the score function.\n\nUnlike model parameters, hyperparameters cannot be “learned” directly from the data. Instead, they are selected through a process of trial and error, guided by the model’s performance on the validation set. (The trial and error search can be random, but nowadays, more sophisticated methods like Bayesian optimization are often used.)\nNote that some frameworks and authors refer to the score function as a metric function of evaluation metric function. The term “metric” is often used in the context of classification tasks, while “loss” is used for regression tasks.\nFor a mathematician, a metric is a function that measures the distance between two points in a space, but in machine learning, it’s a function that measures the quality of a model’s predictions. Some ML metrics are indeed true metrics, but this is not always the case. This can often lead to puzzling looks from mathematicians when they first encounter machine learning terminology! For example R-squared, MAPE, Huber loss, f1, accuracy, precision, recall, AUC, etc. are all metrics in machine learning, but not in the strict mathematical sense. More on this in a later post."
  },
  {
    "objectID": "posts/train-dev-test-splits/index.html#the-importance-of-data-splitting",
    "href": "posts/train-dev-test-splits/index.html#the-importance-of-data-splitting",
    "title": "Train Dev Test Data Splits",
    "section": "The Importance of Data Splitting",
    "text": "The Importance of Data Splitting\nBefore we dive deeper into model selection, it’s essential to understand the concept of data splitting. We typically divide our dataset into three parts:\n\nTraining set\nDevelopment (Validation) set\nTest set\n\n\nWhy Split the Data?\nSplitting the data serves several purposes: - The training set is used to teach the model. - The validation set helps us tune the model and select the best hyperparameters. - The test set provides an unbiased evaluation of the final model’s performance.\n\n\nFactors Affecting Data Splitting\nThe way we split our data can significantly impact our model’s performance. Let’s explore how this split changes based on various factors:\n\nAmount of Data\nThe number of examples (n) in the dataset influences the data split. We’ll split by the following (somewhat arbitrary) guidelines:\n\nSmall (n &lt; 1,000): Allocate a larger proportion to the training set.\nMedium (1,000 &lt; n &lt; 100,000): More balanced allocation.\nLarge (n &gt; 100,000): Can maintain large training sets while also having substantial validation and test sets.\n\nIf the n is 10k or fewer, a widely used practice is combine the train and dev set once a model is selected and retrain on the combined dataset before evaluating on the test set. Sometimes this is called the refit step or refit dataset.\nThis is less important when the number of examples is large, as additional learning on a usually small validation set is unlikely to improve performance much. Again, this all depends on the nature of the data, it’s number of features, richness, noise and algorithm.\n\n\nNoise in the Data\n\nLow noise: Fewer examples needed in validation and test sets.\nHigh noise: Larger validation and test sets to average out the noise.\n\n\n\nRichness of Structure in the Data\n\nSimple structure: Smaller training sets, larger validation and test sets.\nComplex structure: Larger training sets to learn intricate patterns.\n\n\n\n\nData Split Table\nHere’s a table suggesting possible splits for different dataset sizes, assuming moderate noise and complexity:\n\n\n\n\n\n\n\n\n\n\nDataset Size (n)\nTraining Set\nValidation Set\nTest Set\nExample Models/Datasets\n\n\n\n\n100\n70-80%\n10-15%\n10-15%\nSmall custom datasets, toy problems\n\n\n1,000\n70-75%\n15-20%\n10-15%\nIris dataset, small NLP tasks\n\n\n10,000\n70-75%\n15-20%\n10-15%\nMNIST, small to medium Kaggle competitions\n\n\n100,000\n70-80%\n10-15%\n10-15%\nCIFAR-100, medium-sized NLP tasks\n\n\n1 million\n80-85%\n5-10%\n5-10%\nImageNet, large NLP datasets\n\n\n1 billion\n90-95%\n2.5-5%\n2.5-5%\nVery large language models, recommendation systems\n\n\n\n\n\nExamples of Well-Known Models and Their Data Splits\n\nMNIST (70,000 images)\n\nTraining: 60,000 (85.7%)\nTest: 10,000 (14.3%)\nNote: Often, users create their own validation set from the training data.\n\nImageNet (1.2 million images)\n\nTraining: 1,281,167 (85.4%)\nValidation: 50,000 (3.3%)\nTest: 100,000 (6.7%)\n\nBERT (BookCorpus + English Wikipedia, ~3.3 billion words)\n\nUsed a 90-10 split for pre-training and fine-tuning\nExact validation and test set sizes vary by downstream task\n\nGPT-3 (Trained on 300 billion tokens)\n\nTraining: Vast majority of the data\nTest: Varies by task, but typically small (&lt; 1%)\nNote: Uses few-shot learning, so traditional splits are less applicable\n\nNetflix Prize Dataset (~100 million ratings)\n\nTraining: 98,074,901 (98.1%)\nTest: 1,408,395 (1.4%)\nProbe set (public test): 1,408,395 (1.4%)"
  },
  {
    "objectID": "posts/train-dev-test-splits/index.html#implementing-data-splitting-in-popular-libraries",
    "href": "posts/train-dev-test-splits/index.html#implementing-data-splitting-in-popular-libraries",
    "title": "Train Dev Test Data Splits",
    "section": "Implementing Data Splitting in Popular Libraries",
    "text": "Implementing Data Splitting in Popular Libraries\nLet’s look at how to implement data splitting in three popular machine learning libraries: scikit-learn, Keras, and PyTorch.\n\n\nCode\n# install dependencies\n!pip --quiet install datasets tensorflow pandas scikit-learn gdown optuna\n\n\nWe’ll use sentiment140 as our example data, hosted on huggingface by the stanfordnlp group, a dataset of tweets labeled as positive or negative, for our examples. The dataset contains 1.6 million tweets.\nThe dataset comes pre-split into training and test sets, so we will be a bit contrived and first combine the data splits, before using the combined data to demonstrate the splitting process with some popular libraries.\n\n\nCode\nimport os\n\nimport pandas as pd\nfrom datasets import concatenate_datasets, load_dataset\n\n# Set the environment variable to disable the prompt\nos.environ['HF_DATASETS_OFFLINE'] = '1'\n\n# Load the train and test splits\ntrain_dataset = load_dataset('sentiment140', split='train')\ntest_dataset = load_dataset('sentiment140', split='test')\n\n# Concatenate the splits\ncombined_dataset = concatenate_datasets([train_dataset, test_dataset])\n\n# Shuffle the combined dataset\nshuffled_dataset = combined_dataset.shuffle(seed=42)\n\n# Convert to a DataFrame\ndf = shuffled_dataset.to_pandas()\n\n# Separate features (X) and target (y)\nX = df.drop(columns=['sentiment'])\ny = df['sentiment']\n\n# Print the first 5 examples\nprint(X.head())\nprint(X.shape)\nprint(y.head())\nprint(y.shape)\n\n\n                                                text  \\\n0   External HDD crashed !!! Volumes of data lost      \n1  @R33S Are you going to her concert in Sydney? ...   \n2  not as good at super smash bros 64 as I remember    \n3  I want a convertible. A nice black one - hard ...   \n4  nooooo! why isn't Public Enemies being release...   \n\n                           date      user     query  \n0  Sun May 31 10:10:40 PDT 2009  twishmay  NO_QUERY  \n1  Sun May 17 01:20:02 PDT 2009    iB3nji  NO_QUERY  \n2  Fri Jun 05 23:48:43 PDT 2009    bendur  NO_QUERY  \n3  Sat May 30 11:46:52 PDT 2009      ihug  NO_QUERY  \n4  Wed Jun 24 23:16:39 PDT 2009   jssavvy  NO_QUERY  \n(1600498, 4)\n0    0\n1    4\n2    0\n3    4\n4    0\nName: sentiment, dtype: int32\n(1600498,)\n\n\nGiven this data size, of roughly 1.6 million tweets, we will use the following splits:\n\n\n\nSplit\nPercentage\nNumber of Examples\n\n\n\n\nTrain\n80%\n1,280,398\n\n\nDev\n10%\n160,050\n\n\nTest\n10%\n160,050\n\n\n\nThis split works with the rule of thumb and is a common practice in the industry. Because of the noise in the data, we want to ensure that we have enough examples in the validation and test sets to get a good estimate of the model’s performance, so we could have increased the size of the validation and test sets. However, for the sake of simplicity, we will stick with this split.\n\nScikit-learn\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into train and temporary sets (80% train, 20% temp)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Split the temporary set into validation (dev) and test sets (50% dev, 50% test of the remaining 20%)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Calculate the number of examples\ntotal_examples = len(X)\ntrain_examples = len(X_train)\nval_examples = len(X_val)\ntest_examples = len(X_test)\n\n# Calculate the percentages\ntrain_percentage = (train_examples / total_examples) * 100\nval_percentage = (val_examples / total_examples) * 100\ntest_percentage = (test_examples / total_examples) * 100\n\n# Create a DataFrame to store the split information\nsplit_info = pd.DataFrame({\n    'Split': ['Train', 'Dev', 'Test'],\n    'Percentage': [f'{train_percentage:.2f}%', f'{val_percentage:.2f}%', f'{test_percentage:.2f}%'],\n    'Number of Examples': [train_examples, val_examples, test_examples]\n})\n\n# Print the DataFrame\nprint(split_info)\n\n\n   Split Percentage  Number of Examples\n0  Train     80.00%             1280398\n1    Dev     10.00%              160050\n2   Test     10.00%              160050\n\n\n\n\nKeras\nLet’s do it in Keras now. We’ll use a different dataset - Fashion-MNIST.\nThis is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST.\n\n\nCode\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.datasets import fashion_mnist\n\n# Load the Fashion MNIST dataset\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n\n# Split the full training set into train and dev sets (80% train, 20% dev)\nX_train, X_dev, y_train, y_dev = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n\n# Calculate the number of examples\ntotal_examples = len(X_train_full) + len(X_test)\ntrain_examples = len(X_train)\ndev_examples = len(X_dev)\ntest_examples = len(X_test)\n\n# Calculate the percentages\ntrain_percentage = (train_examples / total_examples) * 100\ndev_percentage = (dev_examples / total_examples) * 100\ntest_percentage = (test_examples / total_examples) * 100\n\n# Create a DataFrame to store the split information\nsplit_info = pd.DataFrame({\n    'Split': ['Train', 'Dev', 'Test'],\n    'Percentage': [f'{train_percentage:.2f}%', f'{dev_percentage:.2f}%', f'{test_percentage:.2f}%'],\n    'Number of Examples': [train_examples, dev_examples, test_examples]\n})\n\n# Print the DataFrame\nprint(split_info)\n\n# Print the shapes of the splits\nprint(f\"Train set shape: {X_train.shape}, {y_train.shape}\")\nprint(f\"Dev set shape: {X_dev.shape}, {y_dev.shape}\")\nprint(f\"Test set shape: {X_test.shape}, {y_test.shape}\")\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 ━━━━━━━━━━━━━━━━━━━━ 0s 2us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 ━━━━━━━━━━━━━━━━━━━━ 2s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 ━━━━━━━━━━━━━━━━━━━━ 0s 1us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 ━━━━━━━━━━━━━━━━━━━━ 5s 1us/step\n   Split Percentage  Number of Examples\n0  Train     68.57%               48000\n1    Dev     17.14%               12000\n2   Test     14.29%               10000\nTrain set shape: (48000, 28, 28), (48000,)\nDev set shape: (12000, 28, 28), (12000,)\nTest set shape: (10000, 28, 28), (10000,)\n\n\n\n\nPyTorch\n\n\nCode\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\n\n# Define the transform (resizing, converting to tensor, normalization)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize all images to 224x224\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Download and load the Caltech-101 dataset\ndataset = datasets.Caltech101(root='data', download=True, transform=transform)\n\n# Split dataset into train (80%), dev (10%), and test (10%)\ntrain_size = int(0.8 * len(dataset))\ndev_size = int(0.1 * len(dataset))\ntest_size = len(dataset) - train_size - dev_size\n\ntrain_set, dev_set, test_set = random_split(dataset, [train_size, dev_size, test_size])\n\n# Create DataLoaders for each set\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n\nfor images, labels in train_loader:\n    print(images.shape)\n    print(labels)\n    break \n\n\nFiles already downloaded and verified\ntorch.Size([32, 3, 224, 224])\ntensor([37, 76, 82,  3,  3, 87,  0, 55,  5,  2,  2,  5, 79, 94, 74, 14, 78,  3,\n         3, 86, 54,  5, 68, 28,  3,  0, 21, 61, 85, 31, 56,  0])\n\n\n\n\nHugging Face\n\n\nCode\nimport pandas as pd\nfrom datasets import DatasetDict, load_dataset\nfrom sklearn.model_selection import train_test_split\n\n# Load the emotion dataset\ndataset = load_dataset('emotion')\n\n# Convert the dataset to a pandas DataFrame\ndf = pd.DataFrame(dataset['train'])\n\n# Split the dataset into train and dev sets (80% train, 20% dev)\ntrain_df, dev_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Convert the DataFrames back to Hugging Face datasets\ntrain_dataset = DatasetDict({'train': dataset['train'].select(train_df.index)})\ndev_dataset = DatasetDict({'dev': dataset['train'].select(dev_df.index)})\ntest_dataset = DatasetDict({'test': dataset['test']})\n\n# Calculate the number of examples\ntotal_examples = len(dataset['train']) + len(dataset['test'])\ntrain_examples = len(train_dataset['train'])\ndev_examples = len(dev_dataset['dev'])\ntest_examples = len(test_dataset['test'])\n\n# Calculate the percentages\ntrain_percentage = (train_examples / total_examples) * 100\ndev_percentage = (dev_examples / total_examples) * 100\ntest_percentage = (test_examples / total_examples) * 100\n\n# Create a DataFrame to store the split information\nsplit_info = pd.DataFrame({\n    'Split': ['Train', 'Dev', 'Test'],\n    'Percentage': [f'{train_percentage:.2f}%', f'{dev_percentage:.2f}%', f'{test_percentage:.2f}%'],\n    'Number of Examples': [train_examples, dev_examples, test_examples]\n})\n\n# Print the DataFrame\nprint(split_info)\n\n# Print the sizes of the splits\nprint(f\"Train set size: {train_examples}\")\nprint(f\"Dev set size: {dev_examples}\")\nprint(f\"Test set size: {test_examples}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   Split Percentage  Number of Examples\n0  Train     71.11%               12800\n1    Dev     17.78%                3200\n2   Test     11.11%                2000\nTrain set size: 12800\nDev set size: 3200\nTest set size: 2000"
  },
  {
    "objectID": "posts/train-dev-test-splits/index.html#hyperparameter-tuning-and-bayesian-optimization",
    "href": "posts/train-dev-test-splits/index.html#hyperparameter-tuning-and-bayesian-optimization",
    "title": "Train Dev Test Data Splits",
    "section": "Hyperparameter Tuning and Bayesian Optimization",
    "text": "Hyperparameter Tuning and Bayesian Optimization\nChoosing the right hyperparameters is crucial for model performance. While grid search and random search are common methods for hyperparameter tuning, Bayesian optimization has emerged as a more efficient alternative, especially for computationally expensive models.\n\nUnderstanding Bayesian Optimization\nBayesian optimization is a sequential design strategy for global optimization of black-box functions. In the context of machine learning, it’s used to find the best hyperparameters for a given model.\nKey concepts in Bayesian optimization include:\n\nSurrogate Model: A probabilistic model (often Gaussian Process) that approximates the true objective function (model performance).\nAcquisition Function: A function that determines which hyperparameter configuration to try next, balancing exploration (trying new areas) and exploitation (focusing on promising areas).\nObjective Function: The function we’re trying to optimize, typically the model’s performance on a validation set.\n\n\n\nHow Bayesian Optimization Works\n\nInitialize with a few random hyperparameter configurations and evaluate their performance.\nFit a surrogate model to these observations.\nUse the acquisition function to determine the next promising hyperparameter configuration to try.\nEvaluate the objective function at this new point.\nUpdate the surrogate model with the new observation.\nRepeat steps 3-5 for a specified number of iterations or until a stopping criterion is met.\n\n\n\nAdvantages of Bayesian Optimization\n\nMore efficient than grid or random search, especially for expensive-to-evaluate objective functions.\nCan handle complex hyperparameter spaces with dependencies between parameters.\nProvides uncertainty estimates for its predictions, allowing for more informed decisions.\n\n\n\nImplementing Bayesian Optimization\nThere are a few well-known libraries for implementing Bayesian optimization in Python, including * hyperopt * keras-tuner * optuna\nThese libraries provide easy-to-use interfaces for optimizing hyperparameters of machine learning models.\nHere’s a simple example using the optuna library to perform Bayesian optimization for a Random Forest Classifier from scikit-learn.\n\n\nCode\nimport optuna\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\n# Step 2: Load the dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Step 3: Define the objective function\ndef objective(trial):\n    # Define the hyperparameter search space\n    n_estimators = trial.suggest_int('n_estimators', 10, 100)\n    max_depth = trial.suggest_int('max_depth', 1, 10)\n    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n    \n    # Create the model with the suggested hyperparameters\n    model = RandomForestClassifier(\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        min_samples_split=min_samples_split,\n        min_samples_leaf=min_samples_leaf,\n        bootstrap=bootstrap,\n        random_state=42\n    )\n    \n    # Perform cross-validation and return the mean score\n    score = cross_val_score(model, X, y, cv=3, n_jobs=1, scoring='accuracy').mean()\n    return score\n\n# Step 4: Create a study and optimize the objective function\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\n\n# Step 5: Print the results\nprint(f\"Best hyperparameters: {study.best_params}\")\nprint(f\"Best cross-validated score: {study.best_value}\")\n\n\n[I 2024-09-19 19:33:06,430] A new study created in memory with name: no-name-32e3b206-2b7e-49ad-81fb-252e23db6c6f\n[I 2024-09-19 19:33:06,510] Trial 0 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 71, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 0 with value: 0.9533333333333333.\n[I 2024-09-19 19:33:06,550] Trial 1 finished with value: 0.9466666666666667 and parameters: {'n_estimators': 29, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 0 with value: 0.9533333333333333.\n[I 2024-09-19 19:33:06,628] Trial 2 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 63, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 2 with value: 0.9666666666666667.\n[I 2024-09-19 19:33:06,675] Trial 3 finished with value: 0.6999999999999998 and parameters: {'n_estimators': 56, 'max_depth': 1, 'min_samples_split': 9, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 2 with value: 0.9666666666666667.\n[I 2024-09-19 19:33:06,720] Trial 4 finished with value: 0.9733333333333333 and parameters: {'n_estimators': 34, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:06,802] Trial 5 finished with value: 0.94 and parameters: {'n_estimators': 96, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:06,851] Trial 6 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 38, 'max_depth': 3, 'min_samples_split': 6, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:06,953] Trial 7 finished with value: 0.9333333333333332 and parameters: {'n_estimators': 85, 'max_depth': 1, 'min_samples_split': 10, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:07,026] Trial 8 finished with value: 0.94 and parameters: {'n_estimators': 86, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:07,081] Trial 9 finished with value: 0.94 and parameters: {'n_estimators': 61, 'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:07,143] Trial 10 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 40, 'max_depth': 8, 'min_samples_split': 2, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:07,208] Trial 11 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 45, 'max_depth': 4, 'min_samples_split': 8, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:07,236] Trial 12 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 13, 'max_depth': 4, 'min_samples_split': 4, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:07,268] Trial 13 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 17, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:07,356] Trial 14 finished with value: 0.9733333333333333 and parameters: {'n_estimators': 62, 'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:07,403] Trial 15 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 28, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:07,502] Trial 16 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 72, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:07,573] Trial 17 finished with value: 0.96 and parameters: {'n_estimators': 49, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:07,619] Trial 18 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 28, 'max_depth': 8, 'min_samples_split': 2, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:07,718] Trial 19 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 70, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:07,793] Trial 20 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 50, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:07,884] Trial 21 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 64, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:07,992] Trial 22 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 80, 'max_depth': 4, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:08,076] Trial 23 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 58, 'max_depth': 7, 'min_samples_split': 6, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:08,136] Trial 24 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 38, 'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:08,231] Trial 25 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 67, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:08,288] Trial 26 finished with value: 0.94 and parameters: {'n_estimators': 53, 'max_depth': 2, 'min_samples_split': 7, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:08,393] Trial 27 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 78, 'max_depth': 3, 'min_samples_split': 6, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:08,455] Trial 28 finished with value: 0.9733333333333333 and parameters: {'n_estimators': 42, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:08,485] Trial 29 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 21, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:08,538] Trial 30 finished with value: 0.96 and parameters: {'n_estimators': 33, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:08,608] Trial 31 finished with value: 0.9733333333333333 and parameters: {'n_estimators': 44, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:08,677] Trial 32 finished with value: 0.9733333333333333 and parameters: {'n_estimators': 45, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:08,730] Trial 33 finished with value: 0.96 and parameters: {'n_estimators': 33, 'max_depth': 4, 'min_samples_split': 5, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:08,772] Trial 34 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 23, 'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:08,839] Trial 35 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 45, 'max_depth': 5, 'min_samples_split': 4, 'min_samples_leaf': 10, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:08,900] Trial 36 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 57, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:08,955] Trial 37 finished with value: 0.9466666666666667 and parameters: {'n_estimators': 38, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:09,007] Trial 38 finished with value: 0.9733333333333333 and parameters: {'n_estimators': 32, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:09,056] Trial 39 finished with value: 0.9466666666666667 and parameters: {'n_estimators': 42, 'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:09,183] Trial 40 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 97, 'max_depth': 7, 'min_samples_split': 7, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:09,254] Trial 41 finished with value: 0.9733333333333333 and parameters: {'n_estimators': 49, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:09,324] Trial 42 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 47, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:09,401] Trial 43 finished with value: 0.9733333333333333 and parameters: {'n_estimators': 55, 'max_depth': 5, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:09,465] Trial 44 finished with value: 0.9733333333333333 and parameters: {'n_estimators': 43, 'max_depth': 8, 'min_samples_split': 3, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:09,541] Trial 45 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 52, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:09,627] Trial 46 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 61, 'max_depth': 4, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:09,673] Trial 47 finished with value: 0.9533333333333333 and parameters: {'n_estimators': 36, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:09,719] Trial 48 finished with value: 0.9666666666666667 and parameters: {'n_estimators': 25, 'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n[I 2024-09-19 19:33:09,783] Trial 49 finished with value: 0.9733333333333333 and parameters: {'n_estimators': 41, 'max_depth': 4, 'min_samples_split': 3, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 4 with value: 0.9733333333333333.\n\n\nBest hyperparameters: {'n_estimators': 34, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': True}\nBest cross-validated score: 0.9733333333333333\n\n\n\n\nWhen to Use Bayesian Optimization\nBayesian optimization is particularly useful when:\n\nThe objective function is expensive to evaluate (e.g., training deep neural networks).\nThe hyperparameter space is complex or high-dimensional.\nYou have a limited budget for hyperparameter tuning.\n\nHowever, for simpler models or when you have ample computational resources, simpler methods like grid search or random search might be sufficient."
  },
  {
    "objectID": "posts/train-dev-test-splits/index.html#conclusion",
    "href": "posts/train-dev-test-splits/index.html#conclusion",
    "title": "Train Dev Test Data Splits",
    "section": "Conclusion",
    "text": "Conclusion\nChoosing the right data split and tuning hyperparameters are crucial steps in the machine learning pipeline. By considering factors such as dataset size, noise levels, and data complexity, you can optimize your split to balance between model training and performance estimation. Remember, these are guidelines, and the best split for your project may require some experimentation and adjustment.\nAs you work with larger datasets, you might find that you can allocate smaller percentages to validation and test sets while still maintaining statistical significance. However, always ensure that your validation and test sets are large enough to provide reliable performance estimates for your specific problem.\nBy understanding the relationships between models, data, and hyperparameters, implementing effective data splitting strategies, and utilizing advanced techniques like Bayesian optimization for hyperparameter tuning, you can make more informed decisions and develop more effective machine learning solutions."
  },
  {
    "objectID": "posts/sklearn-vs-r-model-api/index.html",
    "href": "posts/sklearn-vs-r-model-api/index.html",
    "title": "sklearn vs R model APIs",
    "section": "",
    "text": "Made with ❤️ and GitHub Copilot"
  },
  {
    "objectID": "posts/sklearn-vs-r-model-api/index.html#introduction",
    "href": "posts/sklearn-vs-r-model-api/index.html#introduction",
    "title": "sklearn vs R model APIs",
    "section": "Introduction",
    "text": "Introduction\nMany computing on data rely on R and scikit-learn as primary tools for statistical modeling and machine learning. While both languages offer powerful capabilities for creating and working with models, they employ distinct approaches to model creation and fitting. This blog post explores these differences, focusing on how models are instantiated and fitted in each language. The comparison will examine linear models with toy synthetic data.\nNote that the discussion is of R with the dominance of S3 classes (and so generic functions) for popular established models, and scikit-learn for Python. The scikit-learn model approach has become the standard in many Python frameworks, including Keras, mlx and PyTorch. (Some discussions on the topic of R vs Python for computing on data focus on the languages themselves, but this post will focus on the model APIs, in particular how instantiation, fit and predict are invoked.)"
  },
  {
    "objectID": "posts/sklearn-vs-r-model-api/index.html#rs-approach-implicit-fitting-and-s3-classes",
    "href": "posts/sklearn-vs-r-model-api/index.html#rs-approach-implicit-fitting-and-s3-classes",
    "title": "sklearn vs R model APIs",
    "section": "R’s Approach: Implicit Fitting and S3 Classes",
    "text": "R’s Approach: Implicit Fitting and S3 Classes\nIn R, model fitting is often implicit and occurs at the time of model creation. This is facilitated by R’s use of S3 classes, which are based on generic functions.\n(There are many OOP paradigms used for model fitting in R - more on this later. By far the most popular for an important class of models is using the S3 system.)\n\nUnderstanding S3 Generic Functions\nS3 is R’s first and most widely used object-oriented system. It’s based on generic functions, which provide a simple yet powerful way to implement polymorphism in R. Here’s how S3 generic functions work:\n\nA generic function is a function that dispatches method calls to other functions, based on the class of the arguments passed to it.\nWhen a generic function is called, R looks for a method that matches the class of the object passed as an argument.\nIf a matching method is found, it is called; otherwise, R looks for a default method.\n\nLet’s look at an example using the print() function, which is a common S3 generic:\n\n\nCode\n# Define a new S3 class\ncreate_human &lt;- function(name, age) {\n  human &lt;- list(name = name, age = age)\n  class(human) &lt;- \"human\"\n  return(human)\n}\n\n# Define a method for the print generic\nprint.human &lt;- function(x, ...) {\n  cat(\"Human Attributes \\n\")\n  cat(\"Name: \", x$name, \"\\n\")\n  cat(\"Age: \", x$age, \"\\n\")\n}\n\n# Create an object and print it\njohn &lt;- create_human(\"John Doe\", 30)\nprint(john)\n\n\nHuman Attributes \nName:  John Doe \nAge:  30 \n\n\nIn this example: - We define a new S3 class called “human”. - We create a method print.human() for the print() generic function. - When we call print(john), R recognizes that john is of class “human” and calls our print.human() method.\nThis system allows for intuitive and flexible implementation of methods for different classes. In the context of modeling, it enables R to provide a consistent interface for various model types while allowing for specialized behavior based on the specific model class.\n\n\nS3 Classes for Models\nR uses S3 classes for many of its modeling functions. Some examples include:\n\nlm for linear models\nglm for generalized linear models\nrpart for decision trees\nrandomForest for random forest models\n\nWhen you create a model object using these functions, you are actually creating an instance of an S3 class.\nThese classes come with predefined generic function methods for common operations like print(), summary(), and predict(), which behave differently depending on the model type.\n\n\nImplicit Fitting\nIn R, when a model-type object is instantiated with a call, it is typically fitted automatically. Let’s look at linear model as a simple example.\nBut first we need some data, we’ll use a toy example - to illustrate the model fitting process.\nBy using toy synthetic data, we are avoiding details such as:\n\nData cleaning\nInput imbalance\nOutput imbalance\nMulti-collinearity\nMultiple features\nPerfect separation\nImputation\nEncoding\nFeature scaling\nHandling outliers (either removing them, or replacing them with a reasonable value, or even considering robust models)\n\nWe will need a split function to split the data into train-dev-test sets.\n\n\nCode\nsplit_data &lt;- function(data, train_ratio = 0.7, dev_ratio = 0.15, test_ratio = 0.15, seed = 123) {\n  # Check that the ratios sum to 1\n  if (train_ratio + dev_ratio + test_ratio != 1) {\n    stop(\"The sum of train_ratio, dev_ratio, and test_ratio must be 1.\")\n  }\n  \n  # Set seed for reproducibility\n  set.seed(seed)\n  \n  # Calculate the number of samples for each set\n  n &lt;- nrow(data)\n  train_size &lt;- floor(train_ratio * n)\n  dev_size &lt;- floor(dev_ratio * n)\n  \n  # Generate random indices for the training set\n  train_indices &lt;- sample(seq_len(n), size = train_size)\n  \n  # Generate random indices for the dev set from the remaining indices\n  remaining_indices &lt;- setdiff(seq_len(n), train_indices)\n  dev_indices &lt;- sample(remaining_indices, size = dev_size)\n  \n  # The remaining indices are for the test set\n  test_indices &lt;- setdiff(remaining_indices, dev_indices)\n  \n  # Split the data\n  train_data &lt;- data[train_indices, ]\n  dev_data &lt;- data[dev_indices, ]\n  test_data &lt;- data[test_indices, ]\n  \n  return(list(train = train_data, dev = dev_data, test = test_data))\n}\n\n\nGreat. Now we can generate synthetic data suitable for a linear model, and split it into train-dev-test sets.\n\n\nCode\ngenerate_linear_data &lt;- function(n = 100, seed = 123, slope = 2, intercept = 5, noise_sd = 1) {\n  # Set seed for reproducibility\n  set.seed(seed)\n  \n  # Generate predictor variable (x)\n  x &lt;- runif(n, min = 0, max = 10)\n  \n  # Generate response variable (y) with a linear relationship and some noise\n  y &lt;- intercept + slope * x + rnorm(n, mean = 0, sd = noise_sd)\n  \n  # Create a dataframe\n  data &lt;- data.frame(x = x, y = y)\n  \n  return(data)\n}\n\n# Generate the data\nlinear_data &lt;- generate_linear_data(n = 100, seed = 123, slope = 2, intercept = 5, noise_sd = 1)\n\n# Display the first few rows of the data\ndf &lt;- split_data(linear_data)\nhead(df$train)\n\n\n\nA data.frame: 6 x 2\n\n\n\nx\ny\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n31\n9.6302423\n24.266249\n\n\n79\n3.5179791\n11.074102\n\n\n51\n0.4583117\n5.206217\n\n\n14\n5.7263340\n15.434093\n\n\n67\n8.1006435\n21.306963\n\n\n42\n4.1454634\n13.839324\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\n\nggplot(df$train, aes(x = x, y = y)) +\n  geom_point(color = \"blue\") +\n  labs(title = \"Scatter Plot of Training Data\",\n       x = \"Input (x)\",\n       y = \"Output (y)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNow let’s work with the training data. We’ll fit a linear model to predict y based on x using the dev set. For completeless we have split out a test set to evaluate the model’s final performance if we had tuned on the dev set or compared against other models.\nLinear Regression\n\n\nCode\n# Fit a simple linear regression model\nlm_model &lt;- lm(y ~ x, data = df$train)\n\n# Summarize the model\nsummary(lm_model)\n\n\n\nCall:\nlm(formula = y ~ x, data = df$train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.13378 -0.74871 -0.05242  0.59871  2.34082 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.85761    0.26186   18.55   &lt;2e-16 ***\nx            1.99524    0.04485   44.49   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.018 on 68 degrees of freedom\nMultiple R-squared:  0.9668,    Adjusted R-squared:  0.9663 \nF-statistic:  1979 on 1 and 68 DF,  p-value: &lt; 2.2e-16\n\n\nWe can visually inspect the model’s fit by plotting the data points and the regression line.\n\n\nCode\nggplot(linear_data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Simple Linear Regression\",\n       x = \"Predictor (x)\",\n       y = \"Response (y)\") +\n  theme_minimal()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAn objective evaluation metric, or score provides an comparable measure of model performance. In this case, we’ll use the mean absolute percentage error (MAPE) on the dev set.\n\n\nCode\n# Generate predictions for the development set\ndev_predictions &lt;- predict(lm_model, newdata = df$dev)\n\n# Calculate Mean Absolute Percentage Error (MAPE)\ncalculate_mape &lt;- function(actual, predicted) {\n  mean(abs((actual - predicted) / actual)) * 100\n}\n\n# Calculate MAPE for the development set\nmape &lt;- calculate_mape(df$dev$y, dev_predictions)\nprint(paste(\"Mean Absolute Percentage Error (MAPE) on Dev Set: \", round(mape,2), \"%\", sep = \"\"))\n\n\n[1] \"Mean Absolute Percentage Error (MAPE) on Dev Set: 4.49%\"\n\n\nCool, we’ve successfully fitted a linear regression model in R! Note that we used the instance based fitting and predicted using the s3 class method.\nLet’s spell this out in a more detailed way:\n\nWe created a linear model object using the lm() function. When this object was created, the model was fitted to the training data at the same time.\nWe used the predict() method of the model object to make predictions on the dev set.\n\nThe underlying s3 class system in R allows for this implicit fitting and prediction process, making it easy to work with models in a consistent manner.\nA sketch of the flow is:\n\n\n\nS3 workflow of fit and predict\n\n\nAnd a very rough sketch of the code, without all the details (calls to C and FORTRAN routines, checks for edge cases, convergence criteria, etc.):\nlm &lt;- function(formula, data, ...) {\n  # Step 1: Extract the model frame from formula and data\n  mf &lt;- model.frame(formula = formula, data = data)\n  \n  # Step 2: Create model matrix (X) and response vector (y)\n  X &lt;- model.matrix(attr(mf, \"terms\"), mf)\n  y &lt;- model.response(mf)\n  \n  # Step 3: QR decomposition to solve for coefficients\n  fit &lt;- qr.solve(qr(X), y)\n  \n  # Step 4: Create lm object with fitted values and residuals\n  result &lt;- list(coefficients = fit$coefficients,\n                 residuals = y - X %*% fit$coefficients,\n                 fitted.values = X %*% fit$coefficients,\n                 qr = qr(X),\n                 terms = terms(formula),\n                 call = match.call())\n  \n  class(result) &lt;- \"lm\"  # Assign class \"lm\"\n  return(result)\n}\n\nfit &lt;- function(formula, data, model_type = \"lm\", ...) {\n  UseMethod(\"fit\", model_type)  # Dispatch based on model_type\n}\n\nfit.lm &lt;- function(formula, data, ...) {\n  # Calls lm() to fit the linear model\n  model &lt;- lm(formula, data = data, ...)\n  return(model)\n}\n\npredict &lt;- function(object, ...) {\n  UseMethod(\"predict\")\n}\n\npredict.lm &lt;- function(object, newdata, ...) {\n  # Step 1: Construct model matrix for new data\n  new_X &lt;- model.matrix(terms(object), newdata)\n  \n  # Step 2: Use the coefficients to calculate predictions\n  preds &lt;- new_X %*% coef(object)\n  \n  return(preds)\n}\nA brief outline of the code snippet:\n\nThe lm() function creates a linear model object by solving the normal equations using QR decomposition. It depends on receiving a formula and data as input at the time of instantiation.\nThe fit() function is a generic function that dispatches to specific fitting functions based on the model type.\nThe fit.lm() function fits a linear model using the lm() function.\n\nAll of this depends on S3 classes and generic functions.\nNow moving on, let’s use the same dataset with Python using scikit-learn. We’ll save it down to a csv file and load it into a pandas dataframe.\n\n\nCode\nwrite.csv(df$train, \"train_data.csv\", row.names = FALSE)\nwrite.csv(df$dev, \"dev_data.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/sklearn-vs-r-model-api/index.html#pythons-approach-explicit-fitting-with-scikit-learn",
    "href": "posts/sklearn-vs-r-model-api/index.html#pythons-approach-explicit-fitting-with-scikit-learn",
    "title": "sklearn vs R model APIs",
    "section": "Python’s Approach: Explicit Fitting with Scikit-learn",
    "text": "Python’s Approach: Explicit Fitting with Scikit-learn\nPython, on the other hand, takes a more explicit approach to model fitting, especially when using the popular machine learning library Scikit-learn.\nLet’s load the data for train and dev sets.\n\n\nCode\nimport pandas as pd\ndf_train = pd.read_csv(\"train_data.csv\")\ndf_dev = pd.read_csv(\"dev_data.csv\")\n\n\n\nClass-Based Implementation\nPython uses a more traditional class-based implementation for its models. In scikit-learn, models are implemented as classes with specific methods for fitting and prediction.\n\n\nThe scikit-learn Interface\nThe scikit-learn library in Python uses a consistent interface across its models, with separate fit() and predict() methods. Here’s how this looks for Linear Regression\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_percentage_error\n\n\nmodel = LinearRegression() # instantiation\nmodel.fit(df_train[['x']], df_train['y'])  # fitting the model\n\n# Generate predictions for the development set\ndev_predictions = model.predict(df_dev[['x']])  # prediction\n\n# Calculate Mean Absolute Percentage Error (MAPE)\nmape = mean_absolute_percentage_error(df_dev['y'], dev_predictions) * 100\nprint(f\"Mean Absolute Percentage Error (MAPE) on Dev Set: {mape:.2f}%\")\n\n\nMean Absolute Percentage Error (MAPE) on Dev Set: 4.49%\n\n\nThis approach separates the model instantiation, fitting, and prediction steps, providing more explicit control over the process."
  },
  {
    "objectID": "posts/sklearn-vs-r-model-api/index.html#comparing-the-two-approaches",
    "href": "posts/sklearn-vs-r-model-api/index.html#comparing-the-two-approaches",
    "title": "sklearn vs R model APIs",
    "section": "Comparing the Two Approaches",
    "text": "Comparing the Two Approaches\nBoth approaches have their merits:\n\nR’s approach is more concise and can be more intuitive for statistical modeling. It’s particularly convenient for quick exploratory data analysis.\nPython’s approach provides more explicit control and is consistent across different types of models. This can be beneficial in production environments where you might want more fine-grained control over the fitting process.\n\n\nAdvantages of R’s Approach:\n\nMore concise code for simple modeling tasks\nIntuitive for statisticians used to thinking in terms of model formulas\nEasy to quickly try out different model specifications\n\n\n\nAdvantages of Python’s Approach:\n\nConsistent interface across different types of models\nMore explicit control over the fitting process\nEasier to integrate into larger software systems due to its object-oriented nature"
  },
  {
    "objectID": "posts/sklearn-vs-r-model-api/index.html#swapping-styles",
    "href": "posts/sklearn-vs-r-model-api/index.html#swapping-styles",
    "title": "sklearn vs R model APIs",
    "section": "Swapping Styles",
    "text": "Swapping Styles\n\nSimulating R-style Instantiation in Python\nWhile Python’s default approach is different, it’s possible to simulate R-style instantiation and fitting more closely. Here’s an implementation that mirrors R’s behavior, allowing for a two-step process of defining the model class and then fitting it with data:\n\n\nCode\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nclass RStyleModel:\n    def __init__(self, model_class, **kwargs):\n        self.model_class = model_class\n        self.kwargs = kwargs\n        self.fitted_model = None\n\n    def __call__(self, formula, data):\n        # Parse the formula\n        y, X = self._parse_formula(formula, data)\n        \n        # Create and fit the model\n        self.fitted_model = self.model_class(**self.kwargs)\n        self.fitted_model.fit(X, y)\n        \n        return self\n\n    def predict(self, X):\n        if self.fitted_model is None:\n            raise ValueError(\"Model has not been fitted yet.\")\n        return self.fitted_model.predict(X)\n\n    def _parse_formula(self, formula, data):\n        # Simple formula parsing (for demonstration purposes)\n        response, predictors = formula.split('~')\n        y = data[response.strip()]\n        if predictors.strip() == '.':\n            X = data.drop(response.strip(), axis=1)\n        else:\n            X = data[[p.strip() for p in predictors.split('+')]]\n        return y, X\n\n# Usage\nlm = RStyleModel(LinearRegression)\n# Assuming 'my_data' is a pandas DataFrame\nlm('y ~ x', df_train)\n\n# Predictions\nlinear_predictions = lm.predict(df_dev[['x']])\n\n# Calculate Mean Absolute Percentage Error (MAPE)\nmape = mean_absolute_percentage_error(df_dev['y'], linear_predictions) * 100\nprint(f\"Mean Absolute Percentage Error (MAPE) on Dev Set: {mape:.2f}%\")\n\n\nMean Absolute Percentage Error (MAPE) on Dev Set: 4.49%\n\n\nWonderful the results match the R implementation. This approach provides a more R-like workflow in Python, making it easier for R users to transition to Python for modeling tasks.\nThis implementation provides several R-like features:\n\nModel Class Definition: We define the model classes using names familiar to R users: lm for linear models, randomForest for random forests, and glm for generalized linear models (in this case, logistic regression).\nFormula Interface: We use an R-like formula syntax for model fitting (e.g., y ~ x or y ~ . for all variables).\nImplicit Fitting: The model is fitted when we call the model class with a formula and data, similar to R’s behavior.\nFamiliar Prediction: We use the predict method on the fitted model object, just like in R.\nFlexibility: This approach works for different types of models with a consistent interface, mirroring R’s functionality.\n\nThis implementation closely mirrors R’s workflow while still leveraging Python’s scikit-learn backend. It provides a familiar interface for R users while maintaining the flexibility and power of Python’s machine learning ecosystem.\n\n\nSimulating Python-style Instantiation in R\nConversely, it’s also possible to simulate Python’s explicit fitting approach in R. While this is less common due to R’s default behavior, it can be achieved by creating custom classes and methods that separate model instantiation from fitting.\nThere are many object oriented systems in R - you can find a discussion in Hadley Wickham’s Advanced R book.\nThere are many, many ways to do this in R, and the examples below are just a few possibilities.\nS3:\n# Define the LinearRegression S3 class\nLinearRegression &lt;- function() {\n  structure(list(coefficients = NULL, intercept = NULL), class = \"LinearRegression\")\n}\n\n# Fit method\nfit.LinearRegression &lt;- function(model, X, y) {\n  X &lt;- cbind(1, X)  # Add intercept term\n  beta &lt;- solve(t(X) %*% X) %*% t(X) %*% y  # Normal equation\n  model$intercept &lt;- beta[1]\n  model$coefficients &lt;- beta[-1]\n  return(model)\n}\n\n# Predict method\npredict.LinearRegression &lt;- function(model, X) {\n  X &lt;- cbind(1, X)  # Add intercept term\n  return(X %*% c(model$intercept, model$coefficients))  # Predictions\n}\n\n# Score method (R-squared)\nscore.LinearRegression &lt;- function(model, X, y) {\n  preds &lt;- predict(model, X)\n  ss_res &lt;- sum((y - preds) ^ 2)\n  ss_tot &lt;- sum((y - mean(y)) ^ 2)\n  return(1 - (ss_res / ss_tot))  # R-squared\n}\n\nmodel &lt;- LinearRegression()\nmodel &lt;- fit(model, df$train[\"x\"], df$train[\"y\"])\nprint(score(model, df$train[\"x\"], df$train[\"y\"]))\nor another implementation using S4:\n# Define the LinearRegression S4 class\nsetClass(\n  \"LinearRegression\",\n  slots = list(\n    coefficients = \"numeric\",\n    intercept = \"numeric\"\n  )\n)\n\n# Define the fit method\nsetGeneric(\"fit\", function(model, X, y) standardGeneric(\"fit\"))\nsetMethod(\"fit\", \"LinearRegression\", function(model, X, y) {\n  X &lt;- cbind(1, X)  # Add intercept term\n  beta &lt;- solve(t(X) %*% X) %*% t(X) %*% y  # Normal equation\n  model@intercept &lt;- beta[1]\n  model@coefficients &lt;- beta[-1]\n  return(model)\n})\n\n# Define the predict method\nsetGeneric(\"predict\", function(model, X) standardGeneric(\"predict\"))\nsetMethod(\"predict\", \"LinearRegression\", function(model, X) {\n  X &lt;- cbind(1, X)  # Add intercept term\n  return(X %*% c(model@intercept, model@coefficients))  # Predictions\n})\n\n# Define the score method (R-squared)\nsetGeneric(\"score\", function(model, X, y) standardGeneric(\"score\"))\nsetMethod(\"score\", \"LinearRegression\", function(model, X, y) {\n  preds &lt;- predict(model, X)\n  ss_res &lt;- sum((y - preds) ^ 2)\n  ss_tot &lt;- sum((y - mean(y)) ^ 2)\n  return(1 - (ss_res / ss_tot))  # R-squared\n})\n\nmodel &lt;- new(\"LinearRegression\")\nmodel &lt;- fit(model, df$train[\"x\"], df$train[\"y\"])\nprint(score(model, df$train[\"x\"], df$train[\"y\"]))\nor using Reference Classes:\n# Define the LinearRegression reference class\nLinearRegression &lt;- setRefClass(\n  \"LinearRegression\",\n  \n  fields = list(\n    coefficients = \"numeric\",\n    intercept = \"numeric\"\n  ),\n  \n  methods = list(\n    fit = function(X, y) {\n      X &lt;- cbind(1, X)  # Add intercept term\n      beta &lt;- solve(t(X) %*% X) %*% t(X) %*% y  # Normal equation\n      intercept &lt;&lt;- beta[1]\n      coefficients &lt;&lt;- beta[-1]\n      invisible(self)\n    },\n    \n    predict = function(X) {\n      X &lt;- cbind(1, X)  # Add intercept term\n      return(X %*% c(intercept, coefficients))  # Predictions\n    },\n    \n    score = function(X, y) {\n      preds &lt;- predict(X)\n      ss_res &lt;- sum((y - preds) ^ 2)\n      ss_tot &lt;- sum((y - mean(y)) ^ 2)\n      return(1 - (ss_res / ss_tot))  # R-squared\n    }\n  )\n)\n\nmodel &lt;- LinearRegression$new()\nmodel$fit(df$train[\"x\"], df$train[\"y\"])\nprint(model$score(df$train[\"x\"], df$train[\"y\"]))\nor R6:\nlibrary(R6)\n\nLinearRegression &lt;- R6Class(\n  \"LinearRegression\",\n  \n  public = list(\n    coefficients = NULL,\n    intercept = NULL,\n    \n    fit = function(X, y) {\n      X &lt;- cbind(1, X)  # Add intercept term\n      beta &lt;- solve(t(X) %*% X) %*% t(X) %*% y  # Normal equation\n      self$intercept &lt;- beta[1]\n      self$coefficients &lt;- beta[-1]\n      invisible(self)\n    },\n    \n    predict = function(X) {\n      X &lt;- cbind(1, X)  # Add intercept term\n      return(X %*% c(self$intercept, self$coefficients))  # Predictions\n    },\n    \n    score = function(X, y) {\n      preds &lt;- self$predict(X)\n      ss_res &lt;- sum((y - preds) ^ 2)\n      ss_tot &lt;- sum((y - mean(y)) ^ 2)\n      return(1 - (ss_res / ss_tot))  # R-squared\n    }\n  )\n)\n\n# Example usage\ndf_binary &lt;- data.frame(Sal = c(1, 2, 3, 4, 5), Temp = c(15, 20, 25, 30, 35))\nX &lt;- as.matrix(df_binary$Sal)\ny &lt;- as.matrix(df_binary$Temp)\n\nmodel &lt;- LinearRegression$new()\nmodel$fit(df$train[\"x\"], df$train[\"y\"])\nprint(model$score(df$train[\"x\"], df$train[\"y\"]))"
  },
  {
    "objectID": "posts/sklearn-vs-r-model-api/index.html#conclusion",
    "href": "posts/sklearn-vs-r-model-api/index.html#conclusion",
    "title": "sklearn vs R model APIs",
    "section": "Conclusion",
    "text": "Conclusion\nR and Python, while both powerful tools for data analysis and modeling, have different default approaches to model creation and fitting. Although R has many OOP paradigms, for many popular models R leverages S3 classes and generic functions, and is often more concise and intuitive for statistical modeling. It allows for quick model specification and is particularly suited for exploratory data analysis.\nPython’s approach, exemplified by the scikit-learn library, offers more explicit control and consistency across different types of models. This can be advantageous in production environments and when integrating models into larger software systems.\nUnderstanding these differences can help those computing on data choose the right tool for their specific needs and workflow preferences. Whether one prefers R’s implicit fitting or Python’s explicit approach, both languages offer flexible and powerful options for a wide range of statistical modeling and machine learning tasks, from simple linear regression to complex ensemble methods like random forests.\nThe choice between R and Python often comes down to the specific requirements of the project, the background of the practitioner, and the broader ecosystem in which the analysis or model will be deployed. By understanding the strengths of each approach, data professionals can make informed decisions and leverage the best of both worlds when necessary.\nMoreover, as demonstrated in the last section, it’s possible to create interfaces in Python that mimic R’s behavior, providing a bridge for those transitioning between the two languages or working in mixed environments. This flexibility highlights the power of both languages and the creativity of the data science community in adapting tools to suit their needs."
  },
  {
    "objectID": "posts/minimal-data-inspection-pre-splitting/index.html",
    "href": "posts/minimal-data-inspection-pre-splitting/index.html",
    "title": "Minimal Data Inspection Before Splitting The Dataset",
    "section": "",
    "text": "Made with ❤️ and GitHub Copilot"
  },
  {
    "objectID": "posts/minimal-data-inspection-pre-splitting/index.html#loading-and-preparing-the-dataset",
    "href": "posts/minimal-data-inspection-pre-splitting/index.html#loading-and-preparing-the-dataset",
    "title": "Minimal Data Inspection Before Splitting The Dataset",
    "section": "Loading and Preparing the Dataset",
    "text": "Loading and Preparing the Dataset\nLet’s begin our adventure with the Star Wars dataset:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the Star Wars dataset\nurl = \"https://raw.githubusercontent.com/fivethirtyeight/data/master/star-wars-survey/StarWars.csv\"\ndf = pd.read_csv(url, encoding=\"latin1\")\n\n# Clean column names\ndf.columns = df.columns.str.replace(\"Which of the following Star Wars films have you seen? Please select all that apply.\", \"seen_\")\ndf.columns = df.columns.str.replace(\"Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.\", \"rank_\")\ndf.columns = df.columns.str.replace(\"Do you consider yourself to be a fan of the Star Wars film franchise?\", \"is_fan\")\n\n# Select a subset of columns for our analysis\ncolumns_to_use = [\n    'seen_1', 'seen_2', 'seen_3', 'seen_4', 'seen_5', 'seen_6',\n    'rank_1', 'rank_2', 'rank_3', 'rank_4', 'rank_5', 'rank_6',\n    'is_fan', 'Gender', 'Age', 'Household Income', 'Education'\n]\n\ndf = df[columns_to_use]\n\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Features: {df.columns.tolist()}\")\n\n\n   RespondentID Have you seen any of the 6 films in the Star Wars franchise?  \\\n0           NaN                                           Response             \n1  3.292880e+09                                                Yes             \n2  3.292880e+09                                                 No             \n3  3.292765e+09                                                Yes             \n4  3.292763e+09                                                Yes             \n\n  Do you consider yourself to be a fan of the Star Wars film franchise?  \\\n0                                           Response                      \n1                                                Yes                      \n2                                                NaN                      \n3                                                 No                      \n4                                                Yes                      \n\n  Which of the following Star Wars films have you seen? Please select all that apply.  \\\n0           Star Wars: Episode I  The Phantom Menace                                    \n1           Star Wars: Episode I  The Phantom Menace                                    \n2                                                NaN                                    \n3           Star Wars: Episode I  The Phantom Menace                                    \n4           Star Wars: Episode I  The Phantom Menace                                    \n\n                                    Unnamed: 4  \\\n0  Star Wars: Episode II  Attack of the Clones   \n1  Star Wars: Episode II  Attack of the Clones   \n2                                          NaN   \n3  Star Wars: Episode II  Attack of the Clones   \n4  Star Wars: Episode II  Attack of the Clones   \n\n                                    Unnamed: 5  \\\n0  Star Wars: Episode III  Revenge of the Sith   \n1  Star Wars: Episode III  Revenge of the Sith   \n2                                          NaN   \n3  Star Wars: Episode III  Revenge of the Sith   \n4  Star Wars: Episode III  Revenge of the Sith   \n\n                          Unnamed: 6  \\\n0  Star Wars: Episode IV  A New Hope   \n1  Star Wars: Episode IV  A New Hope   \n2                                NaN   \n3                                NaN   \n4  Star Wars: Episode IV  A New Hope   \n\n                                     Unnamed: 7  \\\n0  Star Wars: Episode V The Empire Strikes Back   \n1  Star Wars: Episode V The Empire Strikes Back   \n2                                           NaN   \n3                                           NaN   \n4  Star Wars: Episode V The Empire Strikes Back   \n\n                                 Unnamed: 8  \\\n0  Star Wars: Episode VI Return of the Jedi   \n1  Star Wars: Episode VI Return of the Jedi   \n2                                       NaN   \n3                                       NaN   \n4  Star Wars: Episode VI Return of the Jedi   \n\n  Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.  \\\n0           Star Wars: Episode I  The Phantom Menace                                                                                              \n1                                                  3                                                                                              \n2                                                NaN                                                                                              \n3                                                  1                                                                                              \n4                                                  5                                                                                              \n\n   ...       Unnamed: 28       Which character shot first?  \\\n0  ...              Yoda                          Response   \n1  ...    Very favorably  I don't understand this question   \n2  ...               NaN                               NaN   \n3  ...  Unfamiliar (N/A)  I don't understand this question   \n4  ...    Very favorably  I don't understand this question   \n\n  Are you familiar with the Expanded Universe?  \\\n0                                     Response   \n1                                          Yes   \n2                                          NaN   \n3                                           No   \n4                                           No   \n\n  Do you consider yourself to be a fan of the Expanded Universe?æ  \\\n0                                           Response                 \n1                                                 No                 \n2                                                NaN                 \n3                                                NaN                 \n4                                                NaN                 \n\n  Do you consider yourself to be a fan of the Star Trek franchise?    Gender  \\\n0                                           Response                Response   \n1                                                 No                    Male   \n2                                                Yes                    Male   \n3                                                 No                    Male   \n4                                                Yes                    Male   \n\n        Age     Household Income                         Education  \\\n0  Response             Response                          Response   \n1     18-29                  NaN                High school degree   \n2     18-29         $0 - $24,999                   Bachelor degree   \n3     18-29         $0 - $24,999                High school degree   \n4     18-29  $100,000 - $149,999  Some college or Associate degree   \n\n  Location (Census Region)  \n0                 Response  \n1           South Atlantic  \n2       West South Central  \n3       West North Central  \n4       West North Central  \n\n[5 rows x 38 columns]"
  },
  {
    "objectID": "posts/minimal-data-inspection-pre-splitting/index.html#the-importance-of-proper-data-splitting",
    "href": "posts/minimal-data-inspection-pre-splitting/index.html#the-importance-of-proper-data-splitting",
    "title": "Minimal Data Inspection Before Splitting The Dataset",
    "section": "The Importance of Proper Data Splitting",
    "text": "The Importance of Proper Data Splitting\nSplitting your data is crucial for developing robust machine learning models. The main purposes of these splits are:\n\nTraining set: Used to train the model\nValidation set: Used for hyperparameter tuning and model selection\nTest set: Used to evaluate the final model’s performance on unseen data\n\nProper data splitting helps prevent overfitting and provides a realistic estimate of how your model will perform on new, unseen data."
  },
  {
    "objectID": "posts/minimal-data-inspection-pre-splitting/index.html#the-dangers-of-premature-preprocessing",
    "href": "posts/minimal-data-inspection-pre-splitting/index.html#the-dangers-of-premature-preprocessing",
    "title": "Minimal Data Inspection Before Splitting The Dataset",
    "section": "The Dangers of Premature Preprocessing",
    "text": "The Dangers of Premature Preprocessing\nWhile data preprocessing is essential, performing certain steps before splitting your data can lead to data leakage and biased models. Let’s explore some common preprocessing steps and their associated risks:\n\n1. Handling Outliers\nRemoving or modifying outliers based on the entire dataset before splitting can lead to data leakage.\n\n\nCode\ndef show_outlier_effect(df, column):\n    plt.figure(figsize=(12, 6))\n    plt.subplot(121)\n    df[column].hist(bins=30)\n    plt.title(f\"Original {column} Distribution\")\n    \n    # Incorrect way: Remove outliers before splitting\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    df_filtered = df[(df[column] &gt;= Q1 - 1.5*IQR) & (df[column] &lt;= Q3 + 1.5*IQR)]\n    \n    plt.subplot(122)\n    df_filtered[column].hist(bins=30)\n    plt.title(f\"{column} Distribution After Removing Outliers\")\n    plt.tight_layout()\n    plt.show()\n\nshow_outlier_effect(df, 'Age')\n\n\nRemoving outliers from the entire dataset before splitting would result in a test set that no longer represents the true data distribution. This can lead to overly optimistic performance estimates and poor generalization to new data.\n\n\n2. Bucketing Variables\nCreating categorical variables from continuous ones (bucketing) based on the entire dataset can also cause data leakage:\n\n\nCode\ndef show_bucketing_effect(df, column, n_bins=5):\n    plt.figure(figsize=(12, 6))\n    plt.subplot(121)\n    df[column].hist(bins=30)\n    plt.title(f\"Original {column} Distribution\")\n    \n    # Incorrect way: Create bins based on the entire dataset\n    df['bucketed'] = pd.qcut(df[column], q=n_bins)\n    \n    plt.subplot(122)\n    df['bucketed'].value_counts().sort_index().plot(kind='bar')\n    plt.title(f\"Bucketed {column} Distribution\")\n    plt.tight_layout()\n    plt.show()\n\nshow_bucketing_effect(df, 'Age')\n\n\nBucketing variables using information from the entire dataset can introduce bias, as the bin boundaries are influenced by the test set.\n\n\n3. Handling Missing Data\nImputing missing values using information from the entire dataset can lead to data leakage:\n\n\nCode\ndef show_missingness_effect(df, column):\n    # Introduce some missing values\n    df_missing = df.copy()\n    df_missing.loc[df_missing.sample(frac=0.2).index, column] = np.nan\n    \n    print(f\"Missing values in {column}: {df_missing[column].isnull().sum()}\")\n    \n    # Incorrect way: Impute missing values based on the entire dataset\n    df_imputed = df_missing.copy()\n    df_imputed[column].fillna(df_imputed[column].mean(), inplace=True)\n    \n    plt.figure(figsize=(12, 6))\n    plt.subplot(121)\n    df[column].hist(bins=30)\n    plt.title(f\"Original {column} Distribution\")\n    \n    plt.subplot(122)\n    df_imputed[column].hist(bins=30)\n    plt.title(f\"{column} Distribution After Imputation\")\n    plt.tight_layout()\n    plt.show()\n\nshow_missingness_effect(df, 'Age')\n\n\nImputing missing values using statistics from the entire dataset allows information from the test set to influence the training data, potentially leading to overfitting and unreliable performance estimates.\n\n\n4. Dimensionality Reduction\nApplying dimensionality reduction techniques like PCA to the entire dataset before splitting can also cause data leakage:\n\n\nCode\nfrom sklearn.decomposition import PCA\n\n\ndef show_pca_effect(df, n_components=2):\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    \n    # Incorrect way: Apply PCA to the entire dataset\n    pca = PCA(n_components=n_components)\n    pca_result = pca.fit_transform(df[numeric_cols])\n    \n    plt.figure(figsize=(10, 8))\n    plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)\n    plt.title(\"PCA Result (Incorrectly Applied to Entire Dataset)\")\n    plt.xlabel(\"First Principal Component\")\n    plt.ylabel(\"Second Principal Component\")\n    plt.show()\n    \n    print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n\nshow_pca_effect(df)\n\n\nApplying PCA or other dimensionality reduction techniques to the entire dataset allows information from the test set to influence the feature space of the training data."
  },
  {
    "objectID": "posts/minimal-data-inspection-pre-splitting/index.html#impact-on-model-performance",
    "href": "posts/minimal-data-inspection-pre-splitting/index.html#impact-on-model-performance",
    "title": "Minimal Data Inspection Before Splitting The Dataset",
    "section": "Impact on Model Performance",
    "text": "Impact on Model Performance\nWhen preprocessing steps are applied to the entire dataset before splitting, several problems can arise:\n\nOverfitting: The model may implicitly learn patterns from the test set, leading to overly optimistic performance estimates.\nPoor generalization: The model may not perform well on truly unseen data because it has been trained on a dataset that doesn’t represent the real-world data distribution.\nBiased feature importance: The importance of features may be distorted due to information leakage from the test set.\nUnreliable model selection: When comparing different models, the selection process may be biased towards models that overfit the leaked information."
  },
  {
    "objectID": "posts/minimal-data-inspection-pre-splitting/index.html#the-exception-minimal-target-variable-processing",
    "href": "posts/minimal-data-inspection-pre-splitting/index.html#the-exception-minimal-target-variable-processing",
    "title": "Minimal Data Inspection Before Splitting The Dataset",
    "section": "The Exception: Minimal Target Variable Processing",
    "text": "The Exception: Minimal Target Variable Processing\nWhile most preprocessing should be done after splitting the data, some minimal processing of the target variable can be acceptable and even beneficial when done carefully:\n\n1. Calculating the Empirical Distribution of the Target\nUnderstanding the distribution of your target variable can inform your sampling strategy and help identify potential issues.\n\n\nCode\ndef plot_target_distribution(df, target_column):\n    plt.figure(figsize=(10, 6))\n    df[target_column].value_counts(normalize=True).plot(kind='bar')\n    plt.title(f\"Distribution of {target_column}\")\n    plt.ylabel(\"Proportion\")\n    plt.xlabel(\"Class\")\n    plt.show()\n\nplot_target_distribution(df, 'is_fan')\n\n\n\n\n2. Handling Target Variable Outliers\nAddressing outliers in your target variable before splitting can sometimes be beneficial. Strategies include:\n\nGrouping levels: If certain classes have very few samples, consider grouping them.\nWinsorization: Cap extreme values at a specified percentile.\nRemoving outliers: In some cases, removing extreme outliers might be appropriate.\n\n\n\nCode\ndef handle_target_outliers(df, target_column, strategy='winsorize'):\n    if strategy == 'winsorize':\n        low = df[target_column].quantile(0.01)\n        high = df[target_column].quantile(0.99)\n        df[target_column] = df[target_column].clip(low, high)\n    elif strategy == 'remove':\n        Q1 = df[target_column].quantile(0.25)\n        Q3 = df[target_column].quantile(0.75)\n        IQR = Q3 - Q1\n        df = df[(df[target_column] &gt;= Q1 - 1.5*IQR) & (df[target_column] &lt;= Q3 + 1.5*IQR)]\n    \n    return df\n\n# Example usage (if 'is_fan' were a continuous variable):\n# df = handle_target_outliers(df, 'is_fan', strategy='winsorize')\n\n\n\n\n3. Stratified Sampling for Data Splits\nWhen your target variable is imbalanced, stratified sampling becomes crucial. This ensures that your train, validation, and test sets maintain the same proportion of classes as the original dataset.\n\n\nCode\ndef check_imbalance(df, target_column, threshold=0.8):\n    distribution = df[target_column].value_counts(normalize=True)\n    if distribution.max() &gt; threshold:\n        print(f\"Warning: The target variable is imbalanced. The majority class represents {distribution.max():.2%} of the data.\")\n        return True\n    return False\n\nis_imbalanced = check_imbalance(df, 'is_fan')\n\n# If imbalanced, use stratified sampling\nif is_imbalanced:\n    X = df.drop('is_fan', axis=1)\n    y = df['is_fan']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nelse:\n    X_train, X_test, y_train, y_test = train_test_split(df.drop('is_fan', axis=1), df['is_fan'], test_size=0.2, random_state=42)\n\nprint(f\"Training set class distribution:\\n{y_train.value_counts(normalize=True)}\")\nprint(f\"\\nTest set class distribution:\\n{y_test.value_counts(normalize=True)}\")"
  },
  {
    "objectID": "posts/minimal-data-inspection-pre-splitting/index.html#best-practices-for-data-preprocessing",
    "href": "posts/minimal-data-inspection-pre-splitting/index.html#best-practices-for-data-preprocessing",
    "title": "Minimal Data Inspection Before Splitting The Dataset",
    "section": "Best Practices for Data Preprocessing",
    "text": "Best Practices for Data Preprocessing\nTo avoid these issues and ensure robust models, follow these best practices:\n\nSplit your data first: Always split your data into train, validation, and test sets before any preprocessing.\nPreprocess within cross-validation: Apply preprocessing steps only to the training data within each fold of cross-validation.\nUse pipelines: Scikit-learn’s Pipeline class can help ensure that preprocessing steps are only applied to the training data.\nPreserve test set integrity: Never use information from the test set for preprocessing or model development.\n\nHere’s an example of how to properly split the data and apply preprocessing:\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# First, split the data\nX = df.drop('is_fan', axis=1)\ny = df['is_fan']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a preprocessing and modeling pipeline\npipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler()),\n    ('classifier', RandomForestClassifier(random_state=42))\n])\n\n# Fit the pipeline on the training data\npipeline.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = pipeline.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Model accuracy: {accuracy:.2f}\")"
  },
  {
    "objectID": "posts/minimal-data-inspection-pre-splitting/index.html#conclusion",
    "href": "posts/minimal-data-inspection-pre-splitting/index.html#conclusion",
    "title": "Minimal Data Inspection Before Splitting The Dataset",
    "section": "Conclusion",
    "text": "Conclusion\nProper data splitting and careful preprocessing are crucial for developing robust and reliable machine learning models. By understanding the risks associated with premature preprocessing and following best practices, you can avoid data leakage, obtain realistic performance estimates, and build models that generalize well to new data.\nRemember, the goal is not just to have a model that performs well on your test set, but one that will perform well on truly unseen data in real-world applications. By maintaining the integrity of your data splitting process and applying preprocessing steps correctly, you’ll be well on your way to mastering the art of machine learning."
  },
  {
    "objectID": "posts/developing-pytorch-geometric-on-m1/index.html",
    "href": "posts/developing-pytorch-geometric-on-m1/index.html",
    "title": "Developing Pytorch Geometric on M1 Apple Silicon",
    "section": "",
    "text": "TLDR: My investigation indicates that the library pytorch geometric (also referred to as pyg, using package name torch_geometric) on Apple Silicon (AS) - particularly the M1 chip - has partial developer support.\nA bit more: It is now feasible to perform a full developer installation of torch_geometric using PyTorch on AS with Apple’s native GPU compute framework, Metal Performance Shaders - mps - (which includes its own kind of embedded BLAS). The only remaining challenge is testing with a developer installation. The test suite does not currently build for AS.\nI’ve been talking with the pyg-team and they are interested in supporting AS more fully. It’s a matter of time and resources. rusty1s (the lead developer) has been very helpful and responsive, adding a test runner for Apple Silicon."
  },
  {
    "objectID": "posts/developing-pytorch-geometric-on-m1/index.html#user-installation",
    "href": "posts/developing-pytorch-geometric-on-m1/index.html#user-installation",
    "title": "Developing Pytorch Geometric on M1 Apple Silicon",
    "section": "User Installation",
    "text": "User Installation\nThe installation of the package as a user, utilizing a wheels distribution, is generally straightforward across most CPU/GPU architectures, including AS. Here are the steps:\n\nCheck for conda, install if missing.\nEstablish a clean Python virtual environment.\nActivate the newly created virtual environment.\nInstall the latest version of PyTorch.\nProceed with the installation of PyTorch Geometric.\nCheck that it imports without errors.\n\nThere’s a gist of what worked for me. The cell below uses the %load magic command to load the bash script into the code cell, followed by running it with insertion of the %%bash magic command at the top of the code block.\n\n%%bash\n\n# %load https://gist.githubusercontent.com/project-delphi/38d1db47ed28dde3c8418d5f435c865c/raw/665c079a1f6eff72207309ed97dd6b49194df812/pyg_user_install.sh\n# set variables here\nDIR=\"$HOME/Code/throwaway/pytorch-geometric-user-install\"\nPYTHON_VERSION=3.11\nRECENT_TORCH_VERSION=2.2.0\n\n# install miniconda for apple silicon, if not already installed\nif [ -d \"$HOME/anaconda3\" ] || [ -d \"$HOME/miniconda3\" ]\nthen\n    echo \"Conda is installed\"\nelse\n    curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n    sh Miniconda3-latest-MacOSX-arm64.sh -b -u  &gt; /dev/null 2&gt;&1\nfi\n\nmkdir -p \"$DIR\"\ncd \"$DIR\"\nconda create --yes  -p $DIR/.venv python=$PYTHON_VERSION &gt; /dev/null 2&gt;&1\neval \"$(conda shell.bash hook)\"\nconda activate $DIR/.venv\npip install -q --upgrade pip\n\n##### TORCH BUILD AND INSTALL ON M1, to use GPUs #####\npip install -q numpy # to remove user warning with torch install\npip install -q mpmath==1.3.0 # bugfix\nxcode-select --install  &gt; /dev/null 2&gt;&1 # if xcode not installed\n\n###### install torch ######\npip install -q --pre torch==$RECENT_TORCH_VERSION torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu \n\n# install torch geometric\npip install -q torch-geometric\n\n# check\npython --version\npython -c \"import torch; print(f'torch version: {torch.__version__}')\"\npython -c \"import torch_geometric as pyg; print(f'torch geometric version: {pyg.__version__}')\"\n\nConda is installed\nPython 3.11.8\ntorch version: 2.2.0\ntorch geometric version: 2.5.1\n\n\nWhich is great for using torch_geometric on my M1 Pro.\nThe problem is that I want to develop (and not just use) locally - that is to have an editable local install on my 2021 16 inch Macbook Pro M1 Pro. Sadly the M1 architecture does not have official support by pyg-team 😔 (I suspect the issues observed also apply to later Apple Silicon too.)\nLet’s see how far we can get with the installation process."
  },
  {
    "objectID": "posts/developing-pytorch-geometric-on-m1/index.html#developer-installation",
    "href": "posts/developing-pytorch-geometric-on-m1/index.html#developer-installation",
    "title": "Developing Pytorch Geometric on M1 Apple Silicon",
    "section": "Developer Installation",
    "text": "Developer Installation\n\nNon Apple Silicon Machines\nFrom the project contributing guidelines, the instructions are clear.\n\nInstall a recent version of PyTorch\nOptionally install some dependencies if changes make use of them\nBe sure to uninstall the pytorch_geometric package\nClone the repo\nRun an editable install command for the repo\nRun pytest: local testing is kind of essential for a developer install.\n\nDevelop away - (as long as it’s a supported architecture).\n\n\nM1, Possibly All Apple Silicon\nThis M1 hardware problem for developers has been noted.\nFor developers, if feature development on pytorch_geometric makes use of the listed package dependencies, several M1/AS issues have been raised.\n\npyg-lib\npytorch scatter\npytorch sparse\npytorch cluster\n\n(It’s likely that the package torch-spline-conv, another package dependency, is also an issue for M1/AS users - though no issues mentioning this are given.)\nThese dependencies are being subsumed into other packages (for example torch.scatter); at some point they won’t be a problem.\n\nWhy is Apple Silicon a Problem?\nTo develop locally, I need a an editable install version of pytorch geometric. This editable install needs additional dependencies (for fuller developer functionality such as testing, type checking, compiling, linting and documentation) some of which depend on C++ extensions which are not compiled for the M1/AS architecture. The project founder (@rusty1s) has noted that M1 was not supported from the onset - when it wasn’t available on github actions, and there are no plans to support it now. Later Apple Silicon is supported, but developer build are variable.\n\n\nThe Solution\npyg-team suggested earlier that M1 users wanting the fuller editable version of the package can use the cmake & ninja build systems to create libraries and dependencies that target M1 - this will give a working modifiable install of pytorch geometric. Some OS and compiler flags need to be set.\nLet’s see if I can do this and get a development environment setup.\nWhat I’ll do is as follows:\n\ncheck the OS & Hardware\nmake sure to uninstall all versions of pytorch geometric for all locations.\ncreate and activate a clean Python virtual environment (seems that conda is the best way to go)\ninstall a specific version of PyTorch using conda, as recommended by Apple\nbuild dependencies for clang and macos on my M1 Pro\nbuild the editable install\n\n\nOS and Hardware\nLet’s start by listing my hardware:\n\n%%bash\necho \"Operating System: $(uname -s)\"\necho \"Hardware: $(uname -m)\"\necho \"macOS Version: $(sw_vers -productVersion)\"\necho \"Chipset: $(sysctl -n machdep.cpu.brand_string)\"\n\nOperating System: Darwin\nHardware: arm64\nmacOS Version: 14.4\nChipset: Apple M1 Pro\n\n\n\n\nEditable Developer Install\nI’ve put this gist together from responses in github issues raised relating to AS. It installs, but doesn’t pass all tests - it seems because the tests are composed from objects that don’t work well with Apple Silicon, rather than anything fundamentally broken in AS.\n\n%%bash\n# %load https://gist.githubusercontent.com/project-delphi/b3b5cc91386997ff882f0a3f04a4b89a/raw/6146ec6cf950c2eeb184e0da3e59ff6fdd69550a/pytorch_geometric_apple_silicon_developer_install.sh\n# set variables here\nDIR=\"$HOME/Code/throwaway/pytorch-geometric-developer-install\"\nPYTHON_VERSION=3.11\nRECENT_TORCH_VERSION=2.2.0\nGITHUB_USERNAME=\"project-delphi\"\nMIN_MACOSX_DEPLOYMENT_TARGET=$(sw_vers -productVersion)\n\n# install miniconda for apple silicon, if not already installed\nif [ ! -d \"$HOME/anaconda3\" ] && [ ! -d \"$HOME/miniconda3\" ]\nthen\n    echo \"installing conda...\"\n    curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n    sh Miniconda3-latest-MacOSX-arm64.sh -b -u  &gt; /dev/null 2&gt;&1\nfi\n\nmkdir -p \"$DIR\"\ncd \"$DIR\"\nconda create --yes  -p $DIR/.venv python=$PYTHON_VERSION  &gt; /dev/null 2&gt;&1\neval \"$(conda shell.bash hook)\"\nconda activate $DIR/.venv\n\npip install -q --upgrade pip\n\n##### TORCH BUILD AND INSTALL ON M1, to use GPUs #####\npip install -q numpy # to remove user warning with torch install\npip install -q mpmath==1.3.0 # bugfix\nxcode-select --install  &gt; /dev/null 2&gt;&1 # if xcode not installed\n\n###### install pytorch ######\npip install -q --pre torch==$RECENT_TORCH_VERSION torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n\n# install dev build dependencies\npip install -q cmake\npip install -q ninja wheel\npip install -q git+https://github.com/pyg-team/pyg-lib.git\nMACOSX_DEPLOYMENT_TARGET=$MIN_MACOSX_DEPLOYMENT_TARGET CC=clang CXX=clang++ python -m pip -q --no-cache-dir  install  torch-scatter\nMACOSX_DEPLOYMENT_TARGET=$MIN_MACOSX_DEPLOYMENT_TARGET CC=clang CXX=clang++ python -m pip -q --no-cache-dir  install  torch-sparse\nMACOSX_DEPLOYMENT_TARGET=$MIN_MACOSX_DEPLOYMENT_TARGET CC=clang CXX=clang++ python -m pip -q --no-cache-dir  install  torch-cluster\nMACOSX_DEPLOYMENT_TARGET=$MIN_MACOSX_DEPLOYMENT_TARGET CC=clang CXX=clang++ python -m pip -q --no-cache-dir  install  torch-spline-conv\n\n# clone the forked repository and rebase to original\ngit clone \"https://github.com/$GITHUB_USERNAME/pytorch_geometric.git\"  2&gt;/dev/null\ncd pytorch_geometric\nif ! git remote | grep -q 'upstream'; then\n    git remote add upstream \"https://github.com/pyg-team/pytorch_geometric\"\nfi\ngit fetch upstream  -q\ngit rebase upstream/master\n\n# build dev install\nMACOSX_DEPLOYMENT_TARGET=$MIN_MACOSX_DEPLOYMENT_TARGET CC=clang CXX=clang++ python -m pip install -q --no-cache-dir -e \".[dev,full]\"  #&gt; /dev/null 2&gt;&1\n\n# check\npython --version\npython -c \"import torch; print(f'torch version: {torch.__version__}')\"\npython -c \"import torch_geometric as pyg; print(f'torch geometric version: {pyg.__version__}')\"\n\nSo a kind of success. I can install a full developer version pytorch_geometric on my M1 Pro.\nHowever, we also need to check regarding testing (of which there are several undocumented flavours in the project).\nFor the standard set of tests, today I get this:\n\n%%bash\n# set variables here\nDIR=\"$HOME/Code/throwaway/pytorch-geometric-developer-install\"\ncd \"$DIR\"\neval \"$(conda shell.bash hook)\"\nconda activate $DIR/.venv\n# install missing packages needed for testing\npip install -q matplotlib-inline ipython\npytest -q --tb=no | tail -n 1\n\nFatal Python error: Segmentation fault\n\nCurrent thread 0x00000001f82fbac0 (most recent call first):\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/torch/_ops.py\", line 755 in __call__\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pyg_lib/partition/__init__.py\", line 35 in metis\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/pytorch_geometric/torch_geometric/testing/decorators.py\", line 224 in withMETIS\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/pytorch_geometric/test/distributed/test_dist_link_neighbor_loader.py\", line 140 in &lt;module&gt;\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/assertion/rewrite.py\", line 178 in exec_module\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 690 in _load_unlocked\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 1147 in _find_and_load_unlocked\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 1176 in _find_and_load\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 1204 in _gcd_import\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/importlib/__init__.py\", line 126 in import_module\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/pathlib.py\", line 584 in import_path\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/python.py\", line 520 in importtestmodule\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/python.py\", line 573 in _getobj\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/python.py\", line 315 in obj\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/python.py\", line 589 in _register_setup_module_fixture\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/python.py\", line 576 in collect\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/runner.py\", line 388 in collect\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/runner.py\", line 340 in from_call\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/runner.py\", line 390 in pytest_make_collect_report\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_callers.py\", line 102 in _multicall\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_manager.py\", line 119 in _hookexec\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_hooks.py\", line 501 in __call__\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/runner.py\", line 565 in collect_one_node\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 839 in _collect_one_node\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 976 in genitems\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 981 in genitems\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 981 in genitems\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 981 in genitems\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 981 in genitems\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 813 in perform_collect\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 349 in pytest_collection\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_callers.py\", line 102 in _multicall\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_manager.py\", line 119 in _hookexec\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_hooks.py\", line 501 in __call__\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 338 in _main\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 285 in wrap_session\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 332 in pytest_cmdline_main\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_callers.py\", line 102 in _multicall\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_manager.py\", line 119 in _hookexec\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_hooks.py\", line 501 in __call__\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/config/__init__.py\", line 174 in main\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/config/__init__.py\", line 197 in console_main\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/bin/pytest\", line 8 in &lt;module&gt;\n\nExtension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._flinalg, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, scipy.cluster._vq, scipy.cluster._hierarchy, scipy.cluster._optimal_leaf_ordering, yaml._yaml, numba.core.typeconv._typeconv, numba._helperlib, numba._dynfunc, numba._dispatcher, numba.core.runtime._nrt_python, numba.np.ufunc._internal, numba.experimental.jitclass._box, psutil._psutil_osx, psutil._psutil_posix, markupsafe._speedups, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, matplotlib._c_internal_utils, PIL._imaging, matplotlib._path, kiwisolver._cext, matplotlib._image, rdkit.rdBase (total: 116)\n\n\nA week ago I noticed that the tests were failing, and then a few days ago they were passing. Today they are not passing, due to new tests. This is caused by commit updates which don’t test on Apple Silicon.\nThis inconsistency is a problem for contributing to the project. As mentioned, @rusty1s has committed changes to the tests which accommodate Apple Silicon using his own device - but this is not a sustainable approach.\nThere’s not a good solution to variable pytest runs, since feature updates (commits) to the main line branch – which don’t build for Apple Silicon – are likely to break some test at least some of the time.\nIn some issues, @rusty1s mentioned a community effort to help test Apple Silicon builds, specifically the M1 architecture. I’d like to contribute, it’s just figuring out how to do so for the long term.\nElse, the path of least resistance is to move to the cloud - I’ve recently received cloud credit from major providers."
  },
  {
    "objectID": "posts/developing-pytorch-geometric-on-m1/index.html#more-on-testing",
    "href": "posts/developing-pytorch-geometric-on-m1/index.html#more-on-testing",
    "title": "Developing Pytorch Geometric on M1 Apple Silicon",
    "section": "More On Testing",
    "text": "More On Testing\nThese test related issues are not unique to the default testing for the package. The package has several kinds of tests, including: full, gpu, previous version, and nightly.\nI looked over the github actions workflows, I noticed that full testing and gpu testing are not set up for apple silicon and also have to install undeclared dependencies, such as graphviz and bugfix pinned versions of packages (e.g. mpmath==1.3.0). These minor issues could be useful to work on - even with current default testing this seems to be the case.\nI’m thinking of a Makefile to compose the different installation and testing steps into higher level portable grammars. This would be useful for the community, and also for me to use in the future.\nUpdate, @rusty1s suggested more specific testing for mps (and by implication AS), starting with test decorators. That and test documentation could be a good place to start."
  },
  {
    "objectID": "posts/developing-pytorch-geometric-on-m1/index.html#references",
    "href": "posts/developing-pytorch-geometric-on-m1/index.html#references",
    "title": "Developing Pytorch Geometric on M1 Apple Silicon",
    "section": "References",
    "text": "References\n\npytorch on Apple Metal\npyg-lib M1 issues\npytorch-scatter M1 issue\npytorch-spars M1 issues\npytorch-cluster M1 issues"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To Synthetic Musings",
    "section": "",
    "text": "This is the first blog entry by me Ravi (@project-delphi). Welcome!"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html",
    "href": "posts/reinforcement-learning/index.html",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "Reinforcement Learning (RL) is a branch of machine learning where an agent learns to make decisions by interacting with an environment. Unlike supervised learning, RL does not require labeled data; instead, it optimizes a reward function through exploration and exploitation. It’s lost some steam as a application and research area with Generative AI dominating these days.\nI’ve worked on Reinforcement Learning a few times for course exercises, but never had time to really dig into. I worked on making use on Meta’s Horizon platform for RL. The idea was to modify the UI and notifications in experiments conducted with users - that is each user receive a different version of the app, different UI and notifications. We didn’t get the go ahead to follow through from early stage investigation though.\n\n\nRL problems are typically modeled as Markov Decision Processes (MDPs), consisting of:\n\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nState (s)\nThe environment’s representation at a given time.\n\n\nAction (a)\nThe choice made by the agent.\n\n\nReward (r)\nA scalar feedback signal received after taking an action.\n\n\nPolicy (π)\nA strategy that maps states to actions.\n\n\nValue Function (V or Q)\nMeasures the expected return of being in a state or taking an action."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#core-components-of-rl",
    "href": "posts/reinforcement-learning/index.html#core-components-of-rl",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "RL problems are typically modeled as Markov Decision Processes (MDPs), consisting of:\n\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nState (s)\nThe environment’s representation at a given time.\n\n\nAction (a)\nThe choice made by the agent.\n\n\nReward (r)\nA scalar feedback signal received after taking an action.\n\n\nPolicy (π)\nA strategy that maps states to actions.\n\n\nValue Function (V or Q)\nMeasures the expected return of being in a state or taking an action."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#q-learning",
    "href": "posts/reinforcement-learning/index.html#q-learning",
    "title": "Reinforcement Learning",
    "section": "Q-Learning",
    "text": "Q-Learning\nQ-learning estimates the action-value function Q(s, a) and updates it using the Bellman equation:\n\\[\nQ(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n\\]\nwhere:\n\n\\(\\alpha\\) is the learning rate.\n\\(\\gamma\\) is the discount factor.\n\\(s'\\) and \\(a'\\) are the next state and action.\n\n\nWhat Does “Q” in Q-Learning Stand For?\nThe “Q” in Q-learning stands for “Quality”, representing the quality or value of taking a particular action ( a ) in a given state ( s ).\n\nDefinition of Q-Value\nThe Q-value function, ( Q(s, a) ), estimates the expected cumulative reward an agent will receive if it takes action ( a ) in state ( s ) and then follows an optimal policy thereafter. The function satisfies the Bellman equation:\n\\[\nQ(s, a) = \\mathbb{E} \\left[ r + \\gamma \\max_{a'} Q(s', a') \\right]\n\\]\nwhere: - \\(r\\) is the immediate reward, - \\(\\gamma\\) is the discount factor (determining how much future rewards are valued), - \\(s'\\) is the next state, - \\(a'\\) is the next action.\n\n\nQ-Learning Update Rule\nQ-learning iteratively updates ( Q(s, a) ) estimates using the Bellman equation:\n\\[\nQ(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n\\]\nwhere \\(\\alpha\\) is the learning rate.\n\n\nOptimal Policy from Q-Values\nThe optimal policy is derived by selecting the action with the highest Q-value:\n\\[ \\pi^*(s) = \\arg\\max_a Q(s, a) \\]\nThis means the agent chooses the action that maximizes the expected future reward.\n\n\nExample Code in Python\nBelow is a simple Q-learning update step in Python:\nimport numpy as np\n\n# Initialize Q-table\nQ = np.zeros((5, 2))  # 5 states, 2 actions\n\n# Parameters\nalpha = 0.1   # Learning rate\ngamma = 0.9   # Discount factor\n\n# Sample experience (state, action, reward, next_state)\ns, a, r, s_next = 0, 1, 10, 2\n\n# Q-learning update\nQ[s, a] = Q[s, a] + alpha * (r + gamma * np.max(Q[s_next, :]) - Q[s, a])\n\nprint(Q)\n\nExample: Q-Learning in Python\nimport numpy as np\nimport gym\n\ngamma = 0.99  # Discount factor\nalpha = 0.1   # Learning rate\nepsilon = 0.1  # Exploration rate\n\nenv = gym.make(\"FrozenLake-v1\", is_slippery=False)\nQ = np.zeros((env.observation_space.n, env.action_space.n))\n\nfor episode in range(1000):\n    state = env.reset()[0]\n    done = False\n    while not done:\n        action = np.argmax(Q[state]) if np.random.rand() &gt; epsilon else env.action_space.sample()\n        next_state, reward, done, _, _ = env.step(action)\n        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n        state = next_state\n\n\n\nTemporal Difference (TD) Learning\nTD methods update value estimates using a one-step lookahead:\n\\[\nV(s) \\leftarrow V(s) + \\alpha \\left[ r + \\gamma V(s') - V(s) \\right]\n\\]\nThis balances immediate and future rewards without requiring a full trajectory."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#policy-gradient-pg",
    "href": "posts/reinforcement-learning/index.html#policy-gradient-pg",
    "title": "Reinforcement Learning",
    "section": "Policy Gradient (PG)",
    "text": "Policy Gradient (PG)\nPolicy gradient methods use gradient ascent to maximize expected rewards:\n\\[\n\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)\n\\]\nwhere J(θ) is the expected return under the policy π_θ.\n\nDerivation of \\(J(\\theta)\\) in Policy Gradients\n\n1. Defining the Objective Function\nIn policy gradient methods, the objective function \\(J(\\theta)\\) represents the expected return (total reward) under the parameterized policy \\(\\pi_{\\theta}\\). It is derived from the expected cumulative reward an agent receives when following \\(\\pi_{\\theta}\\).\nWe define \\(J(\\theta)\\) as the expected return over all possible trajectories:\n\\[\nJ(\\theta) = \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\left[ R(\\tau) \\right]\n\\]\nwhere:\n\n\\(\\tau = (s_0, a_0, s_1, a_1, \\dots)\\) is a trajectory (a full sequence of states and actions),\n\\(p_{\\theta}(\\tau)\\) is the probability of a trajectory under policy \\(\\pi_{\\theta}\\),\n\\(R(\\tau)\\) is the total reward for a trajectory, usually defined as:\n\n\\[\nR(\\tau) = \\sum_{t=0}^{T} \\gamma^t r_t\n\\]\nwhere \\(r_t\\) is the reward at time \\(t\\), and \\(\\gamma\\) is the discount factor.\nUsing the chain rule, the trajectory probability can be written as:\n\\[ p_{\\theta}(\\tau) = p(s_0) \\prod_{t=0}^{T} \\pi_{\\theta}(a_t | s_t) p(s_{t+1} | s_t, a_t) \\]\nwhere:\n\n\\(p(s_0)\\) is the initial state distribution,\n\\(\\pi_{\\theta}(a_t | s_t)\\) is the policy,\n\\(p(s_{t+1} | s_t, a_t)\\) is the environment transition probability.\n\n\n\n2. Policy Gradient Theorem\nWe take the gradient of \\(J(\\theta)\\) to optimize the policy:\n\\[\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\left[ R(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) \\right]\n\\]\nUsing the log-derivative trick:\n\\[\n\\nabla_{\\theta} \\log p_{\\theta}(\\tau) = \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t)\n\\]\nwe obtain the final form:\n\\[ \\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau} \\left[ R(\\tau) \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) \\right] \\]\nThis means that we update the policy in the direction that increases the probability of high-reward actions.\n\n\n3. Practical Policy Gradient Update Rule\nThe policy parameters are updated using stochastic gradient ascent:\n\\[\n\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta} J(\\theta)\n\\]\nwhere \\(\\alpha\\) is the learning rate.\n\n\n4. Example Code in Python (REINFORCE Algorithm)\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define a simple policy network\nclass PolicyNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.fc = nn.Linear(state_dim, action_dim)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, state):\n        return self.softmax(self.fc(state))\n\n# Initialize policy, optimizer\npolicy = PolicyNetwork(state_dim=4, action_dim=2)\noptimizer = optim.Adam(policy.parameters(), lr=0.01)\n\n# Sample data\nlog_probs = []  # Store log π(a|s)\nrewards = []  # Store rewards\n\n# Compute policy gradient loss\nR = sum(rewards)  # Total return\npolicy_loss = -sum(log_prob * R for log_prob in log_probs)  # Gradient ascent\n\n# Backpropagate\noptimizer.zero_grad()\npolicy_loss.backward()\noptimizer.step()\n\n\n\n### Example: Policy Gradient with REINFORCE\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport gym\n\nenv = gym.make(\"CartPole-v1\")\npolicy = nn.Sequential(nn.Linear(4, 128), nn.ReLU(), nn.Linear(128, 2), nn.Softmax(dim=-1))\noptimizer = optim.Adam(policy.parameters(), lr=0.01)\n\ndef policy_gradient():\n    state = env.reset()[0]\n    rewards, log_probs = [], []\n    done = False\n    while not done:\n        state = torch.tensor(state, dtype=torch.float32)\n        action_probs = policy(state)\n        action = torch.multinomial(action_probs, 1).item()\n        log_prob = torch.log(action_probs[action])\n        state, reward, done, _, _ = env.step(action)\n        rewards.append(reward)\n        log_probs.append(log_prob)\n    return rewards, log_probs"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#actor-critic",
    "href": "posts/reinforcement-learning/index.html#actor-critic",
    "title": "Reinforcement Learning",
    "section": "Actor-Critic",
    "text": "Actor-Critic\nActor-Critic methods combine value-based and policy-based approaches: - Actor updates the policy π_θ. - Critic estimates the value function V(s) to reduce variance."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#next-steps",
    "href": "posts/reinforcement-learning/index.html#next-steps",
    "title": "Reinforcement Learning",
    "section": "Next Steps",
    "text": "Next Steps\nIf and when I get time, I would like to read more about Meta-RL and explore its applications. It seems like a fascinating area, although less popular these days, with generative models taking over. (Although RLHF on Large Language Models is a special application of RL).\nHere are some canonical papers that have shaped the field of reinforcement learning (RL) and are often considered foundational:\n\nTemporal Difference Learning (TD) • Title: Learning to Predict by the Methods of Temporal Differences • Authors: Richard S. Sutton • Year: 1988 • Summary: This paper introduces Temporal Difference (TD) learning, a core method that combines Monte Carlo methods and dynamic programming to estimate value functions for reinforcement learning tasks.\nQ-Learning • Title: Learning from Delayed Rewards • Authors: Christopher J.C.H. Watkins • Year: 1989 • Summary: Q-learning is introduced in this work, a model-free algorithm for learning the value of action-state pairs without needing a model of the environment. It’s a foundational algorithm for value-based RL methods.\nPolicy Gradient Methods • Title: Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning • Authors: Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour • Year: 2000 • Summary: This paper introduces policy gradient methods, which optimize policies directly by updating the policy parameters using the gradient of expected return, paving the way for better solutions in continuous action spaces.\nDeep Q-Networks (DQN) • Title: Human-level control through deep reinforcement learning • Authors: Volodymyr Mnih, Koray Kavukcuoglu, David Silver, et al. • Year: 2015 • Summary: DQN combines deep learning with Q-learning, using a neural network to approximate the Q-value function. This paper demonstrated human-level performance on several Atari games and marked a significant breakthrough for deep RL.\nAsynchronous Advantage Actor-Critic (A3C) • Title: Asynchronous Methods for Deep Reinforcement Learning • Authors: Volodymyr Mnih, Adrià Puigdomènech Badia, David Silver, et al. • Year: 2016 • Summary: A3C introduces a parallelized framework where multiple agents interact with the environment asynchronously, using actor-critic methods to stabilize learning and improve performance on complex tasks.\nProximal Policy Optimization (PPO) • Title: Proximal Policy Optimization Algorithms • Authors: John Schulman, Filip Wolski, Prafulla Dhariwal, et al. • Year: 2017 • Summary: PPO is introduced as an efficient and simpler alternative to TRPO (Trust Region Policy Optimization). It aims to balance exploration and exploitation by constraining how much the policy changes at each step, leading to stable training.\nAlphaGo • Title: Mastering the game of Go with deep neural networks and tree search • Authors: Silver, Hubert, Schrittwieser, et al. • Year: 2016 • Summary: This paper introduces AlphaGo, a system that combines deep learning, Monte Carlo tree search, and reinforcement learning to play Go at a superhuman level. It demonstrates the power of RL in combining planning and learning.\nMuZero • Title: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model • Authors: Silver, Singh, Precup, et al. • Year: 2020 • Summary: MuZero is a model-based approach that learns a model of the environment (dynamics) and uses it to plan actions. Unlike prior methods, MuZero learns the model itself rather than relying on a predefined one, achieving state-of-the-art performance in multiple domains.\nAlphaStar • Title: Grandmaster level in StarCraft II using multi-agent reinforcement learning • Authors: Vinyals, Babuschkin, et al. • Year: 2019 • Summary: AlphaStar is an RL agent trained to play the complex real-time strategy game StarCraft II. This paper outlines the challenges and methods used to train AlphaStar, including multi-agent learning and hierarchical reinforcement learning.\nModel-Based Reinforcement Learning • Title: World Models • Authors: David Ha, Jürgen Schmidhuber • Year: 2018 • Summary: World Models are introduced here, where an RL agent learns an internal model of the environment that allows it to perform well even with a limited number of real-world interactions, showing how model-based methods can significantly improve sample efficiency.\nTrust Region Policy Optimization (TRPO) • Title: Trust Region Policy Optimization • Authors: John Schulman, Sergey Levine, Philipp Moritz, et al. • Year: 2015 • Summary: TRPO addresses the challenge of large policy updates by introducing a constraint on the size of each update, thus ensuring the policy remains close to the old policy, which improves the stability of training in policy-based methods.\nA Survey of Deep Reinforcement Learning • Title: Deep Reinforcement Learning: An Overview • Authors: Yuxi Li • Year: 2017 • Summary: This is a comprehensive review paper on deep reinforcement learning, covering the evolution of deep RL algorithms, key ideas, and the challenges they aim to solve. It provides a useful resource for understanding the broad scope of deep RL research.\n\nThese papers cover the evolution of RL, from value-based methods like Q-learning to policy-based methods, and the integration of deep learning into RL, all the way to model-based approaches like MuZero. Together, they form the backbone of modern RL research.\nHave thoughts or questions? Contact me"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code and Plot",
    "section": "",
    "text": "Let’s see if blogging with code and plots works here:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(0, 10, 100)\n\nfig = plt.figure()\nplt.plot(x, np.sin(x), '-')\nplt.plot(x, np.cos(x), '--')\n\n\n\n\n\n\n\n\n\n\nkindly taken from Jake Vanderplas’s blog\n\nSuccess!"
  },
  {
    "objectID": "posts/markdown/index.html",
    "href": "posts/markdown/index.html",
    "title": "Markdown: What’s in the name?",
    "section": "",
    "text": "Markdown: What’s in the name?\nI’m giving a workshop on quarto - hence putting sometime into parsing the excellent quarto documentation - It’s of superb quality. It’s based on using a variant of markdown as a central part of the workflow. I’ve never thought deeply about why markdown is called that.\nThen I came across this gem in the quarto docs:\n\nA Markdown-formatted document should be publishable as-is, as plain text, without looking like it’s been marked up with tags or formatting instructions. – John Gruber\n\nI’ve always thought of markdown as a way to write text that can be easily converted to HTML. But this is a nice way to think about it.\nWonderful!"
  },
  {
    "objectID": "posts/clustering-tuning-metrics/index.html",
    "href": "posts/clustering-tuning-metrics/index.html",
    "title": "Hyperparameter Tuning in Clustering Algorithms: A Complete Guide",
    "section": "",
    "text": "Clustering Algorithms"
  },
  {
    "objectID": "posts/clustering-tuning-metrics/index.html#why-tune-hyperparameters-in-clustering",
    "href": "posts/clustering-tuning-metrics/index.html#why-tune-hyperparameters-in-clustering",
    "title": "Hyperparameter Tuning in Clustering Algorithms: A Complete Guide",
    "section": "Why Tune Hyperparameters in Clustering?",
    "text": "Why Tune Hyperparameters in Clustering?\nUnlike supervised learning, clustering lacks ground truth labels to guide optimization. Therefore, we rely on intrinsic metrics or external strategies to evaluate cluster quality. Proper hyperparameter tuning can:\n\nImprove cluster quality.\nReveal meaningful patterns in data.\nAdapt models to specific datasets."
  },
  {
    "objectID": "posts/clustering-tuning-metrics/index.html#key-hyperparameters-in-clustering-algorithms",
    "href": "posts/clustering-tuning-metrics/index.html#key-hyperparameters-in-clustering-algorithms",
    "title": "Hyperparameter Tuning in Clustering Algorithms: A Complete Guide",
    "section": "Key Hyperparameters in Clustering Algorithms",
    "text": "Key Hyperparameters in Clustering Algorithms\n\n\n\n\n\n\n\n\nAlgorithm\nKey Hyperparameters\nNotes\n\n\n\n\nK-Means\nn_clusters, init, max_iter\nWorks well with validation sets and intrinsic metrics.\n\n\nDBSCAN\neps, min_samples\nSensitive to density; validation can be tricky.\n\n\nAgglomerative\nn_clusters, linkage\nValidation works well due to global similarity.\n\n\nGMM\nn_components, covariance_type\nLog-likelihood, BIC/AIC are helpful on validation data.\n\n\nSpectral\nn_clusters, affinity\nNeeds full affinity matrix; splitting data can distort.\n\n\nOPTICS\neps, min_samples\nChallenges arise with varied densities."
  },
  {
    "objectID": "posts/clustering-tuning-metrics/index.html#training-validation-and-test-data",
    "href": "posts/clustering-tuning-metrics/index.html#training-validation-and-test-data",
    "title": "Hyperparameter Tuning in Clustering Algorithms: A Complete Guide",
    "section": "Training, Validation, and Test Data",
    "text": "Training, Validation, and Test Data\n\nWhat is Validation Data?\nValidation data is a subset of the dataset used to select hyperparameters. It acts as an intermediate step, ensuring test data remains untouched during tuning. This is especially useful for larger datasets.\n\n\nWhen to Use Validation Data\nValidation sets are most effective when:\n\nThe dataset is large enough to split into training, validation, and test subsets.\nThe clustering algorithm supports evaluation metrics that work on subsets.\nHyperparameter tuning involves a search space requiring multiple iterations.\n\n\n\nWhy Not Test Data?\nUsing test data for tuning can result in overfitting and biased performance estimates. Always keep the test set as a final evaluation step."
  },
  {
    "objectID": "posts/clustering-tuning-metrics/index.html#evaluation-metrics-for-clustering",
    "href": "posts/clustering-tuning-metrics/index.html#evaluation-metrics-for-clustering",
    "title": "Hyperparameter Tuning in Clustering Algorithms: A Complete Guide",
    "section": "Evaluation Metrics for Clustering",
    "text": "Evaluation Metrics for Clustering\n\nIntrinsic Metrics (No ground truth needed):\n\nSilhouette Score: Measures cluster cohesion and separation.\nInertia (Sum of Squared Distances): Evaluates compactness (specific to K-Means).\nDavies-Bouldin Index: A lower value indicates better-defined clusters.\n\n\n\nExtrinsic Metrics (Require ground truth):\n\nAdjusted Rand Index (ARI): Measures similarity between predicted and true labels.\nNormalized Mutual Information (NMI): Captures mutual dependence between clusters and labels."
  },
  {
    "objectID": "posts/clustering-tuning-metrics/index.html#code-example-tuning-k-means",
    "href": "posts/clustering-tuning-metrics/index.html#code-example-tuning-k-means",
    "title": "Hyperparameter Tuning in Clustering Algorithms: A Complete Guide",
    "section": "Code Example: Tuning K-Means",
    "text": "Code Example: Tuning K-Means\nHere’s how to tune n_clusters using a validation set:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.model_selection import train_test_split\n\n# Generate synthetic data\ndata, _ = make_blobs(n_samples=500, centers=4, cluster_std=0.6, random_state=42)\n\n# Split data: 70% train, 15% validation, 15% test\nX_train, X_temp = train_test_split(data, test_size=0.3, random_state=42)\nX_val, X_test = train_test_split(X_temp, test_size=0.5, random_state=42)\n\n# Hyperparameter tuning\nn_clusters = [2, 3, 4, 5]\nbest_n_cluster = None\nbest_score, best_model = -1, None\n\nfor n in n_clusters:\n    kmeans = KMeans(n_clusters=n, random_state=42)\n    kmeans.fit(X_train)\n    labels_val = kmeans.predict(X_val)\n    score = silhouette_score(X_val, labels_val)\n    print(f\"Clusters: {n}, Validation Silhouette Score: {score:.2f}\")\n\n    if score &gt; best_score:\n        best_score = score\n        best_model = kmeans\n        best_n_cluster = n\n\n# Evaluate on test data\nlabels_test = best_model.predict(X_test)\ntest_score = silhouette_score(X_test, labels_test)\nprint(f\"Test Silhouette Score: {test_score:.2f}\")\nprint(f\"Best Number of Clusters: {best_n_cluster}\")"
  },
  {
    "objectID": "posts/clustering-tuning-metrics/index.html#what-about-algorithms-sensitive-to-validation-data",
    "href": "posts/clustering-tuning-metrics/index.html#what-about-algorithms-sensitive-to-validation-data",
    "title": "Hyperparameter Tuning in Clustering Algorithms: A Complete Guide",
    "section": "What About Algorithms Sensitive to Validation Data?",
    "text": "What About Algorithms Sensitive to Validation Data?\nNot all clustering algorithms work well with validation data:\n\nDBSCAN/OPTICS:\n\nDensity-based algorithms may struggle because density assumptions vary between training and validation sets.\nWorkaround: Use subsets of training data for validation instead of separate splits.\n\nSpectral Clustering:\n\nNeeds the full affinity matrix, which can be distorted when splitting the data.\nWorkaround: Use cross-validation techniques with the full dataset."
  },
  {
    "objectID": "posts/clustering-tuning-metrics/index.html#cross-validation-for-clustering",
    "href": "posts/clustering-tuning-metrics/index.html#cross-validation-for-clustering",
    "title": "Hyperparameter Tuning in Clustering Algorithms: A Complete Guide",
    "section": "Cross-Validation for Clustering",
    "text": "Cross-Validation for Clustering\nWhen splitting the dataset is impractical, use cross-validation for robust hyperparameter tuning. Here’s an example for K-Means:\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import silhouette_score\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\n\nfor train_idx, val_idx in kf.split(data):\n    X_train, X_val = data[train_idx], data[val_idx]\n    kmeans = KMeans(n_clusters=3, random_state=42).fit(X_train)\n    labels = kmeans.predict(X_val)\n    scores.append(silhouette_score(X_val, labels))\n\nprint(f\"Average Validation Score: {sum(scores)/len(scores):.2f}\")"
  },
  {
    "objectID": "posts/clustering-tuning-metrics/index.html#key-takeaways",
    "href": "posts/clustering-tuning-metrics/index.html#key-takeaways",
    "title": "Hyperparameter Tuning in Clustering Algorithms: A Complete Guide",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nUse validation sets for tuning K-Means, Agglomerative Clustering, and GMM.\nCross-validation or intrinsic metrics are alternatives when validation sets are impractical (e.g., DBSCAN, Spectral Clustering).\nAlways evaluate on an untouched test set for unbiased performance estimates.\n\n\n\nQuestions for Reflection\n\nShould validation sets always be used in clustering, or are intrinsic metrics sufficient in some cases?\nHow can cross-validation improve hyperparameter tuning for clustering?\nWhat strategies work best when clusters are imbalanced?\n\nBy following these guidelines, you can achieve robust and meaningful clustering results while avoiding common pitfalls in hyperparameter tuning. Happy clustering!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Synthetic Musings",
    "section": "",
    "text": "Featured Writing\n\n\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\nFirst Job in Industry\n\n\n1 min\n\n\n\n\n\n\n\nFeb 7, 2025\n\n\nPython Books I Recommend\n\n\n2 min\n\n\n\n\n\n\n\nFeb 6, 2025\n\n\nReinforcement Learning\n\n\n12 min\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\nPython Method Naming Conventions: Best Practices for Class Design\n\n\n4 min\n\n\n\n\n\n\n\nFeb 3, 2025\n\n\nBuilding a Simple Vector Database in Python\n\n\n5 min\n\n\n\n\n\n\n\nFeb 2, 2025\n\n\nWhy Use Conda for Scientific Computing?\n\n\n3 min\n\n\n\n\n\n\n\nJan 31, 2025\n\n\nUnderstanding Attention: From Theory to Implementation\n\n\n4 min\n\n\n\n\n\n\n\nJan 27, 2025\n\n\nHyperparameter Tuning in Clustering Algorithms: A Complete Guide\n\n\n5 min\n\n\n\n\n\n\n\nJan 21, 2025\n\n\nMarkdown: What’s in the name?\n\n\n1 min\n\n\n\n\n\n\n\nSep 21, 2024\n\n\nMinimal Data Inspection Before Splitting The Dataset\n\n\n5 min\n\n\n\n\n\n\n\nSep 21, 2024\n\n\nsklearn vs R model APIs\n\n\n14 min\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nTrain Dev Test Data Splits\n\n\n10 min\n\n\n\n\n\n\n\nSep 19, 2024\n\n\nModel Speciation\n\n\n7 min\n\n\n\n\n\n\n\nApr 22, 2024\n\n\nGit Repository Search\n\n\n5 min\n\n\n\n\n\n\n\nApr 11, 2024\n\n\nSparsity with PyTorch Tensors\n\n\n9 min\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nDeveloping Pytorch Geometric on M1 Apple Silicon\n\n\n7 min\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nPost With Code and Plot\n\n\n1 min\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nWelcome To Synthetic Musings\n\n\n1 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/attention/index.html",
    "href": "posts/attention/index.html",
    "title": "Understanding Attention: From Theory to Implementation",
    "section": "",
    "text": "Attention\nMade with ❤️ and GitHub Copilot"
  },
  {
    "objectID": "posts/attention/index.html#overview",
    "href": "posts/attention/index.html#overview",
    "title": "Understanding Attention: From Theory to Implementation",
    "section": "Overview",
    "text": "Overview\nThis article provides a structured mathematical explanation of attention mechanisms in deep learning, focusing on their application in transformer architectures. We’ll explore how sequences are processed through attention layers and understand the mathematical foundations of these powerful neural network components."
  },
  {
    "objectID": "posts/attention/index.html#key-concepts",
    "href": "posts/attention/index.html#key-concepts",
    "title": "Understanding Attention: From Theory to Implementation",
    "section": "1 Key Concepts",
    "text": "1 Key Concepts\nBefore diving into the mathematics, let’s establish our key concepts:\n\nAttention: A mechanism allowing models to focus on relevant parts of input data\nSelf-Attention: A specific form where each element in a sequence attends to all others\nMulti-Head Attention: Multiple parallel attention mechanisms working together\nPositional Encoding: Method to incorporate sequential information"
  },
  {
    "objectID": "posts/attention/index.html#data-preprocessing",
    "href": "posts/attention/index.html#data-preprocessing",
    "title": "Understanding Attention: From Theory to Implementation",
    "section": "2 Data Preprocessing",
    "text": "2 Data Preprocessing\n\n2.1 From Text to Vectors\nThe transformation of text into numerical representations involves several steps:\n\nTokenization: Convert text into token IDs\nEmbedding: Map tokens to dense vectors\nPosition Encoding: Add sequential information\n\nLet’s examine each step in detail.\n\n\n2.2 Tokenization Process\nGiven an input sentence:\n\"The cat sat on the mat\"\nWe convert it to token IDs using subword tokenization:\ntokens = [101, 2023, 3679, 2003, 2307, 102]  # Example IDs\n\n\n2.3 Embedding Layer\nThe embedding process transforms discrete tokens into continuous vectors:\n\\[\nE \\in \\mathbb{R}^{V \\times d}\n\\]\nwhere:\n\n\\(V\\) = vocabulary size\n\\(d\\) = embedding dimension\n\nFor each token \\(t_i\\), we compute:\n\\[\nx_i = E[t_i] \\in \\mathbb{R}^d\n\\]\nResulting in input matrix:\n\\[\nX = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix} \\in \\mathbb{R}^{n \\times d}\n\\]\n\n\n2.4 Positional Encoding\nTo preserve sequence order, we add positional encodings:\n\\[\n\\begin{aligned}\nP_{i,2j} &= \\sin\\left(\\frac{i}{10000^{2j/d}}\\right) \\\\\nP_{i,2j+1} &= \\cos\\left(\\frac{i}{10000^{2j/d}}\\right)\n\\end{aligned}\n\\]\nFinal input representation:\n\\[\nX_{\\text{final}} = X + P\n\\]"
  },
  {
    "objectID": "posts/attention/index.html#self-attention-mechanism",
    "href": "posts/attention/index.html#self-attention-mechanism",
    "title": "Understanding Attention: From Theory to Implementation",
    "section": "3 Self-Attention Mechanism",
    "text": "3 Self-Attention Mechanism\n\n3.1 Query, Key, and Value Transformations\nSelf-attention begins by creating three matrices from the input:\n\\[\n\\begin{aligned}\nQ &= X W_Q \\\\\nK &= X W_K \\\\\nV &= X W_V\n\\end{aligned}\n\\]\nwhere \\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) are learnable parameters.\n\n\n3.2 Attention Computation\n\nCompute Attention Scores:\n\\[\nS = \\frac{Q K^T}{\\sqrt{d}}\n\\]\nApply Softmax:\n\\[\nA = \\text{softmax}(S)\n\\]\nCompute Weighted Values:\n\\[\nZ = A V\n\\]"
  },
  {
    "objectID": "posts/attention/index.html#multi-head-attention",
    "href": "posts/attention/index.html#multi-head-attention",
    "title": "Understanding Attention: From Theory to Implementation",
    "section": "4 Multi-Head Attention",
    "text": "4 Multi-Head Attention\n\n4.1 Parallel Attention Heads\nFor \\(h\\) heads, we compute:\n\\[\n\\begin{aligned}\nQ^{(h)} &= X W_Q^{(h)} \\\\\nK^{(h)} &= X W_K^{(h)} \\\\\nV^{(h)} &= X W_V^{(h)}\n\\end{aligned}\n\\]\n\n\n4.2 Head Outputs\nEach head produces its output:\n\\[\nZ^{(h)} = \\text{softmax} \\left( \\frac{Q^{(h)} (K^{(h)})^T}{\\sqrt{d_k}} \\right) V^{(h)}\n\\]\n\n\n4.3 Combining Head Outputs\n\nConcatenate:\n\\[\nH_{\\text{concat}} = [Z^{(1)} \\| Z^{(2)} \\| \\cdots \\| Z^{(H)}]\n\\]\nProject:\n\\[\nH_{\\text{output}} = H_{\\text{concat}} W_O\n\\]"
  },
  {
    "objectID": "posts/attention/index.html#training-process",
    "href": "posts/attention/index.html#training-process",
    "title": "Understanding Attention: From Theory to Implementation",
    "section": "5 Training Process",
    "text": "5 Training Process\n\n5.1 Gradient Descent Updates\nThe attention weights are updated using:\n\\[\n\\begin{aligned}\nW_Q &\\leftarrow W_Q - \\eta \\frac{\\partial L}{\\partial W_Q} \\\\\nW_K &\\leftarrow W_K - \\eta \\frac{\\partial L}{\\partial W_K} \\\\\nW_V &\\leftarrow W_V - \\eta \\frac{\\partial L}{\\partial W_K}\n\\end{aligned}\n\\]\n\n\n5.2 Optimization Strategy\n\nUse Adam optimizer for stable training\nApply gradient clipping to prevent exploding gradients\nImplement learning rate warmup"
  },
  {
    "objectID": "posts/attention/index.html#implementation-considerations",
    "href": "posts/attention/index.html#implementation-considerations",
    "title": "Understanding Attention: From Theory to Implementation",
    "section": "6 Implementation Considerations",
    "text": "6 Implementation Considerations\nWhen implementing attention mechanisms, consider:\n\nMemory efficiency\nNumerical stability\nParallelization opportunities\nAttention masking for padding"
  },
  {
    "objectID": "posts/attention/index.html#practical-example",
    "href": "posts/attention/index.html#practical-example",
    "title": "Understanding Attention: From Theory to Implementation",
    "section": "7 Practical Example",
    "text": "7 Practical Example\nimport torch\nimport torch.nn as nn\n\nclass SelfAttention(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_o = nn.Linear(d_model, d_model)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n\n        # Linear projections\n        q = self.w_q(x).view(batch_size, -1, self.n_heads, self.d_k)\n        k = self.w_k(x).view(batch_size, -1, self.n_heads, self.d_k)\n        v = self.w_v(x).view(batch_size, -1, self.n_heads, self.d_k)\n\n        # Transpose for attention computation\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        # Compute attention scores\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n        attn = F.softmax(scores, dim=-1)\n\n        # Apply attention to values\n        out = torch.matmul(attn, v)\n\n        # Reshape and project\n        out = out.transpose(1, 2).contiguous()\n        out = out.view(batch_size, -1, self.d_model)\n        return self.w_o(out)"
  },
  {
    "objectID": "posts/first-industry-job/index.html",
    "href": "posts/first-industry-job/index.html",
    "title": "First Job in Industry",
    "section": "",
    "text": "The First Job\n\n\nI was recently asked by a friend to speak to new graduates about how to land their first job in industry. It’s a time of trepidation and excitement, and I remember it well. I put together these slides, maybe they will help you, or someone you know."
  },
  {
    "objectID": "posts/naming-python-classes-methods/index.html",
    "href": "posts/naming-python-classes-methods/index.html",
    "title": "Python Method Naming Conventions: Best Practices for Class Design",
    "section": "",
    "text": "Python Method Naming Conventions"
  },
  {
    "objectID": "posts/naming-python-classes-methods/index.html#instance-methods-self",
    "href": "posts/naming-python-classes-methods/index.html#instance-methods-self",
    "title": "Python Method Naming Conventions: Best Practices for Class Design",
    "section": "1. Instance Methods (self)",
    "text": "1. Instance Methods (self)\nPurpose: Operate on an instance of the class, modifying attributes or performing actions specific to an object.\n\nNaming: Use snake_case (e.g., train_model, predict).\nExample:\nclass Model:\n    def train(self, data):  # Instance method\n        pass\n\n\nUsing _underscore in Instance Methods\n\n_private_method(): A single underscore signals an internal (protected) method, not meant for public use.\nclass Model:\n    def _preprocess(self, data):  # Internal method\n        pass\n__double_underscore_method(): A double underscore triggers name-mangling, making it harder to override in subclasses.\nclass Model:\n    def __compute_loss(self, data):  # Name-mangled method\n        pass"
  },
  {
    "objectID": "posts/naming-python-classes-methods/index.html#class-methods-cls",
    "href": "posts/naming-python-classes-methods/index.html#class-methods-cls",
    "title": "Python Method Naming Conventions: Best Practices for Class Design",
    "section": "2. Class Methods (cls)",
    "text": "2. Class Methods (cls)\nPurpose: Operate on the class itself, rather than on individual instances.\n\nNaming: Use snake_case (e.g., from_config, load_from_checkpoint).\nDecorator: @classmethod\nExample:\nclass Model:\n    @classmethod\n    def from_config(cls, config):  # Class method\n        return cls(**config)"
  },
  {
    "objectID": "posts/naming-python-classes-methods/index.html#static-methods-no-self-or-cls",
    "href": "posts/naming-python-classes-methods/index.html#static-methods-no-self-or-cls",
    "title": "Python Method Naming Conventions: Best Practices for Class Design",
    "section": "3. Static Methods (No self or cls)",
    "text": "3. Static Methods (No self or cls)\nPurpose: Utility functions that don’t modify the instance or class.\n\nNaming: Use snake_case (e.g., normalize_data, sigmoid).\nDecorator: @staticmethod\nExample:\nclass Model:\n    @staticmethod\n    def sigmoid(x):  # Static method\n        return 1 / (1 + np.exp(-x))"
  },
  {
    "objectID": "posts/naming-python-classes-methods/index.html#helper-functions-inside-or-outside-class",
    "href": "posts/naming-python-classes-methods/index.html#helper-functions-inside-or-outside-class",
    "title": "Python Method Naming Conventions: Best Practices for Class Design",
    "section": "4. Helper Functions (Inside or Outside Class)",
    "text": "4. Helper Functions (Inside or Outside Class)\nPurpose: Internal-use methods that aid class functionality.\n\nNaming: Use **_single_underscore** if internal (e.g., _compute_gradient).\nExample (Inside Class):\nclass Model:\n    def _compute_gradient(self, x):  # Helper method\n        return x * 0.1\nExample (Outside Class, Standalone Function):\ndef compute_loss(y_true, y_pred):  # Standalone helper function\n    return ((y_true - y_pred) ** 2).mean()"
  },
  {
    "objectID": "posts/naming-python-classes-methods/index.html#dunder-double-underscore-methods",
    "href": "posts/naming-python-classes-methods/index.html#dunder-double-underscore-methods",
    "title": "Python Method Naming Conventions: Best Practices for Class Design",
    "section": "5. Dunder (Double Underscore) Methods",
    "text": "5. Dunder (Double Underscore) Methods\nPurpose: Customize built-in behaviors (__init__, __call__, __getitem__, etc.).\n\nNaming: Always use double underscores before and after (e.g., __call__, __repr__).\nExample:\nclass Model:\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self, x):  # Makes object callable\n        return x * 2\n\n    def __repr__(self):\n        return f\"Model(name={self.name})\""
  },
  {
    "objectID": "posts/naming-python-classes-methods/index.html#summary-table",
    "href": "posts/naming-python-classes-methods/index.html#summary-table",
    "title": "Python Method Naming Conventions: Best Practices for Class Design",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\n\nMethod Type\nNaming Style\nExample\nNotes\n\n\n\n\nInstance Method\nsnake_case\ntrain(self, data)\nUses self\n\n\nPrivate Instance Method\n_underscore\n_preprocess(self, data)\nConventionally private\n\n\nName-Mangled Method\n__double_underscore\n__compute_loss(self, data)\nAvoid unless needed\n\n\nClass Method\nsnake_case\nfrom_config(cls, config)\nUses @classmethod\n\n\nStatic Method\nsnake_case\nsigmoid(x)\nUses @staticmethod\n\n\nHelper Function\nsnake_case\n_compute_gradient(x)\nInternal use\n\n\nDunder Method\n__double_underscore__\n__call__(self, x)\nBuilt-in behavior"
  },
  {
    "objectID": "posts/naming-python-classes-methods/index.html#conclusion",
    "href": "posts/naming-python-classes-methods/index.html#conclusion",
    "title": "Python Method Naming Conventions: Best Practices for Class Design",
    "section": "Conclusion",
    "text": "Conclusion\nUsing consistent naming conventions for Python methods enhances readability and maintainability. By following these best practices, you can write cleaner, more understandable object-oriented code. 🚀\nWould you like to see an expanded example of a full class using these principles? Let me know! 😊"
  },
  {
    "objectID": "posts/python-books/index.html",
    "href": "posts/python-books/index.html",
    "title": "Python Books I Recommend",
    "section": "",
    "text": "books on Python"
  },
  {
    "objectID": "posts/python-books/index.html#why-read-books-to-learn-python",
    "href": "posts/python-books/index.html#why-read-books-to-learn-python",
    "title": "Python Books I Recommend",
    "section": "Why Read Books to Learn Python?",
    "text": "Why Read Books to Learn Python?\nThere is so much you can learn about python from tutorials, videos and online courses. However, few resources can match the depth and structure of a well-written book. Books provide a curated learning path, continuity, and depth that can help solidify your understanding of Python."
  },
  {
    "objectID": "posts/python-books/index.html#choosing-the-right-python-book",
    "href": "posts/python-books/index.html#choosing-the-right-python-book",
    "title": "Python Books I Recommend",
    "section": "Choosing the Right Python Book",
    "text": "Choosing the Right Python Book\nThere’s also how to approach learning and improving with Python.\n\nBeginner-Friendly Books\n\nAutomate the Boring Stuff with Python – Great for practical automation projects. I found this too long and old, at times just boring.\nPython Crash Course – Covers fundamentals with hands-on projects. It’s not exciting, but helps to get feet wets.\nThink Python – Focuses on computational thinking. Very cool mathematical and academic approach.\n\n\n\nIntermediate Books\n\nFluent Python – A deep dive into Pythonic coding. This is a wonderful book, detailed but balanced so as not to overwhelm.\nEffective Python – 90+ tips for writing better Python code. Great tips, some would now be controversial.\n\n\n\nAdvanced Books\n\nPython Cookbook – A collection of real-world problem-solving recipes. Huge, but very useful. I use it more to dip into.\nHigh Performance Python – Focuses on optimization and efficiency. I’m reading this now.\nMastering Python Design Patterns – Covers best practices in software design."
  },
  {
    "objectID": "posts/python-books/index.html#strategies-for-effective-learning",
    "href": "posts/python-books/index.html#strategies-for-effective-learning",
    "title": "Python Books I Recommend",
    "section": "Strategies for Effective Learning",
    "text": "Strategies for Effective Learning\nWhat has worked for me is a combination of reading, practice, and community engagement. Here are some strategies to enhance your Python learning journey:\n\n1. Read Actively, Not Passively\nInstead of just reading, type out examples, tweak them, and experiment with code.\n\n\n2. Take Notes and Summarize\nJot down key concepts, common pitfalls, and important insights.\n\n\n3. Work on Projects\nApply concepts by building small projects or solving coding challenges.\n\n\n4. Teach What You Learn\nExplaining concepts to others solidifies understanding.\n\n\n5. Join a Community\nParticipate in forums (like slack and discord) or study groups to discuss concepts."
  },
  {
    "objectID": "posts/python-books/index.html#final-thoughts",
    "href": "posts/python-books/index.html#final-thoughts",
    "title": "Python Books I Recommend",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nBooks provide an excellent foundation for learning Python, but pairing reading with hands-on coding and real-world projects will maximize your understanding. Choose a book that fits your level, read actively, and practice consistently.\nWhat are your favorite Python books? Drop me a line and let me know!"
  },
  {
    "objectID": "posts/vector-databases-and-rag/index.html",
    "href": "posts/vector-databases-and-rag/index.html",
    "title": "Building a Simple Vector Database in Python",
    "section": "",
    "text": "vector database\nMade with ❤️ and GitHub Copilot"
  },
  {
    "objectID": "posts/vector-databases-and-rag/index.html#introduction",
    "href": "posts/vector-databases-and-rag/index.html#introduction",
    "title": "Building a Simple Vector Database in Python",
    "section": "1 Introduction",
    "text": "1 Introduction\nVector databases store high-dimensional numerical vectors and enable fast similarity search. They are widely used in Retrieval-Augmented Generation (RAG) systems, recommendation engines, and computer vision applications.\nThis post covers the fundamentals of vector databases and a discussion of RAG. It provides a Python implementation using FAISS, a popular library for fast similarity search."
  },
  {
    "objectID": "posts/vector-databases-and-rag/index.html#what-is-a-vector-database",
    "href": "posts/vector-databases-and-rag/index.html#what-is-a-vector-database",
    "title": "Building a Simple Vector Database in Python",
    "section": "2 What is a Vector Database?",
    "text": "2 What is a Vector Database?\nA vector database is optimized for storing and querying high-dimensional vectors efficiently. Unlike traditional databases that use structured queries, vector databases retrieve data using similarity measures like:\n\nCosine Similarity (angle between vectors)\nEuclidean Distance (L2 norm)\nInner Product (dot product)\n\n\n2.1 Use Cases\n\nRetrieval-Augmented Generation (RAG): Enhancing LLM responses\nRecommendation Systems: Finding similar users or products\nImage & Video Search: Searching by content rather than metadata"
  },
  {
    "objectID": "posts/vector-databases-and-rag/index.html#what-is-retrieval-augmented-generation-rag",
    "href": "posts/vector-databases-and-rag/index.html#what-is-retrieval-augmented-generation-rag",
    "title": "Building a Simple Vector Database in Python",
    "section": "3 What is Retrieval-Augmented Generation (RAG)?",
    "text": "3 What is Retrieval-Augmented Generation (RAG)?\nRetrieval-Augmented Generation (RAG) is an AI framework that combines retrieval-based search with generative models to improve the quality and accuracy of text generation. Instead of relying only on a pre-trained language model’s internal knowledge, RAG dynamically fetches relevant information from external sources (like a database, vector store, or web search) to generate more informed and up-to-date responses.\nHow RAG Works\n\nRetrieval Step\n\nGiven a query, the system retrieves relevant documents from an external knowledge base (e.g., a vector database like FAISS, Pinecone, or a search engine).\nCommon retrieval methods include dense vector search (e.g., using embeddings from transformers like BERT or OpenAI embeddings) and keyword search.\n\nAugmentation Step\n\nThe retrieved documents are provided as additional context to a large language model (LLM).\nThis allows the model to generate responses based on both its pre-trained knowledge and real-time, external information.\n\nGeneration Step\n\nThe LLM synthesizes an answer, incorporating the retrieved knowledge while ensuring coherence and fluency.\n\n\nBenefits of RAG\n\nMore Accurate & Up-to-Date: Retrieves real-time or domain-specific knowledge, reducing hallucinations.\nInterpretable: Users can see the sources used for generating responses.\nEfficient: Allows smaller models to perform better by offloading factual knowledge to retrieval systems.\n\nUse Cases\n\nChatbots & Virtual Assistants: Improved customer support with company-specific knowledge.\nEnterprise Search: Querying internal documents dynamically.\nMedical & Legal AI: Ensuring responses are based on authoritative sources."
  },
  {
    "objectID": "posts/vector-databases-and-rag/index.html#how-are-best-matches-found-for-a-query",
    "href": "posts/vector-databases-and-rag/index.html#how-are-best-matches-found-for-a-query",
    "title": "Building a Simple Vector Database in Python",
    "section": "4 How Are Best Matches Found for a Query?",
    "text": "4 How Are Best Matches Found for a Query?\nThe best matches for a given query vector are found using nearest neighbor search (NNS) techniques. The most common method is k-nearest neighbors (k-NN), which identifies the top-k closest vectors in the database based on a similarity metric.\nFor large-scale search, approximate methods like Hierarchical Navigable Small World (HNSW) and Product Quantization (PQ) can be used to improve efficiency while maintaining accuracy.\n\n4.1 k-NN in FAISS\nFAISS provides both exact and approximate k-NN search. - Exact Search: Uses brute-force comparison for the most accurate results. - Approximate Search: Uses indexing structures like IVF (Inverted File Index) and HNSW for faster retrieval at scale."
  },
  {
    "objectID": "posts/vector-databases-and-rag/index.html#implementing-a-simple-vector-database-in-python",
    "href": "posts/vector-databases-and-rag/index.html#implementing-a-simple-vector-database-in-python",
    "title": "Building a Simple Vector Database in Python",
    "section": "5 Implementing a Simple Vector Database in Python",
    "text": "5 Implementing a Simple Vector Database in Python\nWe’ll start by implementing a minimal in-memory vector database in Python before introducing FAISS for efficient retrieval.\n\n5.1 Step 1: Stub Implementation\nBelow is a basic skeleton for a vector database.\nfrom typing import List, Tuple, Optional\nimport numpy as np\n\nclass SimpleVectorDB:\n    def __init__(self, dim: int):\n        \"\"\"Initialize the vector database with a given dimensionality.\"\"\"\n        self.dim = dim\n        self.vectors = []  # List of stored vectors\n        self.metadata = []  # Optional metadata for each vector\n\n    def add_vector(self, vector: np.ndarray, meta: Optional[dict] = None) -&gt; int:\n        \"\"\"Add a new vector with optional metadata and return its index.\"\"\"\n        self.vectors.append(vector)\n        self.metadata.append(meta)\n        return len(self.vectors) - 1\n\n    def search(self, query: np.ndarray, k: int = 5) -&gt; List[Tuple[int, float]]:\n        \"\"\"Find the top-k closest vectors using cosine similarity.\"\"\"\n        if not self.vectors:\n            return []\n\n        matrix = np.array(self.vectors)\n        similarities = matrix @ query / (np.linalg.norm(matrix, axis=1) * np.linalg.norm(query))\n        top_k = np.argsort(-similarities)[:k]\n        return [(i, similarities[i]) for i in top_k]\n\n    def get_vector(self, index: int) -&gt; Optional[np.ndarray]:\n        \"\"\"Retrieve a vector by index.\"\"\"\n        return self.vectors[index] if 0 &lt;= index &lt; len(self.vectors) else None\n\n\n5.2 Step 2: Using FAISS for Fast Search\nFAISS (Facebook AI Similarity Search) is optimized for large-scale vector search.\nimport faiss\n\nclass FaissVectorDB:\n    def __init__(self, dim: int):\n        \"\"\"Initialize a FAISS-based vector database.\"\"\"\n        self.index = faiss.IndexFlatL2(dim)  # L2 distance index\n        self.vectors = []\n\n    def add_vector(self, vector: np.ndarray):\n        \"\"\"Add a new vector to the FAISS index.\"\"\"\n        self.index.add(vector.reshape(1, -1))\n        self.vectors.append(vector)\n\n    def search(self, query: np.ndarray, k: int = 5):\n        \"\"\"Retrieve top-k closest vectors.\"\"\"\n        distances, indices = self.index.search(query.reshape(1, -1), k)\n        return list(zip(indices[0], distances[0]))"
  },
  {
    "objectID": "posts/vector-databases-and-rag/index.html#conclusion",
    "href": "posts/vector-databases-and-rag/index.html#conclusion",
    "title": "Building a Simple Vector Database in Python",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nVector databases are powerful tools for similarity search and information retrieval. This post introduced a simple implementation and an optimized approach using FAISS. Future improvements could include HNSW for approximate search or metadata storage.\nWould you like a follow-up post on integrating FAISS with RAG? Let me know in the comments!"
  },
  {
    "objectID": "posts/why-use-conda/index.html",
    "href": "posts/why-use-conda/index.html",
    "title": "Why Use Conda for Scientific Computing?",
    "section": "",
    "text": "Conda\nMade with ❤️ and GitHub Copilot\nI recently gave a talk on conda and programming practices."
  },
  {
    "objectID": "posts/why-use-conda/index.html#overview",
    "href": "posts/why-use-conda/index.html#overview",
    "title": "Why Use Conda for Scientific Computing?",
    "section": "Overview",
    "text": "Overview\nConda is a popular package and environment management system widely used in the scientific computing community. In this article, we’ll explore the key features of Conda and understand why it’s a preferred choice for managing dependencies for scientific computing. In particular, why not to use system installed Python, OS R, pip, pyenv when collaboration and reproducibility are a priority?"
  },
  {
    "objectID": "posts/why-use-conda/index.html#key-concepts",
    "href": "posts/why-use-conda/index.html#key-concepts",
    "title": "Why Use Conda for Scientific Computing?",
    "section": "1 Key Concepts",
    "text": "1 Key Concepts\n\n1.1 What Sets Conda Apart?\nConda fundamentally differs from traditional package managers by focusing on environment management rather than just package installation. Unlike pip or system package managers, Conda handles:\n\nComplete environment isolation\nCross-language dependency management\nBinary package distribution\nOS-level dependency resolution\n\n\n\n1.2 Environment Management\nConda environments provide isolated spaces where you can:\n\nSpecify exact versions of multiple programming languages\nManage conflicting dependencies between projects\nShare reproducible environments across different operating systems\n\nThis is particularly valuable when working with data science tools that might require specific versions of Python, R, and their associated libraries."
  },
  {
    "objectID": "posts/why-use-conda/index.html#why-not-alternative-approaches",
    "href": "posts/why-use-conda/index.html#why-not-alternative-approaches",
    "title": "Why Use Conda for Scientific Computing?",
    "section": "2 Why Not Alternative Approaches?",
    "text": "2 Why Not Alternative Approaches?\n\n2.1 Node, Ruby, or Java don’t need Conda?\nLanguages like Node.js, Ruby, and Java have built-in package managers that handle dependencies effectively. Since they don’t need ultra high performance low level dependencies, they can rely on their language-specific package managers.\nThey just don’t need to go down to the level of C, C++, Fortran, and OS, Platform and Chip specific dependencies which is where Conda shines.\n\n\n2.2 System Python/R/Language Limitations\nSystem-installed Python or R (or any other language) can lead to several issues:\n\nVersion conflicts between different projects\nLack of reproducibility across systems\nPotential system stability issues\nLimited control over package versions\n\n\n\n2.3 Pip’s Shortcomings\nWhile pip is excellent for Python-specific packages, it falls short for scientific computing because it:\n\nCannot manage non-Python dependencies\nDoesn’t handle system-level libraries\nLacks environment management capabilities\nCan’t easily switch between Python versions"
  },
  {
    "objectID": "posts/why-use-conda/index.html#condas-advantage-for-scientific-computing",
    "href": "posts/why-use-conda/index.html#condas-advantage-for-scientific-computing",
    "title": "Why Use Conda for Scientific Computing?",
    "section": "3 Conda’s Advantage for Scientific Computing",
    "text": "3 Conda’s Advantage for Scientific Computing\n\n3.1 Package Distribution Model\nConda uses a sophisticated approach to package management:\n\nPre-built binary packages\nMultiple repository channels:\n\nconda-forge (community-maintained)\nbioconda (bioinformatics)\nDomain-specific channels (PyTorch, NVIDIA, Intel)\n\n\n\n\n3.2 Dependency Resolution\nConda employs a SAT solver to:\n\nEnsure all dependencies are compatible\nResolve version conflicts automatically\nHandle cross-language dependencies\nMaintain environment consistency"
  },
  {
    "objectID": "posts/why-use-conda/index.html#best-practices",
    "href": "posts/why-use-conda/index.html#best-practices",
    "title": "Why Use Conda for Scientific Computing?",
    "section": "4 Best Practices",
    "text": "4 Best Practices\n\n4.1 Channel Priority\n\nUse conda-forge as the primary channel\nAvoid conda defaults due to potential licensing issues\nAdd specialized channels only when needed\n\n\n\n4.2 Environment Management\n# Create a new environment\nconda create -n myenv python=3.9\n\n# Install packages\nconda install -c conda-forge numpy pandas scipy\n\n# Export environment\nconda env export &gt; environment.yml\n\n\n4.3 Common Pitfalls to Avoid\n\nDon’t mix pip and conda installations when possible\nUse $HOME instead of ~ in conda commands\nBe patient with dependency resolution\nConsider mamba for faster installations"
  },
  {
    "objectID": "posts/why-use-conda/index.html#conclusion",
    "href": "posts/why-use-conda/index.html#conclusion",
    "title": "Why Use Conda for Scientific Computing?",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nWhile Conda isn’t perfect, it provides the most comprehensive solution for scientific computing environment management. Its ability to handle complex dependencies, ensure reproducibility, and support multiple programming languages makes it invaluable for collaborative scientific work.\nThe initial learning curve and occasional slower installations are small prices to pay for the reliability and reproducibility it offers. For scientific computing projects where reproducibility is crucial, Conda remains the tool of choice."
  },
  {
    "objectID": "posts/git-search/index.html",
    "href": "posts/git-search/index.html",
    "title": "Git Repository Search",
    "section": "",
    "text": "“Robot Magnifying Glass Search”\n\n\nPhoto by Growtika on Unsplash\nThere are many ways to search a repository, particularly a Git Repo. We will outline some use cases with examples for a “Unix-like” file directory and also a Git Repo.\nLet’s do this for a library I’ve been looking at.\nSome unit tests break on Apple Silicon for the open source library pyg. The maintainer disabled some tests. I want to find them. It has something to do with PyTorch not fully supporting compressed sparse tensor representations on Apple’s mps framework for Apple Silicon. I received the following note:\n\nthere are a few tests that were disabled around test_sparse\n\nand\n\nthe convert_coo_to_csr_indices doesn’t seem to be supported.\n\nIt’s likely that test_sparse and convert_coo_to_csr_indices are variable names or tokens inside a code file of the git repository. However, for illustration, we will assume that they could be anywhere in the git repo (filenames, directory names, commit messages, variable names, past commits, current directory).\nThe objective is to find (and then later fix bugs) related to these strings. So find the strings in filenames and/or inside contents of filenames (line numbers of specific files), checking through commit history for occurences of the strings.\nTime to search. The repo can be cloned locally from here and then cd into it.\n\n\nCode\nimport os\n\nos.chdir(\"/Users/ravikalia/Code/github.com/ml-blog/posts/git-search/\")\n\n\n\n\nCode\nprint(os.getcwd())\n\n\n/Users/ravikalia/Code/github.com/ml-blog/posts/git-search\n\n\n\n\nCode\n%%bash\nif [ -d \"pytorch_geometric\" ]; then\n    rm -rf pytorch_geometric\nfi\ngit clone https://github.com/pyg-team/pytorch_geometric.git\ngit -C pytorch_geometric fetch --all\n\n\nCloning into 'pytorch_geometric'...\n\n\nLet’s change the directory to the root of the cloned repo, which makes searching easier\n\n\nCode\nos.chdir(\"./pytorch_geometric\")\n\n\n\n\nWe can look for the string test_sparse in filenames using the shell command line tool find.\n\n\nCode\n%%bash\nfind . -name \"*test_sparse*\" -o -name \"*convert_coo_to_csr_indices*\"\n\n\n./test/utils/test_sparse.py\n\n\ngreat, so we have a file to look at. Let’s look at the file test_sparse.py. It seems to be unit tests related to sparsity, possibly testing utility functions for converting between sparse tensor representations.\n\n\n\nString search is a bit more complicated. grep is an awesome tool for this.\n\n\nCode\n%%bash\ngrep -rn . -e \"test_sparse\" -e \"convert_coo_to_csr_indices\"\n\n\n./test/utils/test_cross_entropy.py:9:def test_sparse_cross_entropy_multiclass(with_edge_label_weight):\n./test/utils/test_cross_entropy.py:32:def test_sparse_cross_entropy_multilabel(with_edge_label_weight):\n./test/test_edge_index.py:102:def test_sparse_tensor(dtype, device):\n./test/test_edge_index.py:992:def test_sparse_narrow(device):\n./test/test_edge_index.py:1026:def test_sparse_resize(device):\n./torch_geometric/testing/asserts.py:24:    test_sparse_layouts: Optional[List[Union[str, torch.layout]]] = None,\n./torch_geometric/testing/asserts.py:49:        test_sparse_layouts (List[str or int], optional): The sparse layouts to\n./torch_geometric/testing/asserts.py:62:    if test_sparse_layouts is None:\n./torch_geometric/testing/asserts.py:63:        test_sparse_layouts = SPARSE_LAYOUTS\n./torch_geometric/testing/asserts.py:74:    if len(test_sparse_layouts) &gt; 0 and sparse_size is None:\n./torch_geometric/testing/asserts.py:75:        raise ValueError(f\"Got sparse layouts {test_sparse_layouts}, but no \"\n./torch_geometric/testing/asserts.py:93:    for layout in (test_sparse_layouts or []):\nBinary file ./.git/index matches\n\n\nMany locations matched to 3 files. It’s possible they aren’t all relevant for testing purpose. The .git/index is a binary file, which is used by git to store information about the repository, it’s not relevant for our task.\n\n\n\nGit is a distributed version control system. It is a tool that tracks changes in files and directories. At user-defined snapshots in time, called commits, it records the changes made to the files and directories. As a consequence it is possible to search for changes in the repository across snapshots.\nAlong with grep and find, there are git specific tools for searching snapshots of the repo, commit messages and filtering by date and author, such as:\n\ngit ls-files\ngit log\ngit grep\n\n\n\n\nThe working tree is what you see when you list the files in your project’s directory that are being tracked. It’s the version of your project that you’re currently working on. The git checkout command is used to update the working directory with a specific commit, matching the snapshot recorded in the commit. Untracked files are not affected by git checkout.\nThe git ls-files command lists the files in the working tree that are being tracked by git. The filenames can be searched for a string using the grep command.\n\n\nCode\n%%bash\ngit ls-files | grep \"test_sparse\"\n\n\ntest/utils/test_sparse.py\n\n\nIf we want to log commit messages (including commit ids) where filenames contain the string test_sparse were modified, we can use the following command, truncating the output with pipe to head:\n\n\nCode\n%%bash\ngit log --all -- *test_sparse* | head -n 20\n\n\ncommit 62fa51e0000913e1b3023b817485d2b248322539\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sun Dec 24 11:56:08 2023 +0100\n\n    Accelerate concatenation of `torch.sparse` tensors (#8670)\n    \n    Fixes #8664\n\ncommit 1c89e751804d1eb2fb626dabc677198a1878c34d\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Wed Oct 4 09:59:36 2023 +0200\n\n    Skip TorchScript bug for PyTorch &lt; 1.12 (#8123)\n\ncommit 51c50c2f9d3372de34f4ac3617f396384a36558c\nAuthor: filipekstrm &lt;filip.ekstrom@hotmail.com&gt;\nDate:   Tue Oct 3 20:39:04 2023 +0200\n\n    Added `mask` argument to `dense_to_sparse` (#8117)\n    \n\n\n\n\n\nTo search for a string inside file contents across commits, we can use the git log and git grep commands. The git log command lists the commits in reverse chronological order.\nThe flag -S, and --all are used to search for change in the number of occurences of the string in the repo across all branches and commits. (Again we’ll pipe to head to truncate the output.)\n\n\nCode\n%%bash\ngit log -S \"test_sparse\" --all | head -n 20\n\n\ncommit dba9659f6c4f29fd2be1f50b5ea12a29a926082f\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Thu Feb 29 14:04:19 2024 +0100\n\n    Fix `EdgeIndex.resize_` linting issues (#8993)\n\ncommit 123e38ef6715f75ed9198d256cc2cb984b431630\nAuthor: Poovaiah Palangappa &lt;98763718+pmpalang@users.noreply.github.com&gt;\nDate:   Sun Feb 11 03:32:44 2024 -0800\n\n    Example of a recommender system (#8546)\n    \n    Hi Everyone,\n    \n    I'm adding a recommender system example with the following salient\n    features\n    \n    1. Dataset MovieLens – a heterogenous use case\n    2. Demonstrates the use of edge based temporal sampling\n    3. Visualization\n\n\nto be specific to a branch, replace –all with the branch name (master in this case)\n\n\nCode\n%%bash\ngit log master -S \"convert_coo_to_csr_indices\"  | head -n 20\n\n\nIf we just want commit hashes and filenames where a file was added (and has the string in its contents), we can use the --name-only flag, made pretty:\n\n\nCode\n%%bash\ngit log master -S \"test_sparse\" --pretty=format:\"%h\" --name-only --diff-filter=A\n\n\n801723efa\ntest/utils/test_cross_entropy.py\n\n1dadc0705\ntorch_geometric/testing/asserts.py\n\n2c01aa22c\ntest/utils/test_sparse.py\n\n\nWith regular expression search use the flag -G ( * glob is not needed as it’s implied with regular expressions).\n\n\nCode\n%%bash\ngit log -G \"convert_coo_to_csr_indices\" --pretty=format:\"%h\" --name-only\n\n\nNothing. It seems that the string convert_coo_to_csr_indices is not in the contents of any files in the repo.\n\n\nCode\n%%bash\ngit log -G \"coo_to_csr\" --pretty=format:\"%h\" --name-only | head -n 20\n\n\n390942fc4\ntorch_geometric/data/edge_index.py\n\n699120e25\ntorch_geometric/data/edge_index.py\n\na6f0f4947\ntorch_geometric/data/edge_index.py\n\ncf786b735\ntorch_geometric/data/edge_index.py\n\nb825dc637\ntorch_geometric/data/edge_index.py\n\nb5ecfd9b4\ntorch_geometric/data/graph_store.py\ntorch_geometric/nn/conv/cugraph/base.py\ntorch_geometric/nn/conv/rgcn_conv.py\ntorch_geometric/nn/dense/linear.py\n\n\nLet’s try a few different strings.\n\n\nCode\n%%bash\ngit log -G \"convert_coo\" --pretty=format:\"%h\" --name-only\n\n\n\n\nCode\n%%bash\ngit log -G \"csr_indices\" --pretty=format:\"%h\" --name-only\n\n\n\n\nCode\n%%bash\ngit log master -G\"test_sparse\" --pretty=format:\"%h\" --name-only\n\n\ndba9659f6\ntest/test_edge_index.py\n\n123e38ef6\ntest/test_edge_index.py\n\n23bbc128d\ntest/test_edge_index.py\n\ned9698d0b\ntorch_geometric/testing/asserts.py\n\n1725f1436\ntest/utils/test_cross_entropy.py\n\n801723efa\ntest/utils/test_cross_entropy.py\n\n1dadc0705\ntorch_geometric/testing/asserts.py\n\n7b4892781\ntest/nn/conv/test_gcn_conv.py\n\n72e8ef33d\ntest/nn/conv/test_gcn_conv.py\n\n93fab2e53\ntest/nn/conv/test_gcn_conv.py\n\nd01ea9dab\ntest/utils/test_sparse.py\n\n2c01aa22c\ntest/utils/test_sparse.py\n\neb4260ce0\ntorch_geometric/nn/functional/pool/voxel_pool_test.py\n\n544f4ad0e\ntorch_geometric/nn/functional/pool/voxel_pool_test.py\n\n\n\n\n\n\n\nCode\n%%bash\ngit log --author=\"ravkalia\"  | head -n 20\n\n\ncommit f0e4c829662df9eb67fd5c0abda002c9b7cd0afb\nAuthor: Ravi Kalia &lt;ravkalia@gmail.com&gt;\nDate:   Sun Mar 24 08:05:12 2024 -0500\n\n    Replace `withCUDA` decorator: `withDevice` (#9082)\n    \n    Replace `withCUDA` for a `withDevice` decorator.\n    \n    Change variable name from devices to processors to reduce confusion\n    against pytorch api (backends/devices) and reflect the hardware choices.\n    \n    Note that at this time:\n    \n    ## Hardware\n    3 repertoires of hardware can be used to run pyTorch code:\n    \n    * CPU only\n    * CPU and GPU\n    * Unified Memory Single Chip\n    \n\n\n\n\nCode\n%%bash\ngit log --author=\"ravkalia\" --since=\"2022-01-01\" --until=\"2024-02-31\" | head -n 20\n\n\ncommit 25b2f208e671eeec285bfafa2e246ea0a234b312\nAuthor: Ravi Kalia &lt;ravkalia@gmail.com&gt;\nDate:   Wed Feb 21 11:11:33 2024 -0500\n\n    docs: fix broken links to source of graph classification datasets (#8946)\n    \n    **Update Broken Dataset Links in Documentation**\n    \n    This PR addresses broken links in the documentation that pointed to the\n    common benchmark datasets. The links were updated to point to the\n    correct URL.\n    \n    Changes were made in the following files:\n    \n    1. `benchmark/kernel/README.md`\n    2. `docs/source/get_started/introduction.rst`\n    \n    The specific changes are as follows:\n    \n    In `benchmark/kernel/README.md`:\n\n\n\n\nCode\n%%bash\ngit log --grep=\"docs\"  --since=\"2022-01-01\" --until=\"2022-02-31\" | head -n 20\n\n\ncommit 24a185e7268f70ee549c7a424b9426b9a18b5706\nAuthor: Ramona Bendias &lt;ramona.bendias@gmail.com&gt;\nDate:   Mon Feb 21 13:03:52 2022 +0000\n\n    Add general `Explainer` Class (#4090)\n    \n    * Add base Explainer\n    \n    * Update Explainer\n    \n    * Fix test\n    \n    * Clean code\n    \n    * Update test/nn/models/test_explainer.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n\n\n\n\nCode\n%%bash\ngit log --oneline --grep=\"docs\"  --since=\"2022-01-01\" --until=\"2022-02-31\" \n\n\n24a185e72 Add general `Explainer` Class (#4090)\n6002170a5 Make models compatible to Captum (#3990)\n14d588d4c Update attention.py (#4009)\n50ff5e6d6 Add `full` extras to install command in contribution docs (#3991)\n1e24b3a16 Refactor: `MLP` initialization (#3957)\n3e4891be6 Doc improvements to set2set layers (#3889)\nfac848c25 Let `TemporalData` inherit from `BaseData` and add docs (#3867)\n0c29b0d5b Updated docstring for shape info - part 2 (#3739)\n\n\n\n\n\nThe main differences between git grep and grep are:\ngit grep only searches through your tracked files, while grep can search through any files. git grep is aware of your Git repository structure and can search through old commits, branches, etc., while grep only searches through the current state of files.\ngit grep is faster than grep when searching through a Git repository because it takes advantage of Git’s index data structure.\n\n\nCode\n%%time\n%%bash\ngit grep \"test_sparse\" &gt; /dev/null\n\n\nCPU times: user 1.52 ms, sys: 4.44 ms, total: 5.96 ms\nWall time: 22.6 ms\n\n\n\n\nCode\n%%time\n%%bash\ngrep -r \"test_sparse\" . &gt; /dev/null\n\n\nCPU times: user 1.94 ms, sys: 3.26 ms, total: 5.21 ms\nWall time: 346 ms\n\n\n\n\n\nThere are many ways to search a repository, particularly a Git Repo. We outlined some use cases with examples for a “Unix-like” file directory and also a Git Repo.\nIn most cases use:\n\ngit grep for searching strings in the repository in the current working tree or a specific commit\ngit log for searching across commits.\n\nThere are many flags and options for these commands - some combinations which produce the same output. Be sure to check the documentation for more information.\nFor the strings we are after, the conclusion is:\n\ntest_sparse is in the filename test_sparse.py and in the contents of the file test_sparse.py in the repo.\nconvert_coo_to_csr_indices is not in the contents of any files in the repo.\nstrings similar to convert_coo_to_csr_indices are available.\n\nThe most promising output from the commands tested are:\n\n\nCode\n%%bash\ngrep -rn . -e \"test_sparse\" || echo \"no match\"\n\n\n./test/utils/test_cross_entropy.py:9:def test_sparse_cross_entropy_multiclass(with_edge_label_weight):\n./test/utils/test_cross_entropy.py:32:def test_sparse_cross_entropy_multilabel(with_edge_label_weight):\n./test/test_edge_index.py:102:def test_sparse_tensor(dtype, device):\n./test/test_edge_index.py:992:def test_sparse_narrow(device):\n./test/test_edge_index.py:1026:def test_sparse_resize(device):\n./torch_geometric/testing/asserts.py:24:    test_sparse_layouts: Optional[List[Union[str, torch.layout]]] = None,\n./torch_geometric/testing/asserts.py:49:        test_sparse_layouts (List[str or int], optional): The sparse layouts to\n./torch_geometric/testing/asserts.py:62:    if test_sparse_layouts is None:\n./torch_geometric/testing/asserts.py:63:        test_sparse_layouts = SPARSE_LAYOUTS\n./torch_geometric/testing/asserts.py:74:    if len(test_sparse_layouts) &gt; 0 and sparse_size is None:\n./torch_geometric/testing/asserts.py:75:        raise ValueError(f\"Got sparse layouts {test_sparse_layouts}, but no \"\n./torch_geometric/testing/asserts.py:93:    for layout in (test_sparse_layouts or []):\nBinary file ./.git/index matches\n\n\n\n\nCode\n%%bash\ngrep -rn . -e \"convert_coo_to_csr_indices\" || echo \"no match\"\n\n\nno match\n\n\n\n\nCode\n%%bash\ngit ls-files | grep \"test_sparse\"\n\n\ntest/utils/test_sparse.py\n\n\n\n\nCode\n%%bash\ngit log -G \"coo_to_csr\" --pretty=format:\"%h\" --name-only | head -n 20\n\n\n390942fc4\ntorch_geometric/data/edge_index.py\n\n699120e25\ntorch_geometric/data/edge_index.py\n\na6f0f4947\ntorch_geometric/data/edge_index.py\n\ncf786b735\ntorch_geometric/data/edge_index.py\n\nb825dc637\ntorch_geometric/data/edge_index.py\n\nb5ecfd9b4\ntorch_geometric/data/graph_store.py\ntorch_geometric/nn/conv/cugraph/base.py\ntorch_geometric/nn/conv/rgcn_conv.py\ntorch_geometric/nn/dense/linear.py\n\n\n\n\nCode\n%%bash\ngit log --pretty=format:\"%h\" -G \"coo_to_csr\" --all | while read commit; do\n    echo \"Commit: $commit\"\n    git grep -n \"coo_to_csr\" $commit\ndone | head -n 20\n\n\nCommit: 390942fc4\n390942fc4:torch_geometric/data/edge_index.py:344:        self._indptr = torch._convert_indices_from_coo_to_csr(\n390942fc4:torch_geometric/data/edge_index.py:382:            rowptr = self._T_indptr = torch._convert_indices_from_coo_to_csr(\n390942fc4:torch_geometric/data/edge_index.py:403:            colptr = self._T_indptr = torch._convert_indices_from_coo_to_csr(\n390942fc4:torch_geometric/utils/sparse.py:480:    return torch._convert_indices_from_coo_to_csr(\nCommit: 699120e25\n699120e25:torch_geometric/data/edge_index.py:323:                self._rowptr = rowptr = torch._convert_indices_from_coo_to_csr(\n699120e25:torch_geometric/data/edge_index.py:351:                self._rowptr = rowptr = torch._convert_indices_from_coo_to_csr(\n699120e25:torch_geometric/data/edge_index.py:375:                self._colptr = colptr = torch._convert_indices_from_coo_to_csr(\n699120e25:torch_geometric/data/edge_index.py:403:                self._colptr = colptr = torch._convert_indices_from_coo_to_csr(\n699120e25:torch_geometric/utils/sparse.py:480:    return torch._convert_indices_from_coo_to_csr(\nCommit: a6f0f4947\na6f0f4947:torch_geometric/data/edge_index.py:321:                self._rowptr = torch._convert_indices_from_coo_to_csr(\na6f0f4947:torch_geometric/data/edge_index.py:352:                self._rowptr = torch._convert_indices_from_coo_to_csr(\na6f0f4947:torch_geometric/data/edge_index.py:379:                self._colptr = torch._convert_indices_from_coo_to_csr(\na6f0f4947:torch_geometric/data/edge_index.py:410:                self._colptr = torch._convert_indices_from_coo_to_csr(\na6f0f4947:torch_geometric/utils/sparse.py:480:    return torch._convert_indices_from_coo_to_csr(\nCommit: cf786b735\ncf786b735:torch_geometric/data/edge_index.py:236:        self._rowptr = torch._convert_indices_from_coo_to_csr(\ncf786b735:torch_geometric/data/edge_index.py:255:        self._colptr = torch._convert_indices_from_coo_to_csr(\n\n\nThis is a good starting point for debugging the issues with the unit tests in the library. Useful and informative :=)\nAnd finally some clean up:\n\n\nCode\nimport shutil\n\nshutil.rmtree(os.getcwd())"
  },
  {
    "objectID": "posts/git-search/index.html#filename-in-unix-like-directory",
    "href": "posts/git-search/index.html#filename-in-unix-like-directory",
    "title": "Git Repository Search",
    "section": "",
    "text": "We can look for the string test_sparse in filenames using the shell command line tool find.\n\n\nCode\n%%bash\nfind . -name \"*test_sparse*\" -o -name \"*convert_coo_to_csr_indices*\"\n\n\n./test/utils/test_sparse.py\n\n\ngreat, so we have a file to look at. Let’s look at the file test_sparse.py. It seems to be unit tests related to sparsity, possibly testing utility functions for converting between sparse tensor representations."
  },
  {
    "objectID": "posts/git-search/index.html#string-in-unix-like-directory",
    "href": "posts/git-search/index.html#string-in-unix-like-directory",
    "title": "Git Repository Search",
    "section": "",
    "text": "String search is a bit more complicated. grep is an awesome tool for this.\n\n\nCode\n%%bash\ngrep -rn . -e \"test_sparse\" -e \"convert_coo_to_csr_indices\"\n\n\n./test/utils/test_cross_entropy.py:9:def test_sparse_cross_entropy_multiclass(with_edge_label_weight):\n./test/utils/test_cross_entropy.py:32:def test_sparse_cross_entropy_multilabel(with_edge_label_weight):\n./test/test_edge_index.py:102:def test_sparse_tensor(dtype, device):\n./test/test_edge_index.py:992:def test_sparse_narrow(device):\n./test/test_edge_index.py:1026:def test_sparse_resize(device):\n./torch_geometric/testing/asserts.py:24:    test_sparse_layouts: Optional[List[Union[str, torch.layout]]] = None,\n./torch_geometric/testing/asserts.py:49:        test_sparse_layouts (List[str or int], optional): The sparse layouts to\n./torch_geometric/testing/asserts.py:62:    if test_sparse_layouts is None:\n./torch_geometric/testing/asserts.py:63:        test_sparse_layouts = SPARSE_LAYOUTS\n./torch_geometric/testing/asserts.py:74:    if len(test_sparse_layouts) &gt; 0 and sparse_size is None:\n./torch_geometric/testing/asserts.py:75:        raise ValueError(f\"Got sparse layouts {test_sparse_layouts}, but no \"\n./torch_geometric/testing/asserts.py:93:    for layout in (test_sparse_layouts or []):\nBinary file ./.git/index matches\n\n\nMany locations matched to 3 files. It’s possible they aren’t all relevant for testing purpose. The .git/index is a binary file, which is used by git to store information about the repository, it’s not relevant for our task."
  },
  {
    "objectID": "posts/git-search/index.html#what-is-git",
    "href": "posts/git-search/index.html#what-is-git",
    "title": "Git Repository Search",
    "section": "",
    "text": "Git is a distributed version control system. It is a tool that tracks changes in files and directories. At user-defined snapshots in time, called commits, it records the changes made to the files and directories. As a consequence it is possible to search for changes in the repository across snapshots.\nAlong with grep and find, there are git specific tools for searching snapshots of the repo, commit messages and filtering by date and author, such as:\n\ngit ls-files\ngit log\ngit grep"
  },
  {
    "objectID": "posts/git-search/index.html#filename-in-git-repository",
    "href": "posts/git-search/index.html#filename-in-git-repository",
    "title": "Git Repository Search",
    "section": "",
    "text": "The working tree is what you see when you list the files in your project’s directory that are being tracked. It’s the version of your project that you’re currently working on. The git checkout command is used to update the working directory with a specific commit, matching the snapshot recorded in the commit. Untracked files are not affected by git checkout.\nThe git ls-files command lists the files in the working tree that are being tracked by git. The filenames can be searched for a string using the grep command.\n\n\nCode\n%%bash\ngit ls-files | grep \"test_sparse\"\n\n\ntest/utils/test_sparse.py\n\n\nIf we want to log commit messages (including commit ids) where filenames contain the string test_sparse were modified, we can use the following command, truncating the output with pipe to head:\n\n\nCode\n%%bash\ngit log --all -- *test_sparse* | head -n 20\n\n\ncommit 62fa51e0000913e1b3023b817485d2b248322539\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sun Dec 24 11:56:08 2023 +0100\n\n    Accelerate concatenation of `torch.sparse` tensors (#8670)\n    \n    Fixes #8664\n\ncommit 1c89e751804d1eb2fb626dabc677198a1878c34d\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Wed Oct 4 09:59:36 2023 +0200\n\n    Skip TorchScript bug for PyTorch &lt; 1.12 (#8123)\n\ncommit 51c50c2f9d3372de34f4ac3617f396384a36558c\nAuthor: filipekstrm &lt;filip.ekstrom@hotmail.com&gt;\nDate:   Tue Oct 3 20:39:04 2023 +0200\n\n    Added `mask` argument to `dense_to_sparse` (#8117)"
  },
  {
    "objectID": "posts/git-search/index.html#string-in-file-contents-of-git-repository",
    "href": "posts/git-search/index.html#string-in-file-contents-of-git-repository",
    "title": "Git Repository Search",
    "section": "",
    "text": "To search for a string inside file contents across commits, we can use the git log and git grep commands. The git log command lists the commits in reverse chronological order.\nThe flag -S, and --all are used to search for change in the number of occurences of the string in the repo across all branches and commits. (Again we’ll pipe to head to truncate the output.)\n\n\nCode\n%%bash\ngit log -S \"test_sparse\" --all | head -n 20\n\n\ncommit dba9659f6c4f29fd2be1f50b5ea12a29a926082f\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Thu Feb 29 14:04:19 2024 +0100\n\n    Fix `EdgeIndex.resize_` linting issues (#8993)\n\ncommit 123e38ef6715f75ed9198d256cc2cb984b431630\nAuthor: Poovaiah Palangappa &lt;98763718+pmpalang@users.noreply.github.com&gt;\nDate:   Sun Feb 11 03:32:44 2024 -0800\n\n    Example of a recommender system (#8546)\n    \n    Hi Everyone,\n    \n    I'm adding a recommender system example with the following salient\n    features\n    \n    1. Dataset MovieLens – a heterogenous use case\n    2. Demonstrates the use of edge based temporal sampling\n    3. Visualization\n\n\nto be specific to a branch, replace –all with the branch name (master in this case)\n\n\nCode\n%%bash\ngit log master -S \"convert_coo_to_csr_indices\"  | head -n 20\n\n\nIf we just want commit hashes and filenames where a file was added (and has the string in its contents), we can use the --name-only flag, made pretty:\n\n\nCode\n%%bash\ngit log master -S \"test_sparse\" --pretty=format:\"%h\" --name-only --diff-filter=A\n\n\n801723efa\ntest/utils/test_cross_entropy.py\n\n1dadc0705\ntorch_geometric/testing/asserts.py\n\n2c01aa22c\ntest/utils/test_sparse.py\n\n\nWith regular expression search use the flag -G ( * glob is not needed as it’s implied with regular expressions).\n\n\nCode\n%%bash\ngit log -G \"convert_coo_to_csr_indices\" --pretty=format:\"%h\" --name-only\n\n\nNothing. It seems that the string convert_coo_to_csr_indices is not in the contents of any files in the repo.\n\n\nCode\n%%bash\ngit log -G \"coo_to_csr\" --pretty=format:\"%h\" --name-only | head -n 20\n\n\n390942fc4\ntorch_geometric/data/edge_index.py\n\n699120e25\ntorch_geometric/data/edge_index.py\n\na6f0f4947\ntorch_geometric/data/edge_index.py\n\ncf786b735\ntorch_geometric/data/edge_index.py\n\nb825dc637\ntorch_geometric/data/edge_index.py\n\nb5ecfd9b4\ntorch_geometric/data/graph_store.py\ntorch_geometric/nn/conv/cugraph/base.py\ntorch_geometric/nn/conv/rgcn_conv.py\ntorch_geometric/nn/dense/linear.py\n\n\nLet’s try a few different strings.\n\n\nCode\n%%bash\ngit log -G \"convert_coo\" --pretty=format:\"%h\" --name-only\n\n\n\n\nCode\n%%bash\ngit log -G \"csr_indices\" --pretty=format:\"%h\" --name-only\n\n\n\n\nCode\n%%bash\ngit log master -G\"test_sparse\" --pretty=format:\"%h\" --name-only\n\n\ndba9659f6\ntest/test_edge_index.py\n\n123e38ef6\ntest/test_edge_index.py\n\n23bbc128d\ntest/test_edge_index.py\n\ned9698d0b\ntorch_geometric/testing/asserts.py\n\n1725f1436\ntest/utils/test_cross_entropy.py\n\n801723efa\ntest/utils/test_cross_entropy.py\n\n1dadc0705\ntorch_geometric/testing/asserts.py\n\n7b4892781\ntest/nn/conv/test_gcn_conv.py\n\n72e8ef33d\ntest/nn/conv/test_gcn_conv.py\n\n93fab2e53\ntest/nn/conv/test_gcn_conv.py\n\nd01ea9dab\ntest/utils/test_sparse.py\n\n2c01aa22c\ntest/utils/test_sparse.py\n\neb4260ce0\ntorch_geometric/nn/functional/pool/voxel_pool_test.py\n\n544f4ad0e\ntorch_geometric/nn/functional/pool/voxel_pool_test.py"
  },
  {
    "objectID": "posts/git-search/index.html#filter-git-commit-messages-for-author-date-and-string",
    "href": "posts/git-search/index.html#filter-git-commit-messages-for-author-date-and-string",
    "title": "Git Repository Search",
    "section": "",
    "text": "Code\n%%bash\ngit log --author=\"ravkalia\"  | head -n 20\n\n\ncommit f0e4c829662df9eb67fd5c0abda002c9b7cd0afb\nAuthor: Ravi Kalia &lt;ravkalia@gmail.com&gt;\nDate:   Sun Mar 24 08:05:12 2024 -0500\n\n    Replace `withCUDA` decorator: `withDevice` (#9082)\n    \n    Replace `withCUDA` for a `withDevice` decorator.\n    \n    Change variable name from devices to processors to reduce confusion\n    against pytorch api (backends/devices) and reflect the hardware choices.\n    \n    Note that at this time:\n    \n    ## Hardware\n    3 repertoires of hardware can be used to run pyTorch code:\n    \n    * CPU only\n    * CPU and GPU\n    * Unified Memory Single Chip\n    \n\n\n\n\nCode\n%%bash\ngit log --author=\"ravkalia\" --since=\"2022-01-01\" --until=\"2024-02-31\" | head -n 20\n\n\ncommit 25b2f208e671eeec285bfafa2e246ea0a234b312\nAuthor: Ravi Kalia &lt;ravkalia@gmail.com&gt;\nDate:   Wed Feb 21 11:11:33 2024 -0500\n\n    docs: fix broken links to source of graph classification datasets (#8946)\n    \n    **Update Broken Dataset Links in Documentation**\n    \n    This PR addresses broken links in the documentation that pointed to the\n    common benchmark datasets. The links were updated to point to the\n    correct URL.\n    \n    Changes were made in the following files:\n    \n    1. `benchmark/kernel/README.md`\n    2. `docs/source/get_started/introduction.rst`\n    \n    The specific changes are as follows:\n    \n    In `benchmark/kernel/README.md`:\n\n\n\n\nCode\n%%bash\ngit log --grep=\"docs\"  --since=\"2022-01-01\" --until=\"2022-02-31\" | head -n 20\n\n\ncommit 24a185e7268f70ee549c7a424b9426b9a18b5706\nAuthor: Ramona Bendias &lt;ramona.bendias@gmail.com&gt;\nDate:   Mon Feb 21 13:03:52 2022 +0000\n\n    Add general `Explainer` Class (#4090)\n    \n    * Add base Explainer\n    \n    * Update Explainer\n    \n    * Fix test\n    \n    * Clean code\n    \n    * Update test/nn/models/test_explainer.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n\n\n\n\nCode\n%%bash\ngit log --oneline --grep=\"docs\"  --since=\"2022-01-01\" --until=\"2022-02-31\" \n\n\n24a185e72 Add general `Explainer` Class (#4090)\n6002170a5 Make models compatible to Captum (#3990)\n14d588d4c Update attention.py (#4009)\n50ff5e6d6 Add `full` extras to install command in contribution docs (#3991)\n1e24b3a16 Refactor: `MLP` initialization (#3957)\n3e4891be6 Doc improvements to set2set layers (#3889)\nfac848c25 Let `TemporalData` inherit from `BaseData` and add docs (#3867)\n0c29b0d5b Updated docstring for shape info - part 2 (#3739)"
  },
  {
    "objectID": "posts/git-search/index.html#git-grep-vs-grep",
    "href": "posts/git-search/index.html#git-grep-vs-grep",
    "title": "Git Repository Search",
    "section": "",
    "text": "The main differences between git grep and grep are:\ngit grep only searches through your tracked files, while grep can search through any files. git grep is aware of your Git repository structure and can search through old commits, branches, etc., while grep only searches through the current state of files.\ngit grep is faster than grep when searching through a Git repository because it takes advantage of Git’s index data structure.\n\n\nCode\n%%time\n%%bash\ngit grep \"test_sparse\" &gt; /dev/null\n\n\nCPU times: user 1.52 ms, sys: 4.44 ms, total: 5.96 ms\nWall time: 22.6 ms\n\n\n\n\nCode\n%%time\n%%bash\ngrep -r \"test_sparse\" . &gt; /dev/null\n\n\nCPU times: user 1.94 ms, sys: 3.26 ms, total: 5.21 ms\nWall time: 346 ms"
  },
  {
    "objectID": "posts/git-search/index.html#takeaways",
    "href": "posts/git-search/index.html#takeaways",
    "title": "Git Repository Search",
    "section": "",
    "text": "There are many ways to search a repository, particularly a Git Repo. We outlined some use cases with examples for a “Unix-like” file directory and also a Git Repo.\nIn most cases use:\n\ngit grep for searching strings in the repository in the current working tree or a specific commit\ngit log for searching across commits.\n\nThere are many flags and options for these commands - some combinations which produce the same output. Be sure to check the documentation for more information.\nFor the strings we are after, the conclusion is:\n\ntest_sparse is in the filename test_sparse.py and in the contents of the file test_sparse.py in the repo.\nconvert_coo_to_csr_indices is not in the contents of any files in the repo.\nstrings similar to convert_coo_to_csr_indices are available.\n\nThe most promising output from the commands tested are:\n\n\nCode\n%%bash\ngrep -rn . -e \"test_sparse\" || echo \"no match\"\n\n\n./test/utils/test_cross_entropy.py:9:def test_sparse_cross_entropy_multiclass(with_edge_label_weight):\n./test/utils/test_cross_entropy.py:32:def test_sparse_cross_entropy_multilabel(with_edge_label_weight):\n./test/test_edge_index.py:102:def test_sparse_tensor(dtype, device):\n./test/test_edge_index.py:992:def test_sparse_narrow(device):\n./test/test_edge_index.py:1026:def test_sparse_resize(device):\n./torch_geometric/testing/asserts.py:24:    test_sparse_layouts: Optional[List[Union[str, torch.layout]]] = None,\n./torch_geometric/testing/asserts.py:49:        test_sparse_layouts (List[str or int], optional): The sparse layouts to\n./torch_geometric/testing/asserts.py:62:    if test_sparse_layouts is None:\n./torch_geometric/testing/asserts.py:63:        test_sparse_layouts = SPARSE_LAYOUTS\n./torch_geometric/testing/asserts.py:74:    if len(test_sparse_layouts) &gt; 0 and sparse_size is None:\n./torch_geometric/testing/asserts.py:75:        raise ValueError(f\"Got sparse layouts {test_sparse_layouts}, but no \"\n./torch_geometric/testing/asserts.py:93:    for layout in (test_sparse_layouts or []):\nBinary file ./.git/index matches\n\n\n\n\nCode\n%%bash\ngrep -rn . -e \"convert_coo_to_csr_indices\" || echo \"no match\"\n\n\nno match\n\n\n\n\nCode\n%%bash\ngit ls-files | grep \"test_sparse\"\n\n\ntest/utils/test_sparse.py\n\n\n\n\nCode\n%%bash\ngit log -G \"coo_to_csr\" --pretty=format:\"%h\" --name-only | head -n 20\n\n\n390942fc4\ntorch_geometric/data/edge_index.py\n\n699120e25\ntorch_geometric/data/edge_index.py\n\na6f0f4947\ntorch_geometric/data/edge_index.py\n\ncf786b735\ntorch_geometric/data/edge_index.py\n\nb825dc637\ntorch_geometric/data/edge_index.py\n\nb5ecfd9b4\ntorch_geometric/data/graph_store.py\ntorch_geometric/nn/conv/cugraph/base.py\ntorch_geometric/nn/conv/rgcn_conv.py\ntorch_geometric/nn/dense/linear.py\n\n\n\n\nCode\n%%bash\ngit log --pretty=format:\"%h\" -G \"coo_to_csr\" --all | while read commit; do\n    echo \"Commit: $commit\"\n    git grep -n \"coo_to_csr\" $commit\ndone | head -n 20\n\n\nCommit: 390942fc4\n390942fc4:torch_geometric/data/edge_index.py:344:        self._indptr = torch._convert_indices_from_coo_to_csr(\n390942fc4:torch_geometric/data/edge_index.py:382:            rowptr = self._T_indptr = torch._convert_indices_from_coo_to_csr(\n390942fc4:torch_geometric/data/edge_index.py:403:            colptr = self._T_indptr = torch._convert_indices_from_coo_to_csr(\n390942fc4:torch_geometric/utils/sparse.py:480:    return torch._convert_indices_from_coo_to_csr(\nCommit: 699120e25\n699120e25:torch_geometric/data/edge_index.py:323:                self._rowptr = rowptr = torch._convert_indices_from_coo_to_csr(\n699120e25:torch_geometric/data/edge_index.py:351:                self._rowptr = rowptr = torch._convert_indices_from_coo_to_csr(\n699120e25:torch_geometric/data/edge_index.py:375:                self._colptr = colptr = torch._convert_indices_from_coo_to_csr(\n699120e25:torch_geometric/data/edge_index.py:403:                self._colptr = colptr = torch._convert_indices_from_coo_to_csr(\n699120e25:torch_geometric/utils/sparse.py:480:    return torch._convert_indices_from_coo_to_csr(\nCommit: a6f0f4947\na6f0f4947:torch_geometric/data/edge_index.py:321:                self._rowptr = torch._convert_indices_from_coo_to_csr(\na6f0f4947:torch_geometric/data/edge_index.py:352:                self._rowptr = torch._convert_indices_from_coo_to_csr(\na6f0f4947:torch_geometric/data/edge_index.py:379:                self._colptr = torch._convert_indices_from_coo_to_csr(\na6f0f4947:torch_geometric/data/edge_index.py:410:                self._colptr = torch._convert_indices_from_coo_to_csr(\na6f0f4947:torch_geometric/utils/sparse.py:480:    return torch._convert_indices_from_coo_to_csr(\nCommit: cf786b735\ncf786b735:torch_geometric/data/edge_index.py:236:        self._rowptr = torch._convert_indices_from_coo_to_csr(\ncf786b735:torch_geometric/data/edge_index.py:255:        self._colptr = torch._convert_indices_from_coo_to_csr(\n\n\nThis is a good starting point for debugging the issues with the unit tests in the library. Useful and informative :=)\nAnd finally some clean up:\n\n\nCode\nimport shutil\n\nshutil.rmtree(os.getcwd())"
  },
  {
    "objectID": "posts/model-speciation/index.html",
    "href": "posts/model-speciation/index.html",
    "title": "Model Speciation",
    "section": "",
    "text": "Made with ❤️ and GitHub Copilot"
  },
  {
    "objectID": "posts/model-speciation/index.html#tldr-machine-learning-model-selection-as-evolution",
    "href": "posts/model-speciation/index.html#tldr-machine-learning-model-selection-as-evolution",
    "title": "Model Speciation",
    "section": "TL;DR: Machine Learning Model Selection as Evolution",
    "text": "TL;DR: Machine Learning Model Selection as Evolution\nViewing machine learning through the lens of evolution:\n\nProto-programs = protocells: Basic building blocks of ML algorithms, like affine transformations, recursion, iteration, object templates; small modular reusable code pieces represent the “simplest forms of life.”\nModels = Species: Each ML model (e.g., linear regression, neural networks) represents a distinct “species” in the ecosystem.\nData = Environment: The dataset shapes and tests models just as environments influence species’ survival and adaptation.\nTraining = Maturation: During training, models learn and become more capable to fit and predict their data environment.\nHyperparameter tuning = Sub-speciation: Tuning hyperparameters is akin to the development of sub-species, where small variations emerge for optimization.\nModel selection = Survival of the fittest: Only the best-performing hyperparameter tuned (sub-species) models that can generalize to a an unseen environment (validation data) “survive” and are selected for deployment.\n\nThis evolutionary perspective on model selection suggests that phylogenetics — the study of evolutionary relationships - could offer novel insights into understanding and categorizing machine learning model families, possibly even for generating novel models."
  },
  {
    "objectID": "posts/model-speciation/index.html#the-evolutionary-analogy",
    "href": "posts/model-speciation/index.html#the-evolutionary-analogy",
    "title": "Model Speciation",
    "section": "The Evolutionary Analogy",
    "text": "The Evolutionary Analogy\nIn the world of machine learning, the process of selecting and refining models can be complex and nuanced. To better understand this process, let’s draw an analogy to biological evolution. This comparison can provide an intuitive framework for grasping the key concepts of model selection and optimization.\n\nProto-programs as protocells\nSimple programs or algorithms can be seen as the building blocks of more complex models. These proto-programs are like protocells in nature, capable of basic functions but not yet fully adapted to their environment.\nThese would be the building blocks of more complex models, like the simplest organisms in nature, capable of basic functions but not yet fully adapted to their environment. Examples might be:\n\nTensor arithmetic\nBranching logic\nBasic object templates\nEncapsulation\nRecursion\n\nThey combine to form more complex species, such as models - templates for a ML algorithm is run.\n\n\nModels as Species\nJust as the natural world is populated by various species, the realm of machine learning is filled with different types of models. Each model type, like a species, has its own characteristics, strengths, and weaknesses. These models depend on receiving data and hyper-parameters, then learning parameters which mature to enable learned prediction function.\n\n\nModel Instances as Complex Organisms\nWithin each model type (species), we can create multiple instances. These instances are akin to individual organisms within a species. Each has its own set of parameters, just as organisms have their own genetic makeup.\n\n\nData as the Environment\nThe data we use to train and evaluate our models serves as the environment in which our “model organisms” must survive and thrive. Different datasets present different resources and challenges, much like varied ecological niches in nature.\n\n\nTraining as Maturation\nThe process of training a model is analogous to the maturation of an organism. Through repeated exposure to the training data (environment), the model adjusts its parameters and improves its performance, much like an organism adapting to its surroundings as it grows. Once mature, these species gain the ability to predict unseen data.\nHowever if they are over matured, then they may memorize the training environment data and fail to generalize to unseen validation environment. This would render them unfit for survival in the validation environment.\n\n\nScore Function as Survival Fitness\nIn nature, organisms that are better adapted to their environment are more likely to survive. Similarly, in machine learning, we use a score function to evaluate how well a model performs. Models with better scores are more likely to be selected for further use or refinement.\nThe score function does not depend on the model itself, but on the model’s predictions vs observed reality relevant to some task of interest for a given the validation data environment. This is similar to how an organism’s fitness is determined by its ability to survive in its habitat.\nThere are many different score functions, each of which can be seen as a different measure of fitness at a task for a model in a given environment.\n\n\nHyperparameters as Creating Subspecies\nWhen we select the hyperparameters of a model, we’re essentially creating a subspecies of the main model species. These subspecies share the basic characteristics of the base model species but have unique traits that may make them better suited to certain types of tasks given the model family and data environment.\n\n\n\n“Darwin’s Finches: Speciation”\n\n\nDarwin’s Finches from Wikipedia by John Gould\nHere’s an analogy from evolution. Darwin’s finches evolved from a common ancestor to adapt to different food sources on the Galapagos Islands. They all go through a maturation phase, but become specialized to their particular validation environment. Similarly, hyperparameters are like the genetic variations that allow models to adapt to different data environments.\nDefault hyperparameters are like the default genetic code of an organism, expected to do reasonably well in most common environments, while tuned hyperparameters are like genetic mutations that have performed well in the validation (data) environment.\n\n\nData Splitting as Creating Different Habitats\nThe practice of splitting our data into training, validation, and test sets is akin to creating different habitats for our models:\n\nTraining Data: This is the primary environment where our model “organisms” mature and adapt.\nValidation Data: This represents a similar but distinct habitat where we score each model’s fitness, the best model selected to survive for downstream use.\nTest Data: This is an unseen habitat to report unbiased fitness estimate of the selected model subspecies.\n\n\n\nBest Scored Model as Natural Selection\nThe process of selecting the best-performing model based on validation scores mirrors natural selection. The model that best adapts to both the training and validation environments is chosen to proceed to the test phase, much like the fittest organisms in nature are more likely to survive and change their environment."
  },
  {
    "objectID": "posts/model-speciation/index.html#implications-of-the-evolutionary-perspective",
    "href": "posts/model-speciation/index.html#implications-of-the-evolutionary-perspective",
    "title": "Model Speciation",
    "section": "Implications of the Evolutionary Perspective",
    "text": "Implications of the Evolutionary Perspective\nViewing model selection through this evolutionary lens can provide several insights:\n\nDiversity is Valuable: Just as biodiversity is crucial in ecosystems, having a diverse set of models can be beneficial in machine learning. We just don’t know a priori which model will be the best for a given dataset.\nScoring is Key: The best model or model subspecies for one dataset may not be the best for another, just as organisms adapted to one environment may struggle in a different one.\nOverfitting as Over-specialization: When a model performs well on training data but poorly on validation data, it’s like an organism that’s over-specialized for a very specific data environment and can’t adapt to realistic changes in data.\nEnsemble Methods as Ecosystems: Ensemble methods, which combine multiple models, can be seen as creating a balanced ecosystem where different “species” of models cooperate to solve a problem.\nContinuous Improvement: The field of machine learning, like the process of evolution, is one of continuous adaptation and improvement as we develop new models and techniques."
  },
  {
    "objectID": "posts/model-speciation/index.html#conclusion",
    "href": "posts/model-speciation/index.html#conclusion",
    "title": "Model Speciation",
    "section": "Conclusion",
    "text": "Conclusion\nWhile the analogy between model selection and biological evolution isn’t perfect, it provides a rich metaphor for understanding many aspects of the machine learning process. By thinking in these terms, we can gain new insights into how to approach model selection, hyperparameter tuning, and the overall process of developing effective machine learning solutions.\nIt would be an interesting exercise to create a phylogenetic tree of models, showing how different models have evolved from simpler proto-programs and how they have adapted to different data environments over time. This could provide a fascinating perspective on the history and development of machine learning algorithms. Perhaps even find horizontal gene transfer between models, where ideas from one model are incorporated directly into another.\nFrom this perspective, geometric learning relates to encoding data regularities into model species internals, another way to view the inductive bias of a model.\nAs a practical takeaway, expect that there is no perfect single model for all datasets and tasks, the best model is the one that is best adapted to the specific data environment at hand, while being able to generalize to similar environments."
  },
  {
    "objectID": "posts/sparse_tensors/index.html",
    "href": "posts/sparse_tensors/index.html",
    "title": "Sparsity with PyTorch Tensors",
    "section": "",
    "text": "Image of Building\n\n\nPhoto by engin akyurt on Unsplash\n\n\nThe canonical format for representing tensors in PyTorch is the dense tensor, with associated contiguous memory proportional to the size of the tensor. However, in many applications, the data is sparse, that is most of the elements are zero. In this notebook, we will see how to work efficiently with sparse tensors in PyTorch.\n\n\nA dataset is considered sparse when most of its elements are zero, for some features of the dataset. The exact threshold for considering a dataset as sparse is not strictly defined and can depend on the context and the specific application. A common rule of thumb is that a dataset is sparse if over 50% of its elements are zero.\nIn many real-world applications, datasets can be extremely sparse, with over 90% or even 99% of elements being zero. For example, in a user-item interaction matrix in a recommendation system, each user typically interacts with only a small fraction of all possible items, so the vast majority of the matrix elements are zero.\nIn PyTorch, datasets are stored in tensors. A tensor is a multi-dimensional array, similar to a NumPy array. PyTorch provides some support for sparse tensors.\n\n\n\nIn a dense tensor, all elements are stored in memory, even if most of them are zero. In a sparse tensor, only the non-zero elements are stored, along with their indices. This can lead to significant memory savings and/or faster compute times, especially for very sparse datasets.\nFor several tasks which are computing on the tensor, sparse tensors can be more efficient than dense tensors. For example, consider matrix multiplication. If the matrices are sparse, then sparse tensors can be orders of magnitude faster than dense tensors - as we will demonstrate at the end.\n\n\n\nIn PyTorch there are several ways to create sparse tensors as containers for sparse data. We will see how to create a sparse tensor from a dense tensor, and how to create a sparse tensor from different formats: COO, CSR, and CSC.\nFirst we’ll need example data to work with - let’s use a graph dataset as an example to understand sparse tensors.\n\n\n\n\nGraph data is a commonly sparse. A graph is a collection of nodes (or vertices) and edges. The nodes represent entities, and the edges represent relationships between the entities. More often than not, graphs are sparse, i.e. most of the nodes are not connected to each other. It is common to see datasets where less than 1% of the possible edges are present.\nThere are many ways to represent a Graph. One common is an adjacency matrix, A. This is particularly bad for sparse graphs, as most of the elements in the matrix are zero.\nThe adjacency matrix is a square matrix A, of size N x N, where N is the number of nodes in the graph. The element A[i, j] is 1 if there is an edge between node i and node j, and 0 otherwise. Since most nodes are not connected to each other, the adjacency matrix is sparse.\nAnother more memory-efficient way to represent a graph is using a tensor of edges, E. Each edge is represented as a 2 long column (r, s) in E, where r and s are the indices of the nodes connected by the edge. This representation is more memory-efficient than the adjacency matrix, as it only stores the non-zero elements, it’s size is 2 x number_edges.\nIn this notebook, we will see how to create a sparse tensor from an adjacency matrix, and how to create a sparse tensor from a list of edges. We will also see how to perform basic operations on sparse tensors, such as matrix multiplication and element-wise operations.\nLet’s consider an example from the ModelNet10 dataset.\n\n\nModelNet10 is a subset of the ModelNet dataset, which is a large-scale 3D CAD model dataset. ModelNet10 specifically consists of 10 categories, with a total of 4899 3D CAD models. The categories include: bed, chair, monitor, desk, dresser, sofa, table, toilet, night stand, and bathtub.\nEach 3D model in the dataset is represented as a graph, where each node represents a point in the 3D object, and each edge represents a relationship between the points. The adjacency matrix of the graph is sparse, as most of the points are not connected to each other.\nThis dataset is commonly used for benchmarking in tasks such as 3D object recognition, shape classification, and other machine learning tasks involving 3D data.\nLet’s look at a one: a Monitor from the training dataset. We’ll need to install some libraries to visualize the 3D object.\n\n\nCode\n!pip -q install numpy matplotlib networkx\n\n\nThe data is in off format - that is nodes and edges making faces. The particular file can be found here. The corners of the monitor are the nodes and the edges are the connections between the corners. Here’s a plot using matplotlib and another with trimesh.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\n\n\ndef read_off(file):\n    if 'OFF' != file.readline().strip():\n        raise('Not a valid OFF header')\n    n_verts, n_faces, _ = tuple(map(int, file.readline().strip().split(' ')))\n    verts = [[float(s) for s in file.readline().strip().split(' ')] for _ in range(n_verts)]\n    faces = [[int(s) for s in file.readline().strip().split(' ')[1:]] for _ in range(n_faces)]\n    return verts, faces\n\nwith open('./data/monitor_0001.off', 'r') as f:\n    verts, faces = read_off(f)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Create a Poly3DCollection object\npolys = [np.array(verts)[face] for face in faces]\ncollection = Poly3DCollection(polys, linewidths=1, alpha=1)\ncollection.set_facecolor((0,0,1,0.5))  # Set the color of the object to blue\ncollection.set_edgecolor((0,0,1,0.5))  # Set the edge color to a darker blue\nax.add_collection3d(collection)\n\n# Add lighting\nax.add_collection3d(Poly3DCollection(polys, facecolors='r', linewidths=1, edgecolors='r', alpha=.20))\n\n# Remove the axes for a cleaner look\nax.axis('off')\n\n# Auto scale to the mesh size\nscale = np.array(verts).flatten()\nax.auto_scale_xyz(scale, scale, scale)\n\n# Add a title to the plot\nax.set_title('3D Model of Monitor')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\npip -q install trimesh\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nCode\nimport trimesh\n\n# Load the mesh from the OFF file\nmesh = trimesh.load_mesh('./data/monitor_0001.off')\n\n# Plot the mesh\nmesh.show()\n\n\n\n\n\nTo see this data as a graph, we will feed in vertices and faces into the networkx package.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\n\ndef off_to_graph(verts, faces):\n    # Create a new graph\n    G = nx.Graph()\n    # Add nodes to the graph\n    for i in range(len(verts)):\n        G.add_node(i)\n    # Add edges to the graph\n    for face in faces:\n        for i in range(len(face)):\n            # Add an edge between each pair of vertices in the face\n            G.add_edge(face[i], face[(i+1)%len(face)])\n    return G\n\nwith open('./data/monitor_0001.off', 'r') as f:\n    verts, faces = read_off(f)\n\nG = off_to_graph(verts, faces)\n\n\n\n\nCode\nprint(G)\n\n\nGraph with 798 nodes and 1476 edges\n\n\n\n\nCode\n# Draw the graph\nnx.draw(G, edge_color=\"black\", width=1, node_size=5, with_labels=False)\nplt.show()\n\n\n\n\n\n\n\n\n\nSo, that’s the graph. It’s difficult to make out the shape of the monitor from the plot above, as we’re not representing the features of the nodes (3d co-ordinates).\nWhat we can do is list out the edges and vertices of the graph. First in edge list format with the corresponding nodes.\n\n\nCode\nfor index, edge in enumerate(G.edges(), start=0):\n    print(edge, end=\", \")\n    if index == 9:\n        break\n\n\n(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (1, 2), (1, 3), (3, 4), (4, 5), (4, 6), \n\n\n\n\nCode\nfor index, edge in enumerate(G.nodes(), start=0):\n    print(edge, end=\", \")\n    if index == 9:\n        break\n\n\n0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n\n\nAnd then as a an adjacency matrix.\n\n\nCode\nA = nx.to_numpy_array(G)\n\n\n\n\nCode\nA.shape\n\n\n(798, 798)\n\n\n\n\nCode\nprint(A)\n\n\n[[0. 1. 1. ... 0. 0. 0.]\n [1. 0. 1. ... 0. 0. 0.]\n [1. 1. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 1. 1.]\n [0. 0. 0. ... 1. 0. 0.]\n [0. 0. 0. ... 1. 0. 0.]]\n\n\nLet’s verify that the number of edges match the adjacency matrix and edge list view.\n\n\nCode\nA.sum()/2 == len(G.edges()) # The number of edges.\n\n\nTrue\n\n\n\n\n\n\nThe adjacency is sparse. We can quantify this sparsity - the ratio of the number of non-zero elements to the total number of elements in the tensor.\n\n\nCode\n# the ratio of the number of non-zero elements to the total number of elements in the tensor A.\nnp.count_nonzero(A) / (A.shape[0] * A.shape[1])\n\n\n0.004635649273559839\n\n\nStoring data in this format is inefficient. We can convert it to a sparse tensor. We’ll create functions for popular formats, as well as look at the native PyTorch implementations.\n\n\n\nSome well known sparse tensor representation formats are:\n\nCoordinate (COO)\nCompressed Sparse Row (CSR)\nCompressed Sparse Column (CSC)\nBlock Compressed Sparse Row (BSR)\nDictionary of Keys (DOK)\nList of Lists (LIL)\n\nWe’ll look at COO, CSR, and CSC formats.\nThe monitor graph is too large and sparse for illustration purposes. We’ll create a smaller graph to demonstrate the conversion back and forth to sparse tensors.\n\n\nCode\n# Create a new graph\nG = nx.Graph()\n\n# Add some edges to the graph\nG.add_edge(0, 1)\nG.add_edge(0, 2)\nG.add_edge(3, 4)\n\n# Draw the graph\nnx.draw(G, with_labels=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\npip -q install torch\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nCode\nimport torch\n\nA = nx.to_numpy_array(G)\nprint(torch.from_numpy(A))\nA_torch = torch.from_numpy(A).bool()\nA_torch\n\n\ntensor([[0., 1., 1., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1.],\n        [0., 0., 0., 1., 0.]], dtype=torch.float64)\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\n\n\nCode\nA.sum()/2\n\n\n3.0\n\n\nLet’s express as a pandas dataframe with source and target nodes for edges.\n\n\nCode\npip -q install pandas\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n# Get the number of nodes\nnum_nodes = A_torch.shape[0]\n\n# Create a list of all pairs of nodes\nsource_node, destination_node = np.meshgrid(np.arange(num_nodes), np.arange(num_nodes))\n\n# Flatten the arrays\nsource_node = source_node.flatten()\ndestination_node = destination_node.flatten()\n\n# Get the edge values\nhas_edge = A_torch[source_node, destination_node] &gt; 0\n\n# Create the DataFrame\ndf = pd.DataFrame({\n    'source_node': source_node,\n    'destination_node': destination_node,\n    'has_edge': has_edge\n})\n\ndf\n\n\n\n\n\n\n\n\n\nsource_node\ndestination_node\nhas_edge\n\n\n\n\n0\n0\n0\nFalse\n\n\n1\n1\n0\nTrue\n\n\n2\n2\n0\nTrue\n\n\n3\n3\n0\nFalse\n\n\n4\n4\n0\nFalse\n\n\n5\n0\n1\nTrue\n\n\n6\n1\n1\nFalse\n\n\n7\n2\n1\nFalse\n\n\n8\n3\n1\nFalse\n\n\n9\n4\n1\nFalse\n\n\n10\n0\n2\nTrue\n\n\n11\n1\n2\nFalse\n\n\n12\n2\n2\nFalse\n\n\n13\n3\n2\nFalse\n\n\n14\n4\n2\nFalse\n\n\n15\n0\n3\nFalse\n\n\n16\n1\n3\nFalse\n\n\n17\n2\n3\nFalse\n\n\n18\n3\n3\nFalse\n\n\n19\n4\n3\nTrue\n\n\n20\n0\n4\nFalse\n\n\n21\n1\n4\nFalse\n\n\n22\n2\n4\nFalse\n\n\n23\n3\n4\nTrue\n\n\n24\n4\n4\nFalse\n\n\n\n\n\n\n\n\n\nThe COO format is simple and flexible for sparse matrices. It consists of three arrays: row, col, and data. The row array contains the row indices of the non-zero elements, the col array contains the column indices of the non-zero elements, and the data array contains the values of the non-zero elements.\nLuckily, PyTorch has built in methods for converting between dense and sparse tensors. We can convert the adjacency matrix to a COO sparse tensor using the .to_sparse() method.\n\n\nCode\nA_coo_pytorch = A_torch.to_sparse()\nA_coo_pytorch\n\n\ntensor(indices=tensor([[0, 0, 1, 2, 3, 4],\n                       [1, 2, 0, 0, 4, 3]]),\n       values=tensor([True, True, True, True, True, True]),\n       size=(5, 5), nnz=6, layout=torch.sparse_coo)\n\n\nAnd convert back\n\n\nCode\nA_coo_pytorch.to_dense()\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\nLet’s look at the sparse tensor as a dataframe of source to destination nodes, but this time only for those pairs where there is an edge.\n\n\nCode\n# Get the source and destination nodes and the edge values\nsource_node = A_coo_pytorch.indices()[0].numpy()\ndestination_node = A_coo_pytorch.indices()[1].numpy()\nhas_edge = A_coo_pytorch.values().numpy()\n\n# Create the DataFrame\ndf = pd.DataFrame({\n    'source_node': source_node,\n    'destination_node': destination_node,\n    'has_edge': has_edge\n})\ndf\n\n\n\n\n\n\n\n\n\nsource_node\ndestination_node\nhas_edge\n\n\n\n\n0\n0\n1\nTrue\n\n\n1\n0\n2\nTrue\n\n\n2\n1\n0\nTrue\n\n\n3\n2\n0\nTrue\n\n\n4\n3\n4\nTrue\n\n\n5\n4\n3\nTrue\n\n\n\n\n\n\n\nWriting our own homebrew function to convert to COO format, is straightforward.\n\n\nCode\ndef to_sparse(tensor):\n    # Get the indices of the non-zero elements\n    indices = torch.nonzero(tensor).t()\n    # Get the values of the non-zero elements\n    values = tensor[indices[0], indices[1]]  # assuming 2D tensor\n    # Get the size of the original tensor\n    size = tensor.size()\n    # Create a sparse tensor\n    sparse_tensor = torch.sparse_coo_tensor(indices, values, size)\n    return sparse_tensor\n\n\n\n\nCode\nto_sparse(A_torch)\n\n\ntensor(indices=tensor([[0, 0, 1, 2, 3, 4],\n                       [1, 2, 0, 0, 4, 3]]),\n       values=tensor([True, True, True, True, True, True]),\n       size=(5, 5), nnz=6, layout=torch.sparse_coo)\n\n\n\n\nCode\ndef to_dense(sparse_tensor):\n    # Get the size of the original tensor\n    size = sparse_tensor.size()\n    # Get the indices and values from the sparse tensor\n    indices = sparse_tensor.coalesce().indices()\n    values = sparse_tensor.coalesce().values()\n    # Create a dense tensor of the same size and data type as values, initialized with zeros\n    dense_tensor = torch.zeros(size, dtype=values.dtype)\n    # Convert indices to a tuple of tensors\n    indices_tuple = tuple(indices[i] for i in range(indices.shape[0]))\n    # Use index_put_ to put the values in the dense tensor at the right indices\n    dense_tensor.index_put_(indices_tuple, values, accumulate=False)\n    return dense_tensor\n\n\n\n\nCode\nto_dense(A_coo_pytorch)\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\n\n\n\nCSR is an even more efficient compact format for sparse data. Details can be found here.\nValues and column indices are stored in the same way as COO format. The row indices are stored in a separate array, which can seem strange at first glance.\nBroadly it relies on slicing the values into per row chunks. The row chunk lengths are calculated as the difference between the row indices of the non-zero elements. The row chunk lengths are stored in a separate array, row_ptr.\nSo, if your original matrix has m rows, n columns, and nnz non-zero values, then:\n\nThe shape of values is [nnz].\nThe shape of column_indices is [nnz].\nThe shape of row_pointers is [m+1].\n\nIn index notation, the row pointer array is defined as:\nrow_ptr[i] = row_ptr[i-1] + number of non-zero elements in row i-1\nSo, the row pointer array stores the cumulative sum of the number of non-zero elements in each row.\nthe row and column number can be determined by the index of the non-zero element in the values array. That is the i-th non-zero element (in the values array) is at row row_ptr[i] and column col[i]. That is:\nvalues[i] = A[row_ptr[i], col[i]]\n\n\nCode\nA_torch.to_sparse_csr()\n\n\ntensor(crow_indices=tensor([0, 2, 3, 4, 5, 6]),\n       col_indices=tensor([1, 2, 0, 0, 4, 3]),\n       values=tensor([True, True, True, True, True, True]), size=(5, 5), nnz=6,\n       layout=torch.sparse_csr)\n\n\nWhich can be recovered by chaining with the .to_dense() method.\n\n\nCode\nA_torch.to_sparse_csr().to_dense()\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\nAnd here we write our homebrew version of to_csr() and from_csr() functions:\n\n\nCode\ndef to_csr(tensor):\n    # Get the indices of the non-zero elements\n    indices = torch.nonzero(tensor, as_tuple=True)\n    # Get the values of the non-zero elements\n    values = tensor[indices]\n    # Get the column indices of the non-zero elements\n    column_indices = indices[1]\n    # Get the row pointers\n    row_pointers = torch.zeros(tensor.size(0) + 1, dtype=torch.long)\n    row_pointers[1:] = torch.bincount(indices[0])\n    row_pointers = torch.cumsum(row_pointers, dim=0)\n\n    return values, column_indices, row_pointers\n\n\n\n\nCode\nto_csr(A_torch)\n\n\n(tensor([True, True, True, True, True, True]),\n tensor([1, 2, 0, 0, 4, 3]),\n tensor([0, 2, 3, 4, 5, 6]))\n\n\n\n\nCode\ndef from_csr(values, column_indices, row_pointers):\n    # Get the number of rows and columns\n    num_rows = row_pointers.size(0) - 1\n    num_cols = torch.max(column_indices).item() + 1\n    # Create a dense tensor of the right size, initialized with zeros\n    tensor = torch.zeros((num_rows, num_cols), dtype=values.dtype)\n    # Loop over the rows\n    for i in range(num_rows):\n        # Get the start and end indices for this row in the values and column_indices arrays\n        start = row_pointers[i].item()\n        end = row_pointers[i + 1].item()\n        # Set the values in the dense tensor for this row\n        tensor[i, column_indices[start:end]] = values[start:end]\n\n    return tensor\n\n\n\n\nCode\nfrom_csr(*to_csr(A_torch))\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\n\n\n\nThe CSC format is similar to the CSR format, but with the rows and columns swapped. The values and row indices are stored in the same way as the CSR format. The column indices are stored in a separate array, col_ptr.\nThis format is efficient for certain slicing operations, such as extracting a column from a matrix.\n\n\n\n\nLet’s see how much faster it is to perform matrix multiplication on the sparse tensor compared to the dense tensor. For this we’ll use the ModelNet10 monitor graph seen earlier.\n\n\nCode\nwith open('./data/monitor_0001.off', 'r') as f:\n    verts, faces = read_off(f)\n\nG = off_to_graph(verts, faces)\nA = nx.adjacency_matrix(G)\nA\n\n\n&lt;798x798 sparse array of type '&lt;class 'numpy.int64'&gt;'\n    with 2952 stored elements in Compressed Sparse Row format&gt;\n\n\nA is already a sparse matrix, but in numpy. We’ll convert it to a PyTorch dense tensor first.\n\n\nCode\nA_dense = torch.from_numpy(A.toarray())\nA_dense.shape\n\n\ntorch.Size([798, 798])\n\n\n\n\nCode\ndef tensor_memory_usage_str(tensor=None):\n    if tensor is None:\n        return \"No tensor provided\"\n    bytes = tensor.element_size() * tensor.nelement()\n    kilobytes = bytes / 1024\n    return f\"Memory usage: {bytes} bytes ({kilobytes} KB)\"\n\n\n\n\nCode\ntensor_memory_usage_str(A_dense)\n\n\n'Memory usage: 5094432 bytes (4975.03125 KB)'\n\n\n\n\nCode\ntensor_memory_usage_str(A_dense.to_sparse())\n\n\n'Memory usage: 5094432 bytes (4975.03125 KB)'\n\n\nSomewhat unexpectedly, the sparse and dense tensors have the same memory usage.\n\n\nCode\nA_sparse = A_dense.to_sparse_csr()\n\n\n\n\nCode\ntensor_memory_usage_str(A_sparse)\n\n\n'Memory usage: 5094432 bytes (4975.03125 KB)'\n\n\nOK, so there was no improvement in memory usage for this particular case. Let’s time the matrix multiplication operations.\n\n\nCode\nimport timeit\n\n\ndef multiply_tensors():\n    return A_dense @ A_dense\n\nnum_runs = 10\ntime_taken_dense = timeit.timeit( multiply_tensors, number=num_runs)\n\nprint(f\"Time taken by my_function over {num_runs} runs: {time_taken_dense} seconds\")\n\n\nTime taken by my_function over 10 runs: 2.746476124972105 seconds\n\n\n\n\nCode\nA_coo = A\n\ndef multiply_tensors():\n    return A_coo@A_coo\n\nnum_runs = 10\ntime_taken_sparse = timeit.timeit( multiply_tensors, number=num_runs)\n\nprint(f\"Time taken by my_function over {num_runs} runs: {time_taken_sparse} seconds\")\n\n\nTime taken by my_function over 10 runs: 0.0038774579879827797 seconds\n\n\n\n\nCode\nf\"Speedup: {time_taken_dense / time_taken_sparse}x\"\n\n\n'Speedup: 708.3187318815902x'\n\n\nWith CSR encoding of sparse tensors, it might be even better - however I’m on Apple Silicon, and the csr sparse tensor support is not yet available for this architecture.\n\n\n\nWe just saw how to work with sparse tensors in PyTorch. We created a sparse tensor from an adjacency matrix and a list of edges, and converted it to different sparse formats. We also saw the performance benefits of using sparse tensors for matrix multiplication. There’s a lot more to sparsity, and PyTorch is actively developing its sparse tensor support. Things will break, but expect ramp up in performance gains."
  },
  {
    "objectID": "posts/sparse_tensors/index.html#introduction",
    "href": "posts/sparse_tensors/index.html#introduction",
    "title": "Sparsity with PyTorch Tensors",
    "section": "",
    "text": "The canonical format for representing tensors in PyTorch is the dense tensor, with associated contiguous memory proportional to the size of the tensor. However, in many applications, the data is sparse, that is most of the elements are zero. In this notebook, we will see how to work efficiently with sparse tensors in PyTorch.\n\n\nA dataset is considered sparse when most of its elements are zero, for some features of the dataset. The exact threshold for considering a dataset as sparse is not strictly defined and can depend on the context and the specific application. A common rule of thumb is that a dataset is sparse if over 50% of its elements are zero.\nIn many real-world applications, datasets can be extremely sparse, with over 90% or even 99% of elements being zero. For example, in a user-item interaction matrix in a recommendation system, each user typically interacts with only a small fraction of all possible items, so the vast majority of the matrix elements are zero.\nIn PyTorch, datasets are stored in tensors. A tensor is a multi-dimensional array, similar to a NumPy array. PyTorch provides some support for sparse tensors.\n\n\n\nIn a dense tensor, all elements are stored in memory, even if most of them are zero. In a sparse tensor, only the non-zero elements are stored, along with their indices. This can lead to significant memory savings and/or faster compute times, especially for very sparse datasets.\nFor several tasks which are computing on the tensor, sparse tensors can be more efficient than dense tensors. For example, consider matrix multiplication. If the matrices are sparse, then sparse tensors can be orders of magnitude faster than dense tensors - as we will demonstrate at the end.\n\n\n\nIn PyTorch there are several ways to create sparse tensors as containers for sparse data. We will see how to create a sparse tensor from a dense tensor, and how to create a sparse tensor from different formats: COO, CSR, and CSC.\nFirst we’ll need example data to work with - let’s use a graph dataset as an example to understand sparse tensors."
  },
  {
    "objectID": "posts/sparse_tensors/index.html#example-graph-data",
    "href": "posts/sparse_tensors/index.html#example-graph-data",
    "title": "Sparsity with PyTorch Tensors",
    "section": "",
    "text": "Graph data is a commonly sparse. A graph is a collection of nodes (or vertices) and edges. The nodes represent entities, and the edges represent relationships between the entities. More often than not, graphs are sparse, i.e. most of the nodes are not connected to each other. It is common to see datasets where less than 1% of the possible edges are present.\nThere are many ways to represent a Graph. One common is an adjacency matrix, A. This is particularly bad for sparse graphs, as most of the elements in the matrix are zero.\nThe adjacency matrix is a square matrix A, of size N x N, where N is the number of nodes in the graph. The element A[i, j] is 1 if there is an edge between node i and node j, and 0 otherwise. Since most nodes are not connected to each other, the adjacency matrix is sparse.\nAnother more memory-efficient way to represent a graph is using a tensor of edges, E. Each edge is represented as a 2 long column (r, s) in E, where r and s are the indices of the nodes connected by the edge. This representation is more memory-efficient than the adjacency matrix, as it only stores the non-zero elements, it’s size is 2 x number_edges.\nIn this notebook, we will see how to create a sparse tensor from an adjacency matrix, and how to create a sparse tensor from a list of edges. We will also see how to perform basic operations on sparse tensors, such as matrix multiplication and element-wise operations.\nLet’s consider an example from the ModelNet10 dataset.\n\n\nModelNet10 is a subset of the ModelNet dataset, which is a large-scale 3D CAD model dataset. ModelNet10 specifically consists of 10 categories, with a total of 4899 3D CAD models. The categories include: bed, chair, monitor, desk, dresser, sofa, table, toilet, night stand, and bathtub.\nEach 3D model in the dataset is represented as a graph, where each node represents a point in the 3D object, and each edge represents a relationship between the points. The adjacency matrix of the graph is sparse, as most of the points are not connected to each other.\nThis dataset is commonly used for benchmarking in tasks such as 3D object recognition, shape classification, and other machine learning tasks involving 3D data.\nLet’s look at a one: a Monitor from the training dataset. We’ll need to install some libraries to visualize the 3D object.\n\n\nCode\n!pip -q install numpy matplotlib networkx\n\n\nThe data is in off format - that is nodes and edges making faces. The particular file can be found here. The corners of the monitor are the nodes and the edges are the connections between the corners. Here’s a plot using matplotlib and another with trimesh.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\n\n\ndef read_off(file):\n    if 'OFF' != file.readline().strip():\n        raise('Not a valid OFF header')\n    n_verts, n_faces, _ = tuple(map(int, file.readline().strip().split(' ')))\n    verts = [[float(s) for s in file.readline().strip().split(' ')] for _ in range(n_verts)]\n    faces = [[int(s) for s in file.readline().strip().split(' ')[1:]] for _ in range(n_faces)]\n    return verts, faces\n\nwith open('./data/monitor_0001.off', 'r') as f:\n    verts, faces = read_off(f)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Create a Poly3DCollection object\npolys = [np.array(verts)[face] for face in faces]\ncollection = Poly3DCollection(polys, linewidths=1, alpha=1)\ncollection.set_facecolor((0,0,1,0.5))  # Set the color of the object to blue\ncollection.set_edgecolor((0,0,1,0.5))  # Set the edge color to a darker blue\nax.add_collection3d(collection)\n\n# Add lighting\nax.add_collection3d(Poly3DCollection(polys, facecolors='r', linewidths=1, edgecolors='r', alpha=.20))\n\n# Remove the axes for a cleaner look\nax.axis('off')\n\n# Auto scale to the mesh size\nscale = np.array(verts).flatten()\nax.auto_scale_xyz(scale, scale, scale)\n\n# Add a title to the plot\nax.set_title('3D Model of Monitor')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\npip -q install trimesh\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nCode\nimport trimesh\n\n# Load the mesh from the OFF file\nmesh = trimesh.load_mesh('./data/monitor_0001.off')\n\n# Plot the mesh\nmesh.show()\n\n\n\n\n\nTo see this data as a graph, we will feed in vertices and faces into the networkx package.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\n\ndef off_to_graph(verts, faces):\n    # Create a new graph\n    G = nx.Graph()\n    # Add nodes to the graph\n    for i in range(len(verts)):\n        G.add_node(i)\n    # Add edges to the graph\n    for face in faces:\n        for i in range(len(face)):\n            # Add an edge between each pair of vertices in the face\n            G.add_edge(face[i], face[(i+1)%len(face)])\n    return G\n\nwith open('./data/monitor_0001.off', 'r') as f:\n    verts, faces = read_off(f)\n\nG = off_to_graph(verts, faces)\n\n\n\n\nCode\nprint(G)\n\n\nGraph with 798 nodes and 1476 edges\n\n\n\n\nCode\n# Draw the graph\nnx.draw(G, edge_color=\"black\", width=1, node_size=5, with_labels=False)\nplt.show()\n\n\n\n\n\n\n\n\n\nSo, that’s the graph. It’s difficult to make out the shape of the monitor from the plot above, as we’re not representing the features of the nodes (3d co-ordinates).\nWhat we can do is list out the edges and vertices of the graph. First in edge list format with the corresponding nodes.\n\n\nCode\nfor index, edge in enumerate(G.edges(), start=0):\n    print(edge, end=\", \")\n    if index == 9:\n        break\n\n\n(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (1, 2), (1, 3), (3, 4), (4, 5), (4, 6), \n\n\n\n\nCode\nfor index, edge in enumerate(G.nodes(), start=0):\n    print(edge, end=\", \")\n    if index == 9:\n        break\n\n\n0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n\n\nAnd then as a an adjacency matrix.\n\n\nCode\nA = nx.to_numpy_array(G)\n\n\n\n\nCode\nA.shape\n\n\n(798, 798)\n\n\n\n\nCode\nprint(A)\n\n\n[[0. 1. 1. ... 0. 0. 0.]\n [1. 0. 1. ... 0. 0. 0.]\n [1. 1. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 1. 1.]\n [0. 0. 0. ... 1. 0. 0.]\n [0. 0. 0. ... 1. 0. 0.]]\n\n\nLet’s verify that the number of edges match the adjacency matrix and edge list view.\n\n\nCode\nA.sum()/2 == len(G.edges()) # The number of edges.\n\n\nTrue"
  },
  {
    "objectID": "posts/sparse_tensors/index.html#sparse-adjacency",
    "href": "posts/sparse_tensors/index.html#sparse-adjacency",
    "title": "Sparsity with PyTorch Tensors",
    "section": "",
    "text": "The adjacency is sparse. We can quantify this sparsity - the ratio of the number of non-zero elements to the total number of elements in the tensor.\n\n\nCode\n# the ratio of the number of non-zero elements to the total number of elements in the tensor A.\nnp.count_nonzero(A) / (A.shape[0] * A.shape[1])\n\n\n0.004635649273559839\n\n\nStoring data in this format is inefficient. We can convert it to a sparse tensor. We’ll create functions for popular formats, as well as look at the native PyTorch implementations."
  },
  {
    "objectID": "posts/sparse_tensors/index.html#sparse-formats",
    "href": "posts/sparse_tensors/index.html#sparse-formats",
    "title": "Sparsity with PyTorch Tensors",
    "section": "",
    "text": "Some well known sparse tensor representation formats are:\n\nCoordinate (COO)\nCompressed Sparse Row (CSR)\nCompressed Sparse Column (CSC)\nBlock Compressed Sparse Row (BSR)\nDictionary of Keys (DOK)\nList of Lists (LIL)\n\nWe’ll look at COO, CSR, and CSC formats.\nThe monitor graph is too large and sparse for illustration purposes. We’ll create a smaller graph to demonstrate the conversion back and forth to sparse tensors.\n\n\nCode\n# Create a new graph\nG = nx.Graph()\n\n# Add some edges to the graph\nG.add_edge(0, 1)\nG.add_edge(0, 2)\nG.add_edge(3, 4)\n\n# Draw the graph\nnx.draw(G, with_labels=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\npip -q install torch\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nCode\nimport torch\n\nA = nx.to_numpy_array(G)\nprint(torch.from_numpy(A))\nA_torch = torch.from_numpy(A).bool()\nA_torch\n\n\ntensor([[0., 1., 1., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1.],\n        [0., 0., 0., 1., 0.]], dtype=torch.float64)\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\n\n\nCode\nA.sum()/2\n\n\n3.0\n\n\nLet’s express as a pandas dataframe with source and target nodes for edges.\n\n\nCode\npip -q install pandas\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n# Get the number of nodes\nnum_nodes = A_torch.shape[0]\n\n# Create a list of all pairs of nodes\nsource_node, destination_node = np.meshgrid(np.arange(num_nodes), np.arange(num_nodes))\n\n# Flatten the arrays\nsource_node = source_node.flatten()\ndestination_node = destination_node.flatten()\n\n# Get the edge values\nhas_edge = A_torch[source_node, destination_node] &gt; 0\n\n# Create the DataFrame\ndf = pd.DataFrame({\n    'source_node': source_node,\n    'destination_node': destination_node,\n    'has_edge': has_edge\n})\n\ndf\n\n\n\n\n\n\n\n\n\nsource_node\ndestination_node\nhas_edge\n\n\n\n\n0\n0\n0\nFalse\n\n\n1\n1\n0\nTrue\n\n\n2\n2\n0\nTrue\n\n\n3\n3\n0\nFalse\n\n\n4\n4\n0\nFalse\n\n\n5\n0\n1\nTrue\n\n\n6\n1\n1\nFalse\n\n\n7\n2\n1\nFalse\n\n\n8\n3\n1\nFalse\n\n\n9\n4\n1\nFalse\n\n\n10\n0\n2\nTrue\n\n\n11\n1\n2\nFalse\n\n\n12\n2\n2\nFalse\n\n\n13\n3\n2\nFalse\n\n\n14\n4\n2\nFalse\n\n\n15\n0\n3\nFalse\n\n\n16\n1\n3\nFalse\n\n\n17\n2\n3\nFalse\n\n\n18\n3\n3\nFalse\n\n\n19\n4\n3\nTrue\n\n\n20\n0\n4\nFalse\n\n\n21\n1\n4\nFalse\n\n\n22\n2\n4\nFalse\n\n\n23\n3\n4\nTrue\n\n\n24\n4\n4\nFalse\n\n\n\n\n\n\n\n\n\nThe COO format is simple and flexible for sparse matrices. It consists of three arrays: row, col, and data. The row array contains the row indices of the non-zero elements, the col array contains the column indices of the non-zero elements, and the data array contains the values of the non-zero elements.\nLuckily, PyTorch has built in methods for converting between dense and sparse tensors. We can convert the adjacency matrix to a COO sparse tensor using the .to_sparse() method.\n\n\nCode\nA_coo_pytorch = A_torch.to_sparse()\nA_coo_pytorch\n\n\ntensor(indices=tensor([[0, 0, 1, 2, 3, 4],\n                       [1, 2, 0, 0, 4, 3]]),\n       values=tensor([True, True, True, True, True, True]),\n       size=(5, 5), nnz=6, layout=torch.sparse_coo)\n\n\nAnd convert back\n\n\nCode\nA_coo_pytorch.to_dense()\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\nLet’s look at the sparse tensor as a dataframe of source to destination nodes, but this time only for those pairs where there is an edge.\n\n\nCode\n# Get the source and destination nodes and the edge values\nsource_node = A_coo_pytorch.indices()[0].numpy()\ndestination_node = A_coo_pytorch.indices()[1].numpy()\nhas_edge = A_coo_pytorch.values().numpy()\n\n# Create the DataFrame\ndf = pd.DataFrame({\n    'source_node': source_node,\n    'destination_node': destination_node,\n    'has_edge': has_edge\n})\ndf\n\n\n\n\n\n\n\n\n\nsource_node\ndestination_node\nhas_edge\n\n\n\n\n0\n0\n1\nTrue\n\n\n1\n0\n2\nTrue\n\n\n2\n1\n0\nTrue\n\n\n3\n2\n0\nTrue\n\n\n4\n3\n4\nTrue\n\n\n5\n4\n3\nTrue\n\n\n\n\n\n\n\nWriting our own homebrew function to convert to COO format, is straightforward.\n\n\nCode\ndef to_sparse(tensor):\n    # Get the indices of the non-zero elements\n    indices = torch.nonzero(tensor).t()\n    # Get the values of the non-zero elements\n    values = tensor[indices[0], indices[1]]  # assuming 2D tensor\n    # Get the size of the original tensor\n    size = tensor.size()\n    # Create a sparse tensor\n    sparse_tensor = torch.sparse_coo_tensor(indices, values, size)\n    return sparse_tensor\n\n\n\n\nCode\nto_sparse(A_torch)\n\n\ntensor(indices=tensor([[0, 0, 1, 2, 3, 4],\n                       [1, 2, 0, 0, 4, 3]]),\n       values=tensor([True, True, True, True, True, True]),\n       size=(5, 5), nnz=6, layout=torch.sparse_coo)\n\n\n\n\nCode\ndef to_dense(sparse_tensor):\n    # Get the size of the original tensor\n    size = sparse_tensor.size()\n    # Get the indices and values from the sparse tensor\n    indices = sparse_tensor.coalesce().indices()\n    values = sparse_tensor.coalesce().values()\n    # Create a dense tensor of the same size and data type as values, initialized with zeros\n    dense_tensor = torch.zeros(size, dtype=values.dtype)\n    # Convert indices to a tuple of tensors\n    indices_tuple = tuple(indices[i] for i in range(indices.shape[0]))\n    # Use index_put_ to put the values in the dense tensor at the right indices\n    dense_tensor.index_put_(indices_tuple, values, accumulate=False)\n    return dense_tensor\n\n\n\n\nCode\nto_dense(A_coo_pytorch)\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\n\n\n\nCSR is an even more efficient compact format for sparse data. Details can be found here.\nValues and column indices are stored in the same way as COO format. The row indices are stored in a separate array, which can seem strange at first glance.\nBroadly it relies on slicing the values into per row chunks. The row chunk lengths are calculated as the difference between the row indices of the non-zero elements. The row chunk lengths are stored in a separate array, row_ptr.\nSo, if your original matrix has m rows, n columns, and nnz non-zero values, then:\n\nThe shape of values is [nnz].\nThe shape of column_indices is [nnz].\nThe shape of row_pointers is [m+1].\n\nIn index notation, the row pointer array is defined as:\nrow_ptr[i] = row_ptr[i-1] + number of non-zero elements in row i-1\nSo, the row pointer array stores the cumulative sum of the number of non-zero elements in each row.\nthe row and column number can be determined by the index of the non-zero element in the values array. That is the i-th non-zero element (in the values array) is at row row_ptr[i] and column col[i]. That is:\nvalues[i] = A[row_ptr[i], col[i]]\n\n\nCode\nA_torch.to_sparse_csr()\n\n\ntensor(crow_indices=tensor([0, 2, 3, 4, 5, 6]),\n       col_indices=tensor([1, 2, 0, 0, 4, 3]),\n       values=tensor([True, True, True, True, True, True]), size=(5, 5), nnz=6,\n       layout=torch.sparse_csr)\n\n\nWhich can be recovered by chaining with the .to_dense() method.\n\n\nCode\nA_torch.to_sparse_csr().to_dense()\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\nAnd here we write our homebrew version of to_csr() and from_csr() functions:\n\n\nCode\ndef to_csr(tensor):\n    # Get the indices of the non-zero elements\n    indices = torch.nonzero(tensor, as_tuple=True)\n    # Get the values of the non-zero elements\n    values = tensor[indices]\n    # Get the column indices of the non-zero elements\n    column_indices = indices[1]\n    # Get the row pointers\n    row_pointers = torch.zeros(tensor.size(0) + 1, dtype=torch.long)\n    row_pointers[1:] = torch.bincount(indices[0])\n    row_pointers = torch.cumsum(row_pointers, dim=0)\n\n    return values, column_indices, row_pointers\n\n\n\n\nCode\nto_csr(A_torch)\n\n\n(tensor([True, True, True, True, True, True]),\n tensor([1, 2, 0, 0, 4, 3]),\n tensor([0, 2, 3, 4, 5, 6]))\n\n\n\n\nCode\ndef from_csr(values, column_indices, row_pointers):\n    # Get the number of rows and columns\n    num_rows = row_pointers.size(0) - 1\n    num_cols = torch.max(column_indices).item() + 1\n    # Create a dense tensor of the right size, initialized with zeros\n    tensor = torch.zeros((num_rows, num_cols), dtype=values.dtype)\n    # Loop over the rows\n    for i in range(num_rows):\n        # Get the start and end indices for this row in the values and column_indices arrays\n        start = row_pointers[i].item()\n        end = row_pointers[i + 1].item()\n        # Set the values in the dense tensor for this row\n        tensor[i, column_indices[start:end]] = values[start:end]\n\n    return tensor\n\n\n\n\nCode\nfrom_csr(*to_csr(A_torch))\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\n\n\n\nThe CSC format is similar to the CSR format, but with the rows and columns swapped. The values and row indices are stored in the same way as the CSR format. The column indices are stored in a separate array, col_ptr.\nThis format is efficient for certain slicing operations, such as extracting a column from a matrix."
  },
  {
    "objectID": "posts/sparse_tensors/index.html#memory-usage-speed-up-with-sparse-tensors-on-the-monitor-graph",
    "href": "posts/sparse_tensors/index.html#memory-usage-speed-up-with-sparse-tensors-on-the-monitor-graph",
    "title": "Sparsity with PyTorch Tensors",
    "section": "",
    "text": "Let’s see how much faster it is to perform matrix multiplication on the sparse tensor compared to the dense tensor. For this we’ll use the ModelNet10 monitor graph seen earlier.\n\n\nCode\nwith open('./data/monitor_0001.off', 'r') as f:\n    verts, faces = read_off(f)\n\nG = off_to_graph(verts, faces)\nA = nx.adjacency_matrix(G)\nA\n\n\n&lt;798x798 sparse array of type '&lt;class 'numpy.int64'&gt;'\n    with 2952 stored elements in Compressed Sparse Row format&gt;\n\n\nA is already a sparse matrix, but in numpy. We’ll convert it to a PyTorch dense tensor first.\n\n\nCode\nA_dense = torch.from_numpy(A.toarray())\nA_dense.shape\n\n\ntorch.Size([798, 798])\n\n\n\n\nCode\ndef tensor_memory_usage_str(tensor=None):\n    if tensor is None:\n        return \"No tensor provided\"\n    bytes = tensor.element_size() * tensor.nelement()\n    kilobytes = bytes / 1024\n    return f\"Memory usage: {bytes} bytes ({kilobytes} KB)\"\n\n\n\n\nCode\ntensor_memory_usage_str(A_dense)\n\n\n'Memory usage: 5094432 bytes (4975.03125 KB)'\n\n\n\n\nCode\ntensor_memory_usage_str(A_dense.to_sparse())\n\n\n'Memory usage: 5094432 bytes (4975.03125 KB)'\n\n\nSomewhat unexpectedly, the sparse and dense tensors have the same memory usage.\n\n\nCode\nA_sparse = A_dense.to_sparse_csr()\n\n\n\n\nCode\ntensor_memory_usage_str(A_sparse)\n\n\n'Memory usage: 5094432 bytes (4975.03125 KB)'\n\n\nOK, so there was no improvement in memory usage for this particular case. Let’s time the matrix multiplication operations.\n\n\nCode\nimport timeit\n\n\ndef multiply_tensors():\n    return A_dense @ A_dense\n\nnum_runs = 10\ntime_taken_dense = timeit.timeit( multiply_tensors, number=num_runs)\n\nprint(f\"Time taken by my_function over {num_runs} runs: {time_taken_dense} seconds\")\n\n\nTime taken by my_function over 10 runs: 2.746476124972105 seconds\n\n\n\n\nCode\nA_coo = A\n\ndef multiply_tensors():\n    return A_coo@A_coo\n\nnum_runs = 10\ntime_taken_sparse = timeit.timeit( multiply_tensors, number=num_runs)\n\nprint(f\"Time taken by my_function over {num_runs} runs: {time_taken_sparse} seconds\")\n\n\nTime taken by my_function over 10 runs: 0.0038774579879827797 seconds\n\n\n\n\nCode\nf\"Speedup: {time_taken_dense / time_taken_sparse}x\"\n\n\n'Speedup: 708.3187318815902x'\n\n\nWith CSR encoding of sparse tensors, it might be even better - however I’m on Apple Silicon, and the csr sparse tensor support is not yet available for this architecture."
  },
  {
    "objectID": "posts/sparse_tensors/index.html#conclusion",
    "href": "posts/sparse_tensors/index.html#conclusion",
    "title": "Sparsity with PyTorch Tensors",
    "section": "",
    "text": "We just saw how to work with sparse tensors in PyTorch. We created a sparse tensor from an adjacency matrix and a list of edges, and converted it to different sparse formats. We also saw the performance benefits of using sparse tensors for matrix multiplication. There’s a lot more to sparsity, and PyTorch is actively developing its sparse tensor support. Things will break, but expect ramp up in performance gains."
  }
]
