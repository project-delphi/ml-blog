[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To Synthetic Musings",
    "section": "",
    "text": "This is the first blog entry by me Ravi (@project-delphi). Welcome!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code and Plot",
    "section": "",
    "text": "Let’s see if blogging with code and plots works here:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(0, 10, 100)\n\nfig = plt.figure()\nplt.plot(x, np.sin(x), '-')\nplt.plot(x, np.cos(x), '--')\n\n\n\n\n\n\n\n\n\n\nkindly taken from Jake Vanderplas’s blog\n\nSuccess!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Synthetic Musings",
    "section": "",
    "text": "Featured Writing\n\n\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\n\nMay 11, 2024\n\n\nGit Repository Search\n\n\n4 min\n\n\n\n\n\n\n\nApr 11, 2024\n\n\nSparsity with PyTorch Tensors\n\n\n9 min\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nDeveloping Pytorch Geometric on M1 Apple Silicon\n\n\n7 min\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nPost With Code and Plot\n\n\n1 min\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nWelcome To Synthetic Musings\n\n\n1 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/developing-pytorch-geometric-on-m1/index.html",
    "href": "posts/developing-pytorch-geometric-on-m1/index.html",
    "title": "Developing Pytorch Geometric on M1 Apple Silicon",
    "section": "",
    "text": "TLDR: My investigation indicates that the library pytorch geometric (also referred to as pyg, using package name torch_geometric) on Apple Silicon (AS) - particularly the M1 chip - has partial developer support.\nA bit more: It is now feasible to perform a full developer installation of torch_geometric using PyTorch on AS with Apple’s native GPU compute framework, Metal Performance Shaders - mps - (which includes its own kind of embedded BLAS). The only remaining challenge is testing with a developer installation. The test suite does not currently build for AS.\nI’ve been talking with the pyg-team and they are interested in supporting AS more fully. It’s a matter of time and resources. rusty1s (the lead developer) has been very helpful and responsive, adding a test runner for Apple Silicon."
  },
  {
    "objectID": "posts/developing-pytorch-geometric-on-m1/index.html#user-installation",
    "href": "posts/developing-pytorch-geometric-on-m1/index.html#user-installation",
    "title": "Developing Pytorch Geometric on M1 Apple Silicon",
    "section": "User Installation",
    "text": "User Installation\nThe installation of the package as a user, utilizing a wheels distribution, is generally straightforward across most CPU/GPU architectures, including AS. Here are the steps:\n\nCheck for conda, install if missing.\nEstablish a clean Python virtual environment.\nActivate the newly created virtual environment.\nInstall the latest version of PyTorch.\nProceed with the installation of PyTorch Geometric.\nCheck that it imports without errors.\n\nThere’s a gist of what worked for me. The cell below uses the %load magic command to load the bash script into the code cell, followed by running it with insertion of the %%bash magic command at the top of the code block.\n\n%%bash\n\n# %load https://gist.githubusercontent.com/project-delphi/38d1db47ed28dde3c8418d5f435c865c/raw/665c079a1f6eff72207309ed97dd6b49194df812/pyg_user_install.sh\n# set variables here\nDIR=\"$HOME/Code/throwaway/pytorch-geometric-user-install\"\nPYTHON_VERSION=3.11\nRECENT_TORCH_VERSION=2.2.0\n\n# install miniconda for apple silicon, if not already installed\nif [ -d \"$HOME/anaconda3\" ] || [ -d \"$HOME/miniconda3\" ]\nthen\n    echo \"Conda is installed\"\nelse\n    curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n    sh Miniconda3-latest-MacOSX-arm64.sh -b -u  &gt; /dev/null 2&gt;&1\nfi\n\nmkdir -p \"$DIR\"\ncd \"$DIR\"\nconda create --yes  -p $DIR/.venv python=$PYTHON_VERSION &gt; /dev/null 2&gt;&1\neval \"$(conda shell.bash hook)\"\nconda activate $DIR/.venv\npip install -q --upgrade pip\n\n##### TORCH BUILD AND INSTALL ON M1, to use GPUs #####\npip install -q numpy # to remove user warning with torch install\npip install -q mpmath==1.3.0 # bugfix\nxcode-select --install  &gt; /dev/null 2&gt;&1 # if xcode not installed\n\n###### install torch ######\npip install -q --pre torch==$RECENT_TORCH_VERSION torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu \n\n# install torch geometric\npip install -q torch-geometric\n\n# check\npython --version\npython -c \"import torch; print(f'torch version: {torch.__version__}')\"\npython -c \"import torch_geometric as pyg; print(f'torch geometric version: {pyg.__version__}')\"\n\nConda is installed\nPython 3.11.8\ntorch version: 2.2.0\ntorch geometric version: 2.5.1\n\n\nWhich is great for using torch_geometric on my M1 Pro.\nThe problem is that I want to develop (and not just use) locally - that is to have an editable local install on my 2021 16 inch Macbook Pro M1 Pro. Sadly the M1 architecture does not have official support by pyg-team 😔 (I suspect the issues observed also apply to later Apple Silicon too.)\nLet’s see how far we can get with the installation process."
  },
  {
    "objectID": "posts/developing-pytorch-geometric-on-m1/index.html#developer-installation",
    "href": "posts/developing-pytorch-geometric-on-m1/index.html#developer-installation",
    "title": "Developing Pytorch Geometric on M1 Apple Silicon",
    "section": "Developer Installation",
    "text": "Developer Installation\n\nNon Apple Silicon Machines\nFrom the project contributing guidelines, the instructions are clear.\n\nInstall a recent version of PyTorch\nOptionally install some dependencies if changes make use of them\nBe sure to uninstall the pytorch_geometric package\nClone the repo\nRun an editable install command for the repo\nRun pytest: local testing is kind of essential for a developer install.\n\nDevelop away - (as long as it’s a supported architecture).\n\n\nM1, Possibly All Apple Silicon\nThis M1 hardware problem for developers has been noted.\nFor developers, if feature development on pytorch_geometric makes use of the listed package dependencies, several M1/AS issues have been raised.\n\npyg-lib\npytorch scatter\npytorch sparse\npytorch cluster\n\n(It’s likely that the package torch-spline-conv, another package dependency, is also an issue for M1/AS users - though no issues mentioning this are given.)\nThese dependencies are being subsumed into other packages (for example torch.scatter); at some point they won’t be a problem.\n\nWhy is Apple Silicon a Problem?\nTo develop locally, I need a an editable install version of pytorch geometric. This editable install needs additional dependencies (for fuller developer functionality such as testing, type checking, compiling, linting and documentation) some of which depend on C++ extensions which are not compiled for the M1/AS architecture. The project founder (@rusty1s) has noted that M1 was not supported from the onset - when it wasn’t available on github actions, and there are no plans to support it now. Later Apple Silicon is supported, but developer build are variable.\n\n\nThe Solution\npyg-team suggested earlier that M1 users wanting the fuller editable version of the package can use the cmake & ninja build systems to create libraries and dependencies that target M1 - this will give a working modifiable install of pytorch geometric. Some OS and compiler flags need to be set.\nLet’s see if I can do this and get a development environment setup.\nWhat I’ll do is as follows:\n\ncheck the OS & Hardware\nmake sure to uninstall all versions of pytorch geometric for all locations.\ncreate and activate a clean Python virtual environment (seems that conda is the best way to go)\ninstall a specific version of PyTorch using conda, as recommended by Apple\nbuild dependencies for clang and macos on my M1 Pro\nbuild the editable install\n\n\nOS and Hardware\nLet’s start by listing my hardware:\n\n%%bash\necho \"Operating System: $(uname -s)\"\necho \"Hardware: $(uname -m)\"\necho \"macOS Version: $(sw_vers -productVersion)\"\necho \"Chipset: $(sysctl -n machdep.cpu.brand_string)\"\n\nOperating System: Darwin\nHardware: arm64\nmacOS Version: 14.4\nChipset: Apple M1 Pro\n\n\n\n\nEditable Developer Install\nI’ve put this gist together from responses in github issues raised relating to AS. It installs, but doesn’t pass all tests - it seems because the tests are composed from objects that don’t work well with Apple Silicon, rather than anything fundamentally broken in AS.\n\n%%bash\n# %load https://gist.githubusercontent.com/project-delphi/b3b5cc91386997ff882f0a3f04a4b89a/raw/6146ec6cf950c2eeb184e0da3e59ff6fdd69550a/pytorch_geometric_apple_silicon_developer_install.sh\n# set variables here\nDIR=\"$HOME/Code/throwaway/pytorch-geometric-developer-install\"\nPYTHON_VERSION=3.11\nRECENT_TORCH_VERSION=2.2.0\nGITHUB_USERNAME=\"project-delphi\"\nMIN_MACOSX_DEPLOYMENT_TARGET=$(sw_vers -productVersion)\n\n# install miniconda for apple silicon, if not already installed\nif [ ! -d \"$HOME/anaconda3\" ] && [ ! -d \"$HOME/miniconda3\" ]\nthen\n    echo \"installing conda...\"\n    curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n    sh Miniconda3-latest-MacOSX-arm64.sh -b -u  &gt; /dev/null 2&gt;&1\nfi\n\nmkdir -p \"$DIR\"\ncd \"$DIR\"\nconda create --yes  -p $DIR/.venv python=$PYTHON_VERSION  &gt; /dev/null 2&gt;&1\neval \"$(conda shell.bash hook)\"\nconda activate $DIR/.venv\n\npip install -q --upgrade pip\n\n##### TORCH BUILD AND INSTALL ON M1, to use GPUs #####\npip install -q numpy # to remove user warning with torch install\npip install -q mpmath==1.3.0 # bugfix\nxcode-select --install  &gt; /dev/null 2&gt;&1 # if xcode not installed\n\n###### install pytorch ######\npip install -q --pre torch==$RECENT_TORCH_VERSION torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n\n# install dev build dependencies\npip install -q cmake\npip install -q ninja wheel\npip install -q git+https://github.com/pyg-team/pyg-lib.git\nMACOSX_DEPLOYMENT_TARGET=$MIN_MACOSX_DEPLOYMENT_TARGET CC=clang CXX=clang++ python -m pip -q --no-cache-dir  install  torch-scatter\nMACOSX_DEPLOYMENT_TARGET=$MIN_MACOSX_DEPLOYMENT_TARGET CC=clang CXX=clang++ python -m pip -q --no-cache-dir  install  torch-sparse\nMACOSX_DEPLOYMENT_TARGET=$MIN_MACOSX_DEPLOYMENT_TARGET CC=clang CXX=clang++ python -m pip -q --no-cache-dir  install  torch-cluster\nMACOSX_DEPLOYMENT_TARGET=$MIN_MACOSX_DEPLOYMENT_TARGET CC=clang CXX=clang++ python -m pip -q --no-cache-dir  install  torch-spline-conv\n\n# clone the forked repository and rebase to original\ngit clone \"https://github.com/$GITHUB_USERNAME/pytorch_geometric.git\"  2&gt;/dev/null\ncd pytorch_geometric\nif ! git remote | grep -q 'upstream'; then\n    git remote add upstream \"https://github.com/pyg-team/pytorch_geometric\"\nfi\ngit fetch upstream  -q\ngit rebase upstream/master\n\n# build dev install\nMACOSX_DEPLOYMENT_TARGET=$MIN_MACOSX_DEPLOYMENT_TARGET CC=clang CXX=clang++ python -m pip install -q --no-cache-dir -e \".[dev,full]\"  #&gt; /dev/null 2&gt;&1\n\n# check\npython --version\npython -c \"import torch; print(f'torch version: {torch.__version__}')\"\npython -c \"import torch_geometric as pyg; print(f'torch geometric version: {pyg.__version__}')\"\n\nSo a kind of success. I can install a full developer version pytorch_geometric on my M1 Pro.\nHowever, we also need to check regarding testing (of which there are several undocumented flavours in the project).\nFor the standard set of tests, today I get this:\n\n%%bash\n# set variables here\nDIR=\"$HOME/Code/throwaway/pytorch-geometric-developer-install\"\ncd \"$DIR\"\neval \"$(conda shell.bash hook)\"\nconda activate $DIR/.venv\n# install missing packages needed for testing\npip install -q matplotlib-inline ipython\npytest -q --tb=no | tail -n 1\n\nFatal Python error: Segmentation fault\n\nCurrent thread 0x00000001f82fbac0 (most recent call first):\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/torch/_ops.py\", line 755 in __call__\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pyg_lib/partition/__init__.py\", line 35 in metis\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/pytorch_geometric/torch_geometric/testing/decorators.py\", line 224 in withMETIS\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/pytorch_geometric/test/distributed/test_dist_link_neighbor_loader.py\", line 140 in &lt;module&gt;\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/assertion/rewrite.py\", line 178 in exec_module\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 690 in _load_unlocked\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 1147 in _find_and_load_unlocked\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 1176 in _find_and_load\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 1204 in _gcd_import\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/importlib/__init__.py\", line 126 in import_module\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/pathlib.py\", line 584 in import_path\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/python.py\", line 520 in importtestmodule\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/python.py\", line 573 in _getobj\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/python.py\", line 315 in obj\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/python.py\", line 589 in _register_setup_module_fixture\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/python.py\", line 576 in collect\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/runner.py\", line 388 in collect\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/runner.py\", line 340 in from_call\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/runner.py\", line 390 in pytest_make_collect_report\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_callers.py\", line 102 in _multicall\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_manager.py\", line 119 in _hookexec\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_hooks.py\", line 501 in __call__\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/runner.py\", line 565 in collect_one_node\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 839 in _collect_one_node\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 976 in genitems\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 981 in genitems\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 981 in genitems\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 981 in genitems\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 981 in genitems\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 813 in perform_collect\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 349 in pytest_collection\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_callers.py\", line 102 in _multicall\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_manager.py\", line 119 in _hookexec\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_hooks.py\", line 501 in __call__\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 338 in _main\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 285 in wrap_session\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/main.py\", line 332 in pytest_cmdline_main\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_callers.py\", line 102 in _multicall\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_manager.py\", line 119 in _hookexec\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/pluggy/_hooks.py\", line 501 in __call__\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/config/__init__.py\", line 174 in main\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.11/site-packages/_pytest/config/__init__.py\", line 197 in console_main\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/bin/pytest\", line 8 in &lt;module&gt;\n\nExtension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._flinalg, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, scipy.cluster._vq, scipy.cluster._hierarchy, scipy.cluster._optimal_leaf_ordering, yaml._yaml, numba.core.typeconv._typeconv, numba._helperlib, numba._dynfunc, numba._dispatcher, numba.core.runtime._nrt_python, numba.np.ufunc._internal, numba.experimental.jitclass._box, psutil._psutil_osx, psutil._psutil_posix, markupsafe._speedups, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, matplotlib._c_internal_utils, PIL._imaging, matplotlib._path, kiwisolver._cext, matplotlib._image, rdkit.rdBase (total: 116)\n\n\nA week ago I noticed that the tests were failing, and then a few days ago they were passing. Today they are not passing, due to new tests. This is caused by commit updates which don’t test on Apple Silicon.\nThis inconsistency is a problem for contributing to the project. As mentioned, @rusty1s has committed changes to the tests which accommodate Apple Silicon using his own device - but this is not a sustainable approach.\nThere’s not a good solution to variable pytest runs, since feature updates (commits) to the main line branch – which don’t build for Apple Silicon – are likely to break some test at least some of the time.\nIn some issues, @rusty1s mentioned a community effort to help test Apple Silicon builds, specifically the M1 architecture. I’d like to contribute, it’s just figuring out how to do so for the long term.\nElse, the path of least resistance is to move to the cloud - I’ve recently received cloud credit from major providers."
  },
  {
    "objectID": "posts/developing-pytorch-geometric-on-m1/index.html#takeaways",
    "href": "posts/developing-pytorch-geometric-on-m1/index.html#takeaways",
    "title": "Developing Pytorch Geometric on M1 Apple Silicon",
    "section": "Takeaways",
    "text": "Takeaways\nChecking with @rusty1s, this is a problem with the type of tensors used in tests. I’m going to work on replacing COO tensors with CSR tensors for testing. A conversation with chatGPT helped to explain the differences between the two types of tensors.\nAlso, mkl is part of Intel’s oneAPI project. It seems that supporting apple silicon is out of scope at this time.\nSo, developing pyg on recent Apple devices may just not be possible, but running on Apple Silicon is. Well if that is true, there’s always the cloud for Apple based developers who want to contribute.\nUPDATE: @rusty1s made a pull request that updates testing, accounting for MKL. pytorch-geometric as of now works great.\nOne just needs to install a few undeclared test dependencies and specific packages versions to fix downstream PyTorch dependencies breaking tests.\n\n%%bash\n# set variables here\nDIR=\"$HOME/Code/throwaway/pytorch-geometric-developer-install\"\nRECENT_TORCH_VERSION=2.2.0\nGITHUB_USERNAME=\"project-delphi\"\nMIN_MACOSX_DEPLOYMENT_TARGET=$(sw_vers -productVersion)\n\nmkdir -p \"$DIR\"\ncd \"$DIR\"\nconda create --yes  -p $DIR/.venv python=3.12  &gt; /dev/null 2&gt;&1\neval \"$(conda shell.bash hook)\"\nconda activate $DIR/.venv\nwhich python\npython --version\npip install -q --upgrade pip\n\n##### TORCH BUILD AND INSTALL ON M1, to use GPUs #####\npip install -q numpy # to remove user warning with torch install\npip install -q mpmath==1.3.0 # bugfix\nxcode-select --install  &gt; /dev/null 2&gt;&1 # if xcode not installed\n# install miniconda for apple silicon, if not already installed\nif [ ! -d \"$HOME/anaconda3\" ] && [ ! -d \"$HOME/miniconda3\" ]\nthen\n    echo \"installing conda...\"\n    curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n    sh Miniconda3-latest-MacOSX-arm64.sh -b -u  &gt; /dev/null 2&gt;&1\nfi\n###### install pytorch ######\npip install -q --pre torch==$RECENT_TORCH_VERSION torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n\n\n# install dev build dependencies\npip install -q cmake\npip install -q ninja wheel\npip install -q git+https://github.com/pyg-team/pyg-lib.git\nMACOSX_DEPLOYMENT_TARGET=$MIN_MACOSX_DEPLOYMENT_TARGET CC=clang CXX=clang++ python -m pip -q --no-cache-dir  install  torch-scatter\nMACOSX_DEPLOYMENT_TARGET=$MIN_MACOSX_DEPLOYMENT_TARGET CC=clang CXX=clang++ python -m pip -q --no-cache-dir  install  torch-sparse\nMACOSX_DEPLOYMENT_TARGET=$MIN_MACOSX_DEPLOYMENT_TARGET CC=clang CXX=clang++ python -m pip -q --no-cache-dir  install  torch-cluster\nMACOSX_DEPLOYMENT_TARGET=$MIN_MACOSX_DEPLOYMENT_TARGET CC=clang CXX=clang++ python -m pip -q --no-cache-dir  install  torch-spline-conv\n\n# clone the forked repository and rebase to original\ngit clone \"https://github.com/$GITHUB_USERNAME/pytorch_geometric.git\"  2&gt;/dev/null\ncd pytorch_geometric\nif ! git remote | grep -q 'upstream'; then\n    git remote add upstream \"https://github.com/pyg-team/pytorch_geometric\"\nfi\ngit fetch upstream  -q\ngit rebase upstream/master\n\n# build dev install\nMACOSX_DEPLOYMENT_TARGET=$MIN_MACOSX_DEPLOYMENT_TARGET CC=clang CXX=clang++ python -m pip install -q --no-cache-dir -e \".[dev,full]\"  #&gt; /dev/null 2&gt;&1\n\n# check\npython --version\npython -c \"import torch; print(f'torch version: {torch.__version__}')\"\npython -c \"import torch_geometric as pyg; print(f'torch geometric version: {pyg.__version__}')\"\n# install missing packages\npip install -q matplotlib-inline ipython\npytest -q --tb=no | tail -n 1\n\nChannels:\n - defaults\nPlatform: osx-arm64\nCollecting package metadata (repodata.json): ...working... done\nSolving environment: ...working... done\n\n## Package Plan ##\n\n  environment location: /Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv\n\n  added / updated specs:\n    - python=3.12\n\n\nThe following NEW packages will be INSTALLED:\n\n  bzip2              pkgs/main/osx-arm64::bzip2-1.0.8-h80987f9_5 \n  ca-certificates    pkgs/main/osx-arm64::ca-certificates-2023.12.12-hca03da5_0 \n  expat              pkgs/main/osx-arm64::expat-2.5.0-h313beb8_0 \n  libcxx             pkgs/main/osx-arm64::libcxx-14.0.6-h848a8c0_0 \n  libffi             pkgs/main/osx-arm64::libffi-3.4.4-hca03da5_0 \n  ncurses            pkgs/main/osx-arm64::ncurses-6.4-h313beb8_0 \n  openssl            pkgs/main/osx-arm64::openssl-3.0.13-h1a28f6b_0 \n  pip                pkgs/main/osx-arm64::pip-23.3.1-py312hca03da5_0 \n  python             pkgs/main/osx-arm64::python-3.12.2-h99e199e_0 \n  readline           pkgs/main/osx-arm64::readline-8.2-h1a28f6b_0 \n  setuptools         pkgs/main/osx-arm64::setuptools-68.2.2-py312hca03da5_0 \n  sqlite             pkgs/main/osx-arm64::sqlite-3.41.2-h80987f9_0 \n  tk                 pkgs/main/osx-arm64::tk-8.6.12-hb8d0fd4_0 \n  tzdata             pkgs/main/noarch::tzdata-2024a-h04d1e81_0 \n  wheel              pkgs/main/osx-arm64::wheel-0.41.2-py312hca03da5_0 \n  xz                 pkgs/main/osx-arm64::xz-5.4.6-h80987f9_0 \n  zlib               pkgs/main/osx-arm64::zlib-1.2.13-h5a0b063_0 \n\n\n\nDownloading and Extracting Packages: ...working... done\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n#\n# To activate this environment, use\n#\n#     $ conda activate /Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\n/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/bin/python\nPython 3.12.2\nPython 3.12.2\ntorch version: 2.2.0\ntorch geometric version: 2.6.0\ncollecting ... \n\n\nSuccessfully rebased and updated refs/heads/master.\nFatal Python error: Segmentation fault\n\nCurrent thread 0x00000001f82fbac0 (most recent call first):\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/torch/_ops.py\", line 755 in __call__\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/pyg_lib/partition/__init__.py\", line 35 in metis\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/pytorch_geometric/torch_geometric/testing/decorators.py\", line 224 in withMETIS\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/pytorch_geometric/test/distributed/test_dist_link_neighbor_loader.py\", line 140 in &lt;module&gt;\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py\", line 178 in exec_module\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 935 in _load_unlocked\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 1331 in _find_and_load_unlocked\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 1360 in _find_and_load\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 1387 in _gcd_import\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/importlib/__init__.py\", line 90 in import_module\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/pathlib.py\", line 584 in import_path\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/python.py\", line 520 in importtestmodule\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/python.py\", line 573 in _getobj\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/python.py\", line 315 in obj\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/python.py\", line 589 in _register_setup_module_fixture\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/python.py\", line 576 in collect\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/runner.py\", line 388 in collect\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/runner.py\", line 340 in from_call\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/runner.py\", line 390 in pytest_make_collect_report\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/pluggy/_callers.py\", line 102 in _multicall\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/pluggy/_manager.py\", line 119 in _hookexec\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/pluggy/_hooks.py\", line 501 in __call__\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/runner.py\", line 565 in collect_one_node\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/main.py\", line 839 in _collect_one_node\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/main.py\", line 976 in genitems\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/main.py\", line 981 in genitems\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/main.py\", line 981 in genitems\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/main.py\", line 981 in genitems\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/main.py\", line 813 in perform_collect\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/main.py\", line 349 in pytest_collection\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/pluggy/_callers.py\", line 102 in _multicall\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/pluggy/_manager.py\", line 119 in _hookexec\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/pluggy/_hooks.py\", line 501 in __call__\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/main.py\", line 338 in _main\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/main.py\", line 285 in wrap_session\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/main.py\", line 332 in pytest_cmdline_main\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/pluggy/_callers.py\", line 102 in _multicall\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/pluggy/_manager.py\", line 119 in _hookexec\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/pluggy/_hooks.py\", line 501 in __call__\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/config/__init__.py\", line 174 in main\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/lib/python3.12/site-packages/_pytest/config/__init__.py\", line 197 in console_main\n  File \"/Users/ravikalia/Code/throwaway/pytorch-geometric-developer-install/.venv/bin/pytest\", line 8 in &lt;module&gt;\n\nExtension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._flinalg, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, scipy.cluster._vq, scipy.cluster._hierarchy, scipy.cluster._optimal_leaf_ordering, yaml._yaml, numba.core.typeconv._typeconv, numba._helperlib, numba._dynfunc, numba._dispatcher, numba.core.runtime._nrt_python, numba.np.ufunc._internal, numba.experimental.jitclass._box, psutil._psutil_osx, psutil._psutil_posix, markupsafe._speedups, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, matplotlib._c_internal_utils, PIL._imaging, matplotlib._path, kiwisolver._cext, matplotlib._image, rdkit.rdBase (total: 116)\n\n\nSuccess! As of now it works.\nHowever, as I looked over the github actions workflows, I noticed that full testing and gpu testing are not set up for apple silicon and also have to install undeclared dependencies. These minor issues could be fun useful to work on."
  },
  {
    "objectID": "posts/developing-pytorch-geometric-on-m1/index.html#references",
    "href": "posts/developing-pytorch-geometric-on-m1/index.html#references",
    "title": "Developing Pytorch Geometric on M1 Apple Silicon",
    "section": "References",
    "text": "References\n\npytorch on Apple Metal\npyg-lib M1 issues\npytorch-scatter M1 issue\npytorch-spars M1 issues\npytorch-cluster M1 issues"
  },
  {
    "objectID": "posts/developing-pytorch-geometric-on-m1/index.html#more-on-testing",
    "href": "posts/developing-pytorch-geometric-on-m1/index.html#more-on-testing",
    "title": "Developing Pytorch Geometric on M1 Apple Silicon",
    "section": "More On Testing",
    "text": "More On Testing\nThese test related issues are not unique to the default testing for the package. The package has several kinds of tests, including: full, gpu, previous version, and nightly.\nI looked over the github actions workflows, I noticed that full testing and gpu testing are not set up for apple silicon and also have to install undeclared dependencies, such as graphviz and bugfix pinned versions of packages (e.g. mpmath==1.3.0). These minor issues could be useful to work on - even with current default testing this seems to be the case.\nI’m thinking of a Makefile to compose the different installation and testing steps into higher level portable grammars. This would be useful for the community, and also for me to use in the future.\nUpdate, @rusty1s suggested more specific testing for mps (and by implication AS), starting with test decorators. That and test documentation could be a good place to start."
  },
  {
    "objectID": "posts/sparse_tensors/index.html",
    "href": "posts/sparse_tensors/index.html",
    "title": "Sparsity with PyTorch Tensors",
    "section": "",
    "text": "Image of Building\n\n\nPhoto by engin akyurt on Unsplash\n\n\nThe canonical format for representing tensors in PyTorch is the dense tensor, with associated contiguous memory proportional to the size of the tensor. However, in many applications, the data is sparse, that is most of the elements are zero. In this notebook, we will see how to work efficiently with sparse tensors in PyTorch.\n\n\nA dataset is considered sparse when most of its elements are zero, for some features of the dataset. The exact threshold for considering a dataset as sparse is not strictly defined and can depend on the context and the specific application. A common rule of thumb is that a dataset is sparse if over 50% of its elements are zero.\nIn many real-world applications, datasets can be extremely sparse, with over 90% or even 99% of elements being zero. For example, in a user-item interaction matrix in a recommendation system, each user typically interacts with only a small fraction of all possible items, so the vast majority of the matrix elements are zero.\nIn PyTorch, datasets are stored in tensors. A tensor is a multi-dimensional array, similar to a NumPy array. PyTorch provides some support for sparse tensors.\n\n\n\nIn a dense tensor, all elements are stored in memory, even if most of them are zero. In a sparse tensor, only the non-zero elements are stored, along with their indices. This can lead to significant memory savings and/or faster compute times, especially for very sparse datasets.\nFor several tasks which are computing on the tensor, sparse tensors can be more efficient than dense tensors. For example, consider matrix multiplication. If the matrices are sparse, then sparse tensors can be orders of magnitude faster than dense tensors - as we will demonstrate at the end.\n\n\n\nIn PyTorch there are several ways to create sparse tensors as containers for sparse data. We will see how to create a sparse tensor from a dense tensor, and how to create a sparse tensor from different formats: COO, CSR, and CSC.\nFirst we’ll need example data to work with - let’s use a graph dataset as an example to understand sparse tensors.\n\n\n\n\nGraph data is a commonly sparse. A graph is a collection of nodes (or vertices) and edges. The nodes represent entities, and the edges represent relationships between the entities. More often than not, graphs are sparse, i.e. most of the nodes are not connected to each other. It is common to see datasets where less than 1% of the possible edges are present.\nThere are many ways to represent a Graph. One common is an adjacency matrix, A. This is particularly bad for sparse graphs, as most of the elements in the matrix are zero.\nThe adjacency matrix is a square matrix A, of size N x N, where N is the number of nodes in the graph. The element A[i, j] is 1 if there is an edge between node i and node j, and 0 otherwise. Since most nodes are not connected to each other, the adjacency matrix is sparse.\nAnother more memory-efficient way to represent a graph is using a tensor of edges, E. Each edge is represented as a 2 long column (r, s) in E, where r and s are the indices of the nodes connected by the edge. This representation is more memory-efficient than the adjacency matrix, as it only stores the non-zero elements, it’s size is 2 x number_edges.\nIn this notebook, we will see how to create a sparse tensor from an adjacency matrix, and how to create a sparse tensor from a list of edges. We will also see how to perform basic operations on sparse tensors, such as matrix multiplication and element-wise operations.\nLet’s consider an example from the ModelNet10 dataset.\n\n\nModelNet10 is a subset of the ModelNet dataset, which is a large-scale 3D CAD model dataset. ModelNet10 specifically consists of 10 categories, with a total of 4899 3D CAD models. The categories include: bed, chair, monitor, desk, dresser, sofa, table, toilet, night stand, and bathtub.\nEach 3D model in the dataset is represented as a graph, where each node represents a point in the 3D object, and each edge represents a relationship between the points. The adjacency matrix of the graph is sparse, as most of the points are not connected to each other.\nThis dataset is commonly used for benchmarking in tasks such as 3D object recognition, shape classification, and other machine learning tasks involving 3D data.\nLet’s look at a one: a Monitor from the training dataset. We’ll need to install some libraries to visualize the 3D object.\n\n\nCode\n!pip -q install numpy matplotlib networkx\n\n\nThe data is in off format - that is nodes and edges making faces. The particular file can be found here. The corners of the monitor are the nodes and the edges are the connections between the corners. Here’s a plot using matplotlib and another with trimesh.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\n\n\ndef read_off(file):\n    if 'OFF' != file.readline().strip():\n        raise('Not a valid OFF header')\n    n_verts, n_faces, _ = tuple(map(int, file.readline().strip().split(' ')))\n    verts = [[float(s) for s in file.readline().strip().split(' ')] for _ in range(n_verts)]\n    faces = [[int(s) for s in file.readline().strip().split(' ')[1:]] for _ in range(n_faces)]\n    return verts, faces\n\nwith open('./data/monitor_0001.off', 'r') as f:\n    verts, faces = read_off(f)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Create a Poly3DCollection object\npolys = [np.array(verts)[face] for face in faces]\ncollection = Poly3DCollection(polys, linewidths=1, alpha=1)\ncollection.set_facecolor((0,0,1,0.5))  # Set the color of the object to blue\ncollection.set_edgecolor((0,0,1,0.5))  # Set the edge color to a darker blue\nax.add_collection3d(collection)\n\n# Add lighting\nax.add_collection3d(Poly3DCollection(polys, facecolors='r', linewidths=1, edgecolors='r', alpha=.20))\n\n# Remove the axes for a cleaner look\nax.axis('off')\n\n# Auto scale to the mesh size\nscale = np.array(verts).flatten()\nax.auto_scale_xyz(scale, scale, scale)\n\n# Add a title to the plot\nax.set_title('3D Model of Monitor')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\npip -q install trimesh\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nCode\nimport trimesh\n\n# Load the mesh from the OFF file\nmesh = trimesh.load_mesh('./data/monitor_0001.off')\n\n# Plot the mesh\nmesh.show()\n\n\n\n\n\nTo see this data as a graph, we will feed in vertices and faces into the networkx package.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\n\ndef off_to_graph(verts, faces):\n    # Create a new graph\n    G = nx.Graph()\n    # Add nodes to the graph\n    for i in range(len(verts)):\n        G.add_node(i)\n    # Add edges to the graph\n    for face in faces:\n        for i in range(len(face)):\n            # Add an edge between each pair of vertices in the face\n            G.add_edge(face[i], face[(i+1)%len(face)])\n    return G\n\nwith open('./data/monitor_0001.off', 'r') as f:\n    verts, faces = read_off(f)\n\nG = off_to_graph(verts, faces)\n\n\n\n\nCode\nprint(G)\n\n\nGraph with 798 nodes and 1476 edges\n\n\n\n\nCode\n# Draw the graph\nnx.draw(G, edge_color=\"black\", width=1, node_size=5, with_labels=False)\nplt.show()\n\n\n\n\n\n\n\n\n\nSo, that’s the graph. It’s difficult to make out the shape of the monitor from the plot above, as we’re not representing the features of the nodes (3d co-ordinates).\nWhat we can do is list out the edges and vertices of the graph. First in edge list format with the corresponding nodes.\n\n\nCode\nfor index, edge in enumerate(G.edges(), start=0):\n    print(edge, end=\", \")\n    if index == 9:\n        break\n\n\n(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (1, 2), (1, 3), (3, 4), (4, 5), (4, 6), \n\n\n\n\nCode\nfor index, edge in enumerate(G.nodes(), start=0):\n    print(edge, end=\", \")\n    if index == 9:\n        break\n\n\n0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n\n\nAnd then as a an adjacency matrix.\n\n\nCode\nA = nx.to_numpy_array(G)\n\n\n\n\nCode\nA.shape\n\n\n(798, 798)\n\n\n\n\nCode\nprint(A)\n\n\n[[0. 1. 1. ... 0. 0. 0.]\n [1. 0. 1. ... 0. 0. 0.]\n [1. 1. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 1. 1.]\n [0. 0. 0. ... 1. 0. 0.]\n [0. 0. 0. ... 1. 0. 0.]]\n\n\nLet’s verify that the number of edges match the adjacency matrix and edge list view.\n\n\nCode\nA.sum()/2 == len(G.edges()) # The number of edges.\n\n\nTrue\n\n\n\n\n\n\nThe adjacency is sparse. We can quantify this sparsity - the ratio of the number of non-zero elements to the total number of elements in the tensor.\n\n\nCode\n# the ratio of the number of non-zero elements to the total number of elements in the tensor A.\nnp.count_nonzero(A) / (A.shape[0] * A.shape[1])\n\n\n0.004635649273559839\n\n\nStoring data in this format is inefficient. We can convert it to a sparse tensor. We’ll create functions for popular formats, as well as look at the native PyTorch implementations.\n\n\n\nSome well known sparse tensor representation formats are:\n\nCoordinate (COO)\nCompressed Sparse Row (CSR)\nCompressed Sparse Column (CSC)\nBlock Compressed Sparse Row (BSR)\nDictionary of Keys (DOK)\nList of Lists (LIL)\n\nWe’ll look at COO, CSR, and CSC formats.\nThe monitor graph is too large and sparse for illustration purposes. We’ll create a smaller graph to demonstrate the conversion back and forth to sparse tensors.\n\n\nCode\n# Create a new graph\nG = nx.Graph()\n\n# Add some edges to the graph\nG.add_edge(0, 1)\nG.add_edge(0, 2)\nG.add_edge(3, 4)\n\n# Draw the graph\nnx.draw(G, with_labels=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\npip -q install torch\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nCode\nimport torch\n\nA = nx.to_numpy_array(G)\nprint(torch.from_numpy(A))\nA_torch = torch.from_numpy(A).bool()\nA_torch\n\n\ntensor([[0., 1., 1., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1.],\n        [0., 0., 0., 1., 0.]], dtype=torch.float64)\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\n\n\nCode\nA.sum()/2\n\n\n3.0\n\n\nLet’s express as a pandas dataframe with source and target nodes for edges.\n\n\nCode\npip -q install pandas\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n# Get the number of nodes\nnum_nodes = A_torch.shape[0]\n\n# Create a list of all pairs of nodes\nsource_node, destination_node = np.meshgrid(np.arange(num_nodes), np.arange(num_nodes))\n\n# Flatten the arrays\nsource_node = source_node.flatten()\ndestination_node = destination_node.flatten()\n\n# Get the edge values\nhas_edge = A_torch[source_node, destination_node] &gt; 0\n\n# Create the DataFrame\ndf = pd.DataFrame({\n    'source_node': source_node,\n    'destination_node': destination_node,\n    'has_edge': has_edge\n})\n\ndf\n\n\n\n\n\n\n\n\n\nsource_node\ndestination_node\nhas_edge\n\n\n\n\n0\n0\n0\nFalse\n\n\n1\n1\n0\nTrue\n\n\n2\n2\n0\nTrue\n\n\n3\n3\n0\nFalse\n\n\n4\n4\n0\nFalse\n\n\n5\n0\n1\nTrue\n\n\n6\n1\n1\nFalse\n\n\n7\n2\n1\nFalse\n\n\n8\n3\n1\nFalse\n\n\n9\n4\n1\nFalse\n\n\n10\n0\n2\nTrue\n\n\n11\n1\n2\nFalse\n\n\n12\n2\n2\nFalse\n\n\n13\n3\n2\nFalse\n\n\n14\n4\n2\nFalse\n\n\n15\n0\n3\nFalse\n\n\n16\n1\n3\nFalse\n\n\n17\n2\n3\nFalse\n\n\n18\n3\n3\nFalse\n\n\n19\n4\n3\nTrue\n\n\n20\n0\n4\nFalse\n\n\n21\n1\n4\nFalse\n\n\n22\n2\n4\nFalse\n\n\n23\n3\n4\nTrue\n\n\n24\n4\n4\nFalse\n\n\n\n\n\n\n\n\n\nThe COO format is simple and flexible for sparse matrices. It consists of three arrays: row, col, and data. The row array contains the row indices of the non-zero elements, the col array contains the column indices of the non-zero elements, and the data array contains the values of the non-zero elements.\nLuckily, PyTorch has built in methods for converting between dense and sparse tensors. We can convert the adjacency matrix to a COO sparse tensor using the .to_sparse() method.\n\n\nCode\nA_coo_pytorch = A_torch.to_sparse()\nA_coo_pytorch\n\n\ntensor(indices=tensor([[0, 0, 1, 2, 3, 4],\n                       [1, 2, 0, 0, 4, 3]]),\n       values=tensor([True, True, True, True, True, True]),\n       size=(5, 5), nnz=6, layout=torch.sparse_coo)\n\n\nAnd convert back\n\n\nCode\nA_coo_pytorch.to_dense()\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\nLet’s look at the sparse tensor as a dataframe of source to destination nodes, but this time only for those pairs where there is an edge.\n\n\nCode\n# Get the source and destination nodes and the edge values\nsource_node = A_coo_pytorch.indices()[0].numpy()\ndestination_node = A_coo_pytorch.indices()[1].numpy()\nhas_edge = A_coo_pytorch.values().numpy()\n\n# Create the DataFrame\ndf = pd.DataFrame({\n    'source_node': source_node,\n    'destination_node': destination_node,\n    'has_edge': has_edge\n})\ndf\n\n\n\n\n\n\n\n\n\nsource_node\ndestination_node\nhas_edge\n\n\n\n\n0\n0\n1\nTrue\n\n\n1\n0\n2\nTrue\n\n\n2\n1\n0\nTrue\n\n\n3\n2\n0\nTrue\n\n\n4\n3\n4\nTrue\n\n\n5\n4\n3\nTrue\n\n\n\n\n\n\n\nWriting our own homebrew function to convert to COO format, is straightforward.\n\n\nCode\ndef to_sparse(tensor):\n    # Get the indices of the non-zero elements\n    indices = torch.nonzero(tensor).t()\n    # Get the values of the non-zero elements\n    values = tensor[indices[0], indices[1]]  # assuming 2D tensor\n    # Get the size of the original tensor\n    size = tensor.size()\n    # Create a sparse tensor\n    sparse_tensor = torch.sparse_coo_tensor(indices, values, size)\n    return sparse_tensor\n\n\n\n\nCode\nto_sparse(A_torch)\n\n\ntensor(indices=tensor([[0, 0, 1, 2, 3, 4],\n                       [1, 2, 0, 0, 4, 3]]),\n       values=tensor([True, True, True, True, True, True]),\n       size=(5, 5), nnz=6, layout=torch.sparse_coo)\n\n\n\n\nCode\ndef to_dense(sparse_tensor):\n    # Get the size of the original tensor\n    size = sparse_tensor.size()\n    # Get the indices and values from the sparse tensor\n    indices = sparse_tensor.coalesce().indices()\n    values = sparse_tensor.coalesce().values()\n    # Create a dense tensor of the same size and data type as values, initialized with zeros\n    dense_tensor = torch.zeros(size, dtype=values.dtype)\n    # Convert indices to a tuple of tensors\n    indices_tuple = tuple(indices[i] for i in range(indices.shape[0]))\n    # Use index_put_ to put the values in the dense tensor at the right indices\n    dense_tensor.index_put_(indices_tuple, values, accumulate=False)\n    return dense_tensor\n\n\n\n\nCode\nto_dense(A_coo_pytorch)\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\n\n\n\nCSR is an even more efficient compact format for sparse data. Details can be found here.\nValues and column indices are stored in the same way as COO format. The row indices are stored in a separate array, which can seem strange at first glance.\nBroadly it relies on slicing the values into per row chunks. The row chunk lengths are calculated as the difference between the row indices of the non-zero elements. The row chunk lengths are stored in a separate array, row_ptr.\nSo, if your original matrix has m rows, n columns, and nnz non-zero values, then:\n\nThe shape of values is [nnz].\nThe shape of column_indices is [nnz].\nThe shape of row_pointers is [m+1].\n\nIn index notation, the row pointer array is defined as:\nrow_ptr[i] = row_ptr[i-1] + number of non-zero elements in row i-1\nSo, the row pointer array stores the cumulative sum of the number of non-zero elements in each row.\nthe row and column number can be determined by the index of the non-zero element in the values array. That is the i-th non-zero element (in the values array) is at row row_ptr[i] and column col[i]. That is:\nvalues[i] = A[row_ptr[i], col[i]]\n\n\nCode\nA_torch.to_sparse_csr()\n\n\ntensor(crow_indices=tensor([0, 2, 3, 4, 5, 6]),\n       col_indices=tensor([1, 2, 0, 0, 4, 3]),\n       values=tensor([True, True, True, True, True, True]), size=(5, 5), nnz=6,\n       layout=torch.sparse_csr)\n\n\nWhich can be recovered by chaining with the .to_dense() method.\n\n\nCode\nA_torch.to_sparse_csr().to_dense()\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\nAnd here we write our homebrew version of to_csr() and from_csr() functions:\n\n\nCode\ndef to_csr(tensor):\n    # Get the indices of the non-zero elements\n    indices = torch.nonzero(tensor, as_tuple=True)\n    # Get the values of the non-zero elements\n    values = tensor[indices]\n    # Get the column indices of the non-zero elements\n    column_indices = indices[1]\n    # Get the row pointers\n    row_pointers = torch.zeros(tensor.size(0) + 1, dtype=torch.long)\n    row_pointers[1:] = torch.bincount(indices[0])\n    row_pointers = torch.cumsum(row_pointers, dim=0)\n\n    return values, column_indices, row_pointers\n\n\n\n\nCode\nto_csr(A_torch)\n\n\n(tensor([True, True, True, True, True, True]),\n tensor([1, 2, 0, 0, 4, 3]),\n tensor([0, 2, 3, 4, 5, 6]))\n\n\n\n\nCode\ndef from_csr(values, column_indices, row_pointers):\n    # Get the number of rows and columns\n    num_rows = row_pointers.size(0) - 1\n    num_cols = torch.max(column_indices).item() + 1\n    # Create a dense tensor of the right size, initialized with zeros\n    tensor = torch.zeros((num_rows, num_cols), dtype=values.dtype)\n    # Loop over the rows\n    for i in range(num_rows):\n        # Get the start and end indices for this row in the values and column_indices arrays\n        start = row_pointers[i].item()\n        end = row_pointers[i + 1].item()\n        # Set the values in the dense tensor for this row\n        tensor[i, column_indices[start:end]] = values[start:end]\n\n    return tensor\n\n\n\n\nCode\nfrom_csr(*to_csr(A_torch))\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\n\n\n\nThe CSC format is similar to the CSR format, but with the rows and columns swapped. The values and row indices are stored in the same way as the CSR format. The column indices are stored in a separate array, col_ptr.\nThis format is efficient for certain slicing operations, such as extracting a column from a matrix.\n\n\n\n\nLet’s see how much faster it is to perform matrix multiplication on the sparse tensor compared to the dense tensor. For this we’ll use the ModelNet10 monitor graph seen earlier.\n\n\nCode\nwith open('./data/monitor_0001.off', 'r') as f:\n    verts, faces = read_off(f)\n\nG = off_to_graph(verts, faces)\nA = nx.adjacency_matrix(G)\nA\n\n\n&lt;798x798 sparse array of type '&lt;class 'numpy.int64'&gt;'\n    with 2952 stored elements in Compressed Sparse Row format&gt;\n\n\nA is already a sparse matrix, but in numpy. We’ll convert it to a PyTorch dense tensor first.\n\n\nCode\nA_dense = torch.from_numpy(A.toarray())\nA_dense.shape\n\n\ntorch.Size([798, 798])\n\n\n\n\nCode\ndef tensor_memory_usage_str(tensor=None):\n    if tensor is None:\n        return \"No tensor provided\"\n    bytes = tensor.element_size() * tensor.nelement()\n    kilobytes = bytes / 1024\n    return f\"Memory usage: {bytes} bytes ({kilobytes} KB)\"\n\n\n\n\nCode\ntensor_memory_usage_str(A_dense)\n\n\n'Memory usage: 5094432 bytes (4975.03125 KB)'\n\n\n\n\nCode\ntensor_memory_usage_str(A_dense.to_sparse())\n\n\n'Memory usage: 5094432 bytes (4975.03125 KB)'\n\n\nSomewhat unexpectedly, the sparse and dense tensors have the same memory usage.\n\n\nCode\nA_sparse = A_dense.to_sparse_csr()\n\n\n\n\nCode\ntensor_memory_usage_str(A_sparse)\n\n\n'Memory usage: 5094432 bytes (4975.03125 KB)'\n\n\nOK, so there was no improvement in memory usage for this particular case. Let’s time the matrix multiplication operations.\n\n\nCode\nimport timeit\n\n\ndef multiply_tensors():\n    return A_dense @ A_dense\n\nnum_runs = 10\ntime_taken_dense = timeit.timeit( multiply_tensors, number=num_runs)\n\nprint(f\"Time taken by my_function over {num_runs} runs: {time_taken_dense} seconds\")\n\n\nTime taken by my_function over 10 runs: 2.746476124972105 seconds\n\n\n\n\nCode\nA_coo = A\n\ndef multiply_tensors():\n    return A_coo@A_coo\n\nnum_runs = 10\ntime_taken_sparse = timeit.timeit( multiply_tensors, number=num_runs)\n\nprint(f\"Time taken by my_function over {num_runs} runs: {time_taken_sparse} seconds\")\n\n\nTime taken by my_function over 10 runs: 0.0038774579879827797 seconds\n\n\n\n\nCode\nf\"Speedup: {time_taken_dense / time_taken_sparse}x\"\n\n\n'Speedup: 708.3187318815902x'\n\n\nWith CSR encoding of sparse tensors, it might be even better - however I’m on Apple Silicon, and the csr sparse tensor support is not yet available for this architecture.\n\n\n\nWe just saw how to work with sparse tensors in PyTorch. We created a sparse tensor from an adjacency matrix and a list of edges, and converted it to different sparse formats. We also saw the performance benefits of using sparse tensors for matrix multiplication. There’s a lot more to sparsity, and PyTorch is actively developing its sparse tensor support. Things will break, but expect ramp up in performance gains."
  },
  {
    "objectID": "posts/sparse_tensors/index.html#introduction",
    "href": "posts/sparse_tensors/index.html#introduction",
    "title": "Sparsity with PyTorch Tensors",
    "section": "",
    "text": "The canonical format for representing tensors in PyTorch is the dense tensor, with associated contiguous memory proportional to the size of the tensor. However, in many applications, the data is sparse, that is most of the elements are zero. In this notebook, we will see how to work efficiently with sparse tensors in PyTorch.\n\n\nA dataset is considered sparse when most of its elements are zero, for some features of the dataset. The exact threshold for considering a dataset as sparse is not strictly defined and can depend on the context and the specific application. A common rule of thumb is that a dataset is sparse if over 50% of its elements are zero.\nIn many real-world applications, datasets can be extremely sparse, with over 90% or even 99% of elements being zero. For example, in a user-item interaction matrix in a recommendation system, each user typically interacts with only a small fraction of all possible items, so the vast majority of the matrix elements are zero.\nIn PyTorch, datasets are stored in tensors. A tensor is a multi-dimensional array, similar to a NumPy array. PyTorch provides some support for sparse tensors.\n\n\n\nIn a dense tensor, all elements are stored in memory, even if most of them are zero. In a sparse tensor, only the non-zero elements are stored, along with their indices. This can lead to significant memory savings and/or faster compute times, especially for very sparse datasets.\nFor several tasks which are computing on the tensor, sparse tensors can be more efficient than dense tensors. For example, consider matrix multiplication. If the matrices are sparse, then sparse tensors can be orders of magnitude faster than dense tensors - as we will demonstrate at the end.\n\n\n\nIn PyTorch there are several ways to create sparse tensors as containers for sparse data. We will see how to create a sparse tensor from a dense tensor, and how to create a sparse tensor from different formats: COO, CSR, and CSC.\nFirst we’ll need example data to work with - let’s use a graph dataset as an example to understand sparse tensors."
  },
  {
    "objectID": "posts/sparse_tensors/index.html#example-graph-data",
    "href": "posts/sparse_tensors/index.html#example-graph-data",
    "title": "Sparsity with PyTorch Tensors",
    "section": "",
    "text": "Graph data is a commonly sparse. A graph is a collection of nodes (or vertices) and edges. The nodes represent entities, and the edges represent relationships between the entities. More often than not, graphs are sparse, i.e. most of the nodes are not connected to each other. It is common to see datasets where less than 1% of the possible edges are present.\nThere are many ways to represent a Graph. One common is an adjacency matrix, A. This is particularly bad for sparse graphs, as most of the elements in the matrix are zero.\nThe adjacency matrix is a square matrix A, of size N x N, where N is the number of nodes in the graph. The element A[i, j] is 1 if there is an edge between node i and node j, and 0 otherwise. Since most nodes are not connected to each other, the adjacency matrix is sparse.\nAnother more memory-efficient way to represent a graph is using a tensor of edges, E. Each edge is represented as a 2 long column (r, s) in E, where r and s are the indices of the nodes connected by the edge. This representation is more memory-efficient than the adjacency matrix, as it only stores the non-zero elements, it’s size is 2 x number_edges.\nIn this notebook, we will see how to create a sparse tensor from an adjacency matrix, and how to create a sparse tensor from a list of edges. We will also see how to perform basic operations on sparse tensors, such as matrix multiplication and element-wise operations.\nLet’s consider an example from the ModelNet10 dataset.\n\n\nModelNet10 is a subset of the ModelNet dataset, which is a large-scale 3D CAD model dataset. ModelNet10 specifically consists of 10 categories, with a total of 4899 3D CAD models. The categories include: bed, chair, monitor, desk, dresser, sofa, table, toilet, night stand, and bathtub.\nEach 3D model in the dataset is represented as a graph, where each node represents a point in the 3D object, and each edge represents a relationship between the points. The adjacency matrix of the graph is sparse, as most of the points are not connected to each other.\nThis dataset is commonly used for benchmarking in tasks such as 3D object recognition, shape classification, and other machine learning tasks involving 3D data.\nLet’s look at a one: a Monitor from the training dataset. We’ll need to install some libraries to visualize the 3D object.\n\n\nCode\n!pip -q install numpy matplotlib networkx\n\n\nThe data is in off format - that is nodes and edges making faces. The particular file can be found here. The corners of the monitor are the nodes and the edges are the connections between the corners. Here’s a plot using matplotlib and another with trimesh.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\n\n\ndef read_off(file):\n    if 'OFF' != file.readline().strip():\n        raise('Not a valid OFF header')\n    n_verts, n_faces, _ = tuple(map(int, file.readline().strip().split(' ')))\n    verts = [[float(s) for s in file.readline().strip().split(' ')] for _ in range(n_verts)]\n    faces = [[int(s) for s in file.readline().strip().split(' ')[1:]] for _ in range(n_faces)]\n    return verts, faces\n\nwith open('./data/monitor_0001.off', 'r') as f:\n    verts, faces = read_off(f)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Create a Poly3DCollection object\npolys = [np.array(verts)[face] for face in faces]\ncollection = Poly3DCollection(polys, linewidths=1, alpha=1)\ncollection.set_facecolor((0,0,1,0.5))  # Set the color of the object to blue\ncollection.set_edgecolor((0,0,1,0.5))  # Set the edge color to a darker blue\nax.add_collection3d(collection)\n\n# Add lighting\nax.add_collection3d(Poly3DCollection(polys, facecolors='r', linewidths=1, edgecolors='r', alpha=.20))\n\n# Remove the axes for a cleaner look\nax.axis('off')\n\n# Auto scale to the mesh size\nscale = np.array(verts).flatten()\nax.auto_scale_xyz(scale, scale, scale)\n\n# Add a title to the plot\nax.set_title('3D Model of Monitor')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\npip -q install trimesh\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nCode\nimport trimesh\n\n# Load the mesh from the OFF file\nmesh = trimesh.load_mesh('./data/monitor_0001.off')\n\n# Plot the mesh\nmesh.show()\n\n\n\n\n\nTo see this data as a graph, we will feed in vertices and faces into the networkx package.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\n\ndef off_to_graph(verts, faces):\n    # Create a new graph\n    G = nx.Graph()\n    # Add nodes to the graph\n    for i in range(len(verts)):\n        G.add_node(i)\n    # Add edges to the graph\n    for face in faces:\n        for i in range(len(face)):\n            # Add an edge between each pair of vertices in the face\n            G.add_edge(face[i], face[(i+1)%len(face)])\n    return G\n\nwith open('./data/monitor_0001.off', 'r') as f:\n    verts, faces = read_off(f)\n\nG = off_to_graph(verts, faces)\n\n\n\n\nCode\nprint(G)\n\n\nGraph with 798 nodes and 1476 edges\n\n\n\n\nCode\n# Draw the graph\nnx.draw(G, edge_color=\"black\", width=1, node_size=5, with_labels=False)\nplt.show()\n\n\n\n\n\n\n\n\n\nSo, that’s the graph. It’s difficult to make out the shape of the monitor from the plot above, as we’re not representing the features of the nodes (3d co-ordinates).\nWhat we can do is list out the edges and vertices of the graph. First in edge list format with the corresponding nodes.\n\n\nCode\nfor index, edge in enumerate(G.edges(), start=0):\n    print(edge, end=\", \")\n    if index == 9:\n        break\n\n\n(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (1, 2), (1, 3), (3, 4), (4, 5), (4, 6), \n\n\n\n\nCode\nfor index, edge in enumerate(G.nodes(), start=0):\n    print(edge, end=\", \")\n    if index == 9:\n        break\n\n\n0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n\n\nAnd then as a an adjacency matrix.\n\n\nCode\nA = nx.to_numpy_array(G)\n\n\n\n\nCode\nA.shape\n\n\n(798, 798)\n\n\n\n\nCode\nprint(A)\n\n\n[[0. 1. 1. ... 0. 0. 0.]\n [1. 0. 1. ... 0. 0. 0.]\n [1. 1. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 1. 1.]\n [0. 0. 0. ... 1. 0. 0.]\n [0. 0. 0. ... 1. 0. 0.]]\n\n\nLet’s verify that the number of edges match the adjacency matrix and edge list view.\n\n\nCode\nA.sum()/2 == len(G.edges()) # The number of edges.\n\n\nTrue"
  },
  {
    "objectID": "posts/sparse_tensors/index.html#sparse-adjacency",
    "href": "posts/sparse_tensors/index.html#sparse-adjacency",
    "title": "Sparsity with PyTorch Tensors",
    "section": "",
    "text": "The adjacency is sparse. We can quantify this sparsity - the ratio of the number of non-zero elements to the total number of elements in the tensor.\n\n\nCode\n# the ratio of the number of non-zero elements to the total number of elements in the tensor A.\nnp.count_nonzero(A) / (A.shape[0] * A.shape[1])\n\n\n0.004635649273559839\n\n\nStoring data in this format is inefficient. We can convert it to a sparse tensor. We’ll create functions for popular formats, as well as look at the native PyTorch implementations."
  },
  {
    "objectID": "posts/sparse_tensors/index.html#sparse-formats",
    "href": "posts/sparse_tensors/index.html#sparse-formats",
    "title": "Sparsity with PyTorch Tensors",
    "section": "",
    "text": "Some well known sparse tensor representation formats are:\n\nCoordinate (COO)\nCompressed Sparse Row (CSR)\nCompressed Sparse Column (CSC)\nBlock Compressed Sparse Row (BSR)\nDictionary of Keys (DOK)\nList of Lists (LIL)\n\nWe’ll look at COO, CSR, and CSC formats.\nThe monitor graph is too large and sparse for illustration purposes. We’ll create a smaller graph to demonstrate the conversion back and forth to sparse tensors.\n\n\nCode\n# Create a new graph\nG = nx.Graph()\n\n# Add some edges to the graph\nG.add_edge(0, 1)\nG.add_edge(0, 2)\nG.add_edge(3, 4)\n\n# Draw the graph\nnx.draw(G, with_labels=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\npip -q install torch\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nCode\nimport torch\n\nA = nx.to_numpy_array(G)\nprint(torch.from_numpy(A))\nA_torch = torch.from_numpy(A).bool()\nA_torch\n\n\ntensor([[0., 1., 1., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1.],\n        [0., 0., 0., 1., 0.]], dtype=torch.float64)\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\n\n\nCode\nA.sum()/2\n\n\n3.0\n\n\nLet’s express as a pandas dataframe with source and target nodes for edges.\n\n\nCode\npip -q install pandas\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n# Get the number of nodes\nnum_nodes = A_torch.shape[0]\n\n# Create a list of all pairs of nodes\nsource_node, destination_node = np.meshgrid(np.arange(num_nodes), np.arange(num_nodes))\n\n# Flatten the arrays\nsource_node = source_node.flatten()\ndestination_node = destination_node.flatten()\n\n# Get the edge values\nhas_edge = A_torch[source_node, destination_node] &gt; 0\n\n# Create the DataFrame\ndf = pd.DataFrame({\n    'source_node': source_node,\n    'destination_node': destination_node,\n    'has_edge': has_edge\n})\n\ndf\n\n\n\n\n\n\n\n\n\nsource_node\ndestination_node\nhas_edge\n\n\n\n\n0\n0\n0\nFalse\n\n\n1\n1\n0\nTrue\n\n\n2\n2\n0\nTrue\n\n\n3\n3\n0\nFalse\n\n\n4\n4\n0\nFalse\n\n\n5\n0\n1\nTrue\n\n\n6\n1\n1\nFalse\n\n\n7\n2\n1\nFalse\n\n\n8\n3\n1\nFalse\n\n\n9\n4\n1\nFalse\n\n\n10\n0\n2\nTrue\n\n\n11\n1\n2\nFalse\n\n\n12\n2\n2\nFalse\n\n\n13\n3\n2\nFalse\n\n\n14\n4\n2\nFalse\n\n\n15\n0\n3\nFalse\n\n\n16\n1\n3\nFalse\n\n\n17\n2\n3\nFalse\n\n\n18\n3\n3\nFalse\n\n\n19\n4\n3\nTrue\n\n\n20\n0\n4\nFalse\n\n\n21\n1\n4\nFalse\n\n\n22\n2\n4\nFalse\n\n\n23\n3\n4\nTrue\n\n\n24\n4\n4\nFalse\n\n\n\n\n\n\n\n\n\nThe COO format is simple and flexible for sparse matrices. It consists of three arrays: row, col, and data. The row array contains the row indices of the non-zero elements, the col array contains the column indices of the non-zero elements, and the data array contains the values of the non-zero elements.\nLuckily, PyTorch has built in methods for converting between dense and sparse tensors. We can convert the adjacency matrix to a COO sparse tensor using the .to_sparse() method.\n\n\nCode\nA_coo_pytorch = A_torch.to_sparse()\nA_coo_pytorch\n\n\ntensor(indices=tensor([[0, 0, 1, 2, 3, 4],\n                       [1, 2, 0, 0, 4, 3]]),\n       values=tensor([True, True, True, True, True, True]),\n       size=(5, 5), nnz=6, layout=torch.sparse_coo)\n\n\nAnd convert back\n\n\nCode\nA_coo_pytorch.to_dense()\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\nLet’s look at the sparse tensor as a dataframe of source to destination nodes, but this time only for those pairs where there is an edge.\n\n\nCode\n# Get the source and destination nodes and the edge values\nsource_node = A_coo_pytorch.indices()[0].numpy()\ndestination_node = A_coo_pytorch.indices()[1].numpy()\nhas_edge = A_coo_pytorch.values().numpy()\n\n# Create the DataFrame\ndf = pd.DataFrame({\n    'source_node': source_node,\n    'destination_node': destination_node,\n    'has_edge': has_edge\n})\ndf\n\n\n\n\n\n\n\n\n\nsource_node\ndestination_node\nhas_edge\n\n\n\n\n0\n0\n1\nTrue\n\n\n1\n0\n2\nTrue\n\n\n2\n1\n0\nTrue\n\n\n3\n2\n0\nTrue\n\n\n4\n3\n4\nTrue\n\n\n5\n4\n3\nTrue\n\n\n\n\n\n\n\nWriting our own homebrew function to convert to COO format, is straightforward.\n\n\nCode\ndef to_sparse(tensor):\n    # Get the indices of the non-zero elements\n    indices = torch.nonzero(tensor).t()\n    # Get the values of the non-zero elements\n    values = tensor[indices[0], indices[1]]  # assuming 2D tensor\n    # Get the size of the original tensor\n    size = tensor.size()\n    # Create a sparse tensor\n    sparse_tensor = torch.sparse_coo_tensor(indices, values, size)\n    return sparse_tensor\n\n\n\n\nCode\nto_sparse(A_torch)\n\n\ntensor(indices=tensor([[0, 0, 1, 2, 3, 4],\n                       [1, 2, 0, 0, 4, 3]]),\n       values=tensor([True, True, True, True, True, True]),\n       size=(5, 5), nnz=6, layout=torch.sparse_coo)\n\n\n\n\nCode\ndef to_dense(sparse_tensor):\n    # Get the size of the original tensor\n    size = sparse_tensor.size()\n    # Get the indices and values from the sparse tensor\n    indices = sparse_tensor.coalesce().indices()\n    values = sparse_tensor.coalesce().values()\n    # Create a dense tensor of the same size and data type as values, initialized with zeros\n    dense_tensor = torch.zeros(size, dtype=values.dtype)\n    # Convert indices to a tuple of tensors\n    indices_tuple = tuple(indices[i] for i in range(indices.shape[0]))\n    # Use index_put_ to put the values in the dense tensor at the right indices\n    dense_tensor.index_put_(indices_tuple, values, accumulate=False)\n    return dense_tensor\n\n\n\n\nCode\nto_dense(A_coo_pytorch)\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\n\n\n\nCSR is an even more efficient compact format for sparse data. Details can be found here.\nValues and column indices are stored in the same way as COO format. The row indices are stored in a separate array, which can seem strange at first glance.\nBroadly it relies on slicing the values into per row chunks. The row chunk lengths are calculated as the difference between the row indices of the non-zero elements. The row chunk lengths are stored in a separate array, row_ptr.\nSo, if your original matrix has m rows, n columns, and nnz non-zero values, then:\n\nThe shape of values is [nnz].\nThe shape of column_indices is [nnz].\nThe shape of row_pointers is [m+1].\n\nIn index notation, the row pointer array is defined as:\nrow_ptr[i] = row_ptr[i-1] + number of non-zero elements in row i-1\nSo, the row pointer array stores the cumulative sum of the number of non-zero elements in each row.\nthe row and column number can be determined by the index of the non-zero element in the values array. That is the i-th non-zero element (in the values array) is at row row_ptr[i] and column col[i]. That is:\nvalues[i] = A[row_ptr[i], col[i]]\n\n\nCode\nA_torch.to_sparse_csr()\n\n\ntensor(crow_indices=tensor([0, 2, 3, 4, 5, 6]),\n       col_indices=tensor([1, 2, 0, 0, 4, 3]),\n       values=tensor([True, True, True, True, True, True]), size=(5, 5), nnz=6,\n       layout=torch.sparse_csr)\n\n\nWhich can be recovered by chaining with the .to_dense() method.\n\n\nCode\nA_torch.to_sparse_csr().to_dense()\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\nAnd here we write our homebrew version of to_csr() and from_csr() functions:\n\n\nCode\ndef to_csr(tensor):\n    # Get the indices of the non-zero elements\n    indices = torch.nonzero(tensor, as_tuple=True)\n    # Get the values of the non-zero elements\n    values = tensor[indices]\n    # Get the column indices of the non-zero elements\n    column_indices = indices[1]\n    # Get the row pointers\n    row_pointers = torch.zeros(tensor.size(0) + 1, dtype=torch.long)\n    row_pointers[1:] = torch.bincount(indices[0])\n    row_pointers = torch.cumsum(row_pointers, dim=0)\n\n    return values, column_indices, row_pointers\n\n\n\n\nCode\nto_csr(A_torch)\n\n\n(tensor([True, True, True, True, True, True]),\n tensor([1, 2, 0, 0, 4, 3]),\n tensor([0, 2, 3, 4, 5, 6]))\n\n\n\n\nCode\ndef from_csr(values, column_indices, row_pointers):\n    # Get the number of rows and columns\n    num_rows = row_pointers.size(0) - 1\n    num_cols = torch.max(column_indices).item() + 1\n    # Create a dense tensor of the right size, initialized with zeros\n    tensor = torch.zeros((num_rows, num_cols), dtype=values.dtype)\n    # Loop over the rows\n    for i in range(num_rows):\n        # Get the start and end indices for this row in the values and column_indices arrays\n        start = row_pointers[i].item()\n        end = row_pointers[i + 1].item()\n        # Set the values in the dense tensor for this row\n        tensor[i, column_indices[start:end]] = values[start:end]\n\n    return tensor\n\n\n\n\nCode\nfrom_csr(*to_csr(A_torch))\n\n\ntensor([[False,  True,  True, False, False],\n        [ True, False, False, False, False],\n        [ True, False, False, False, False],\n        [False, False, False, False,  True],\n        [False, False, False,  True, False]])\n\n\n\n\n\nThe CSC format is similar to the CSR format, but with the rows and columns swapped. The values and row indices are stored in the same way as the CSR format. The column indices are stored in a separate array, col_ptr.\nThis format is efficient for certain slicing operations, such as extracting a column from a matrix."
  },
  {
    "objectID": "posts/sparse_tensors/index.html#memory-usage-speed-up-with-sparse-tensors-on-the-monitor-graph",
    "href": "posts/sparse_tensors/index.html#memory-usage-speed-up-with-sparse-tensors-on-the-monitor-graph",
    "title": "Sparsity with PyTorch Tensors",
    "section": "",
    "text": "Let’s see how much faster it is to perform matrix multiplication on the sparse tensor compared to the dense tensor. For this we’ll use the ModelNet10 monitor graph seen earlier.\n\n\nCode\nwith open('./data/monitor_0001.off', 'r') as f:\n    verts, faces = read_off(f)\n\nG = off_to_graph(verts, faces)\nA = nx.adjacency_matrix(G)\nA\n\n\n&lt;798x798 sparse array of type '&lt;class 'numpy.int64'&gt;'\n    with 2952 stored elements in Compressed Sparse Row format&gt;\n\n\nA is already a sparse matrix, but in numpy. We’ll convert it to a PyTorch dense tensor first.\n\n\nCode\nA_dense = torch.from_numpy(A.toarray())\nA_dense.shape\n\n\ntorch.Size([798, 798])\n\n\n\n\nCode\ndef tensor_memory_usage_str(tensor=None):\n    if tensor is None:\n        return \"No tensor provided\"\n    bytes = tensor.element_size() * tensor.nelement()\n    kilobytes = bytes / 1024\n    return f\"Memory usage: {bytes} bytes ({kilobytes} KB)\"\n\n\n\n\nCode\ntensor_memory_usage_str(A_dense)\n\n\n'Memory usage: 5094432 bytes (4975.03125 KB)'\n\n\n\n\nCode\ntensor_memory_usage_str(A_dense.to_sparse())\n\n\n'Memory usage: 5094432 bytes (4975.03125 KB)'\n\n\nSomewhat unexpectedly, the sparse and dense tensors have the same memory usage.\n\n\nCode\nA_sparse = A_dense.to_sparse_csr()\n\n\n\n\nCode\ntensor_memory_usage_str(A_sparse)\n\n\n'Memory usage: 5094432 bytes (4975.03125 KB)'\n\n\nOK, so there was no improvement in memory usage for this particular case. Let’s time the matrix multiplication operations.\n\n\nCode\nimport timeit\n\n\ndef multiply_tensors():\n    return A_dense @ A_dense\n\nnum_runs = 10\ntime_taken_dense = timeit.timeit( multiply_tensors, number=num_runs)\n\nprint(f\"Time taken by my_function over {num_runs} runs: {time_taken_dense} seconds\")\n\n\nTime taken by my_function over 10 runs: 2.746476124972105 seconds\n\n\n\n\nCode\nA_coo = A\n\ndef multiply_tensors():\n    return A_coo@A_coo\n\nnum_runs = 10\ntime_taken_sparse = timeit.timeit( multiply_tensors, number=num_runs)\n\nprint(f\"Time taken by my_function over {num_runs} runs: {time_taken_sparse} seconds\")\n\n\nTime taken by my_function over 10 runs: 0.0038774579879827797 seconds\n\n\n\n\nCode\nf\"Speedup: {time_taken_dense / time_taken_sparse}x\"\n\n\n'Speedup: 708.3187318815902x'\n\n\nWith CSR encoding of sparse tensors, it might be even better - however I’m on Apple Silicon, and the csr sparse tensor support is not yet available for this architecture."
  },
  {
    "objectID": "posts/sparse_tensors/index.html#conclusion",
    "href": "posts/sparse_tensors/index.html#conclusion",
    "title": "Sparsity with PyTorch Tensors",
    "section": "",
    "text": "We just saw how to work with sparse tensors in PyTorch. We created a sparse tensor from an adjacency matrix and a list of edges, and converted it to different sparse formats. We also saw the performance benefits of using sparse tensors for matrix multiplication. There’s a lot more to sparsity, and PyTorch is actively developing its sparse tensor support. Things will break, but expect ramp up in performance gains."
  },
  {
    "objectID": "posts/git-search/index.html",
    "href": "posts/git-search/index.html",
    "title": "Git Repository Search",
    "section": "",
    "text": "“Robot Magnifying Glass Search”\n\n\nPhoto by Growtika on Unsplash\nThere are many ways to search a repository, particularly a Git Repo. We will outline some use cases with examples for a “Unix-like” file directory and also a Git Repo.\nLet’s do this for a library I’ve been looking at.\nSome unit tests break on Apple Silicon for the open source library pyg. The maintainer disabled some tests. I want to find them. It has something to do with PyTorch not fully supporting compressed sparse tensor representations on Apple’s mps framework for Apple Silicon. I received the following note:\n\nthere are a few tests that were disabled around test_sparse\n\nand\n\nthe convert_coo_to_csr_indices doesn’t seem to be supported.\n\nIt’s likely that test_sparse and convert_coo_to_csr_indices are variable names or tokens inside a code file of the git repository. However, for illustration, we will assume that they could be anywhere in the git repo (filenames, directory names, commit messages, variable names, past commits, current directory).\nTime to search. The repo can be cloned locally from here and then cd into it.\n\n\nCode\nimport os\n\nos.chdir(\"/Users/ravikalia/Code/github.com/ml-blog/posts/git-search/\")\n\n\n\n\nCode\nprint(os.getcwd())\n\n\n/Users/ravikalia/Code/github.com/ml-blog/posts/git-search\n\n\n\n\nCode\n%%bash\nrm -rf pytorch_geometric\ngit clone https://github.com/pyg-team/pytorch_geometric.git\ngit fetch --all\n\n\nCloning into 'pytorch_geometric'...\n\n\nLet’s change the directory to the root of the cloned repo, which makes searching easier\n\n\nCode\nos.chdir(\"./pytorch_geometric\")\n\n\n\n\nWe can look for the string test_sparse in filenames using the shell command line tool find.\n\n\nCode\n%%bash\nfind . -name \"*test_sparse*\" -o -name \"*convert_coo_to_csr_indices*\"\n\n\n./test/utils/test_sparse.py\n\n\ngreat, so we have a file to look at. Let’s look at the file test_sparse.py. It seems to be unit tests related to sparsity, possibly testing utility functions for converting between sparse tensor representations.\n\n\n\nString search is a bit more complicated. grep is an awesome tool for this.\n\n\nCode\n%%bash\ngrep -rn . -e \"test_sparse\" -e \"convert_coo_to_csr_indices\"\n\n\n./test/utils/test_cross_entropy.py:9:def test_sparse_cross_entropy_multiclass(with_edge_label_weight):\n./test/utils/test_cross_entropy.py:32:def test_sparse_cross_entropy_multilabel(with_edge_label_weight):\n./test/test_edge_index.py:102:def test_sparse_tensor(dtype, device):\n./test/test_edge_index.py:992:def test_sparse_narrow(device):\n./test/test_edge_index.py:1026:def test_sparse_resize(device):\n./torch_geometric/testing/asserts.py:24:    test_sparse_layouts: Optional[List[Union[str, torch.layout]]] = None,\n./torch_geometric/testing/asserts.py:49:        test_sparse_layouts (List[str or int], optional): The sparse layouts to\n./torch_geometric/testing/asserts.py:62:    if test_sparse_layouts is None:\n./torch_geometric/testing/asserts.py:63:        test_sparse_layouts = SPARSE_LAYOUTS\n./torch_geometric/testing/asserts.py:74:    if len(test_sparse_layouts) &gt; 0 and sparse_size is None:\n./torch_geometric/testing/asserts.py:75:        raise ValueError(f\"Got sparse layouts {test_sparse_layouts}, but no \"\n./torch_geometric/testing/asserts.py:93:    for layout in (test_sparse_layouts or []):\nBinary file ./.git/index matches\n\n\nMany locations matched to 3 files. It’s possible they aren’t all relevant for testing purpose. The .git/index is a binary file, which is used by git to store information about the repository, it’s not relevant for our task.\n\n\n\nGit is a distributed version control system. It is a tool that tracks changes in files and directories. At user-defined snapshots in time, called commits, it records the changes made to the files and directories. As a consequence it is possible to search for changes in the repository across snapshots.\nAlong with grep and find, there are git specific tools for searching snapshots of the repo, commit messages and filtering by date and author, such as:\n\ngit ls-files\ngit log\ngit grep\n\n\n\n\nThe working tree is what you see when you list the files in your project’s directory that are being tracked. It’s the version of your project that you’re currently working on. The git checkout command is used to update the working directory with a specific commit, matching the snapshot recorded in the commit. Untracked files are not affected by git checkout.\nThe git ls-files command lists the files in the working tree that are being tracked by git. The filenames can be searched for a string using the grep command.\n\n\nCode\n%%bash\ngit ls-files | grep \"test_sparse\"\n\n\ntest/utils/test_sparse.py\n\n\nThere’s not much else that can be done with git ls-files for searching filenames over snapshot commits. Some ways to search for filenames in the git repository across commits are:\nlog commit messages with commit where the filename was changed.\n\n\nCode\n%%bash\ngit log --all -- *test_sparse*\n\n\ncommit 62fa51e0000913e1b3023b817485d2b248322539\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sun Dec 24 11:56:08 2023 +0100\n\n    Accelerate concatenation of `torch.sparse` tensors (#8670)\n    \n    Fixes #8664\n\ncommit 1c89e751804d1eb2fb626dabc677198a1878c34d\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Wed Oct 4 09:59:36 2023 +0200\n\n    Skip TorchScript bug for PyTorch &lt; 1.12 (#8123)\n\ncommit 51c50c2f9d3372de34f4ac3617f396384a36558c\nAuthor: filipekstrm &lt;filip.ekstrom@hotmail.com&gt;\nDate:   Tue Oct 3 20:39:04 2023 +0200\n\n    Added `mask` argument to `dense_to_sparse` (#8117)\n    \n    Added optional argument mask to dense_to_sparse so that it can correctly\n    invert a call to to_dense_adj by returning the correct edge_index in\n    case there are graphs with different number of nodes (and hence, the\n    dense adjacency matrix contains some padding)\n    \n    ---------\n    \n    Co-authored-by: Filip Ekström Kelvinius &lt;filek51@lnx00195.ad.liu.se&gt;\n    Co-authored-by: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 2e0e7e35ce0e5a3dbe951ced527aa436e591bccd\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Thu Aug 24 08:28:32 2023 +0200\n\n    [Test] PyTorch 2.1 supports gradients on CSR matrices (#7924)\n\ncommit f80afef6fc27b8534261659ebdfd10efed8f4540\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Fri Jul 7 06:27:56 2023 +0700\n\n    Replace `torch.Tensor`calls (3/4) (#7697)\n\ncommit c0eb49f95198183e8cf15007bbd286c9ced7829a\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Thu Jun 29 06:10:56 2023 +0200\n\n    Test PyG on PyTorch &gt;= 1.11 (#7656)\n\ncommit 1e57a6c69cf15868bccc10b79479fdc4e7468211\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sun Jun 4 11:27:47 2023 +0200\n\n    Update the documentation of `GraphMaskExplainer` (#7504)\n\ncommit 0d35d8d9f0ad73b05a0bfd02e86caa7131f64177\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Thu May 11 15:21:49 2023 +0200\n\n    Test PyTorch `coalesce` bug on `torch.load` (#7351)\n\ncommit 93f9f590bb62a848e3f57b8fcaee5fd48c768436\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sun Mar 26 16:14:07 2023 +0200\n\n    Accelerated sparse tensor conversions (#7042)\n\ncommit c78c5b217f9fb652555512405fb561c33ae0b439\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sat Mar 25 16:59:59 2023 +0100\n\n    Test against nightly PyTorch releases (#7036)\n\ncommit c78f358bb664a15650c68113f7108be89aca98da\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Thu Mar 23 07:05:11 2023 +0100\n\n    Fix full test (#7007)\n\ncommit 153cb5cc6bd014b8fb453c5457a86d91f4b4483d\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sat Mar 18 23:16:05 2023 +0100\n\n    Add PyTorch 1.13 CI tests (#6960)\n\ncommit 9bda455cf0527dad6ee872175fa1f9a8b036fc80\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sat Mar 18 11:25:40 2023 +0100\n\n    Added `to_torch_csc_tensor` functionality (#6951)\n\ncommit d31a275b2d688bc5c8e4df17736ff3b6bbcbe965\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Fri Mar 17 10:31:41 2023 +0100\n\n    Fix PyTorch 2.0 warnings and add `torch.sparse_csr` utilities (#6939)\n\ncommit f865ab56cb4a8360d428978d67bd77ebdf01a15c\nAuthor: Jintang Li &lt;cnljt@outlook.com&gt;\nDate:   Fri Feb 17 22:40:05 2023 +0800\n\n    Added `utils.to_edge_index` to convert sparse tensors to edge indices and edge attributes (#6728)\n    \n    This PR implements `to_edge_index` to simplify the conversion between\n    PyTorch Sparse Tensor and edge indices. Perhaps there is a better name\n    than `to_edge_index`?\n    \n    ---------\n    \n    Co-authored-by: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 0c8f35fa9458bc4eb8ce43c3694ba35481623dde\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Mon Jan 30 16:22:46 2023 +0100\n\n    Bipartite `dense_to_sparse` support (#6546)\n    \n    Fixes #6507\n\ncommit 01037dbbedd48b4bff665f66cae9a5f23dad2e0e\nAuthor: Jintang Li &lt;cnljt@outlook.com&gt;\nDate:   Tue Nov 22 03:09:14 2022 +0800\n\n    Add `is_sparse` and `to_torch_coo_tensor` (#6003)\n    \n    This PR aims to\n    + Add `is_sparse` to check if the input is either `torch.sparse.Tensor`\n    or `torch_sparse.SparseTensor`\n    + Add `to_torch_coo_tensor` to convert `edge_index` and `edge_weight` to\n    `torch.sparse.Tensor` (in COO format)\n    + Move `is_torch_sparse_tensor` from `torch_sparse_tensor.py` to\n    `sparse.py`. I think this makes the structure in `torch_geometric.utils`\n    clearer.\n    + Clean up duplicated code in `message_passing.py`.\n    \n    Co-authored-by: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 0be0eca73920ca19d8fc99c963e6b49ff86c75f6\nAuthor: Akash Vartak &lt;cg82493@umbc.edu&gt;\nDate:   Thu Oct 13 14:30:35 2022 -0400\n\n    [Type Hints] `utils.dense_to_sparse` (#5683)\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 66b17806b1f4a2008e8be766064d9ef9a883ff03\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Fri Jan 21 11:34:22 2022 +0100\n\n    sort imports via isort\n\ncommit f29f27a26a97304995a02f92716c8e2d858f2051\nAuthor: pre-commit-ci[bot] &lt;66853113+pre-commit-ci[bot]@users.noreply.github.com&gt;\nDate:   Fri Jan 21 10:31:10 2022 +0000\n\n    [pre-commit.ci] auto fixes from pre-commit.com hooks\n    \n    for more information, see https://pre-commit.ci\n\ncommit ff0ff6190b66a9a40720073b4eefc8fc92d726ca\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Mon May 24 11:27:18 2021 +0200\n\n    dense_to_sparse with batched adjacencies\n\ncommit 550c6319261a56b87112837aae392927f401aed1\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sun Jun 16 18:08:31 2019 +0200\n\n    dense to sparse impl\n\ncommit d01ea9dab80be0a698fbdac341d3f777013ac527\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Mon Jun 10 19:30:51 2019 +0200\n\n    to dense adj implementation + removal of unused convert methods\n\ncommit 1c8ea5cdac21e8dd35b1a2c43c9f644900d65937\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Thu Mar 28 17:33:44 2019 +0100\n\n    test sparse\n\ncommit 2c01aa22c697b4326db05b7729f85613ebb8a8a0\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Fri Dec 28 17:04:26 2018 +0100\n\n    test utils\n\n\n\n\n\nIf we want to search for a string across commits, the main power of git comes from the git log and git grep commands.\n\n\nCode\n%%bash\ngit log -S \"test_sparse\" --all\n\n\ncommit dba9659f6c4f29fd2be1f50b5ea12a29a926082f\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Thu Feb 29 14:04:19 2024 +0100\n\n    Fix `EdgeIndex.resize_` linting issues (#8993)\n\ncommit 123e38ef6715f75ed9198d256cc2cb984b431630\nAuthor: Poovaiah Palangappa &lt;98763718+pmpalang@users.noreply.github.com&gt;\nDate:   Sun Feb 11 03:32:44 2024 -0800\n\n    Example of a recommender system (#8546)\n    \n    Hi Everyone,\n    \n    I'm adding a recommender system example with the following salient\n    features\n    \n    1. Dataset MovieLens – a heterogenous use case\n    2. Demonstrates the use of edge based temporal sampling\n    3. Visualization\n      t-SNE based visualization (--visualize_emb)\n    4. Uses torch_geometric.nn.pool.MIPSKNNIndex for getting recommedations\n    5. Integration of the LinkPred metrics -- precision@k and ndcg@k\n    \n    Thanks,\n    Poovaiah\n    \n    ---------\n    \n    Co-authored-by: pre-commit-ci[bot] &lt;66853113+pre-commit-ci[bot]@users.noreply.github.com&gt;\n    Co-authored-by: Akihiro Nitta &lt;nitta@akihironitta.com&gt;\n    Co-authored-by: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 23bbc128d3df42fb11f187d0abd1c10eb569b3a0\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sat Dec 30 17:10:10 2023 +0100\n\n    Allow `EdgeIndex` creation from `torch.sparse` tensors (#8690)\n\ncommit ed9698d0bff2049413473c87f26774bedd190867\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Mon Dec 4 11:07:31 2023 +0100\n\n    Include `mypy` in `pre-commit` (#8520)\n    \n    Co-authored-by: Akihiro Nitta &lt;nitta@akihironitta.com&gt;\n\ncommit 801723efacee5ad2597256ba0d9934600512e626\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sat May 27 13:20:34 2023 +0200\n\n    Sparse `cross_entropy` implementation (#7447)\n\ncommit 1dadc070565ae45ce3a00f150b2fe50809c49959\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Mon Apr 10 02:50:51 2023 +0100\n\n    Helper assert function to test for module equivalence and invariance (#7144)\n\ncommit 7b4892781e2198ad99a8655da03133505619040a\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sun Jun 28 11:23:29 2020 +0200\n\n    arma conv jittable\n\ncommit 93fab2e53dbd812c8cb08fc7a789ec0ace9e8768\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Mon Jun 8 15:26:35 2020 +0200\n\n    gcn conv\n\ncommit d01ea9dab80be0a698fbdac341d3f777013ac527\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Mon Jun 10 19:30:51 2019 +0200\n\n    to dense adj implementation + removal of unused convert methods\n\ncommit 2c01aa22c697b4326db05b7729f85613ebb8a8a0\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Fri Dec 28 17:04:26 2018 +0100\n\n    test utils\n\ncommit eb4260ce0154d2804f1e552db5be1d607a21d87d\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Fri Apr 13 15:58:53 2018 +0200\n\n    outsourced spline_conv, new voxel grid implementation\n\ncommit 544f4ad0e254e8fd7c96570756ac68ea15e6fe1d\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Mon Apr 9 09:05:36 2018 +0200\n\n    fixed tests\n\n\nto be specific to a branch, replace –all with the branch name (master in this case)\n\n\nCode\n%%bash\ngit log master -S \"convert_coo_to_csr_indices\" \n\n\n\n\nCode\n%%bash\ngit log master -S \"test_sparse\" --pretty=format:\"%h\" --name-only --diff-filter=A\n\n\n801723efa\ntest/utils/test_cross_entropy.py\n\n1dadc0705\ntorch_geometric/testing/asserts.py\n\n2c01aa22c\ntest/utils/test_sparse.py\n\n\nWith regular expression search use G (* glob is not needed as it’s implied with regular expressions).\n\n\nCode\n%%bash\ngit log -G \"convert_coo_to_csr_indices\" --pretty=format:\"%h\" --name-only\n\n\n\n\nCode\n%%bash\ngit log -G \"coo_to_csr\" --pretty=format:\"%h\" --name-only\n\n\n390942fc4\ntorch_geometric/data/edge_index.py\n\n699120e25\ntorch_geometric/data/edge_index.py\n\na6f0f4947\ntorch_geometric/data/edge_index.py\n\ncf786b735\ntorch_geometric/data/edge_index.py\n\nb825dc637\ntorch_geometric/data/edge_index.py\n\nb5ecfd9b4\ntorch_geometric/data/graph_store.py\ntorch_geometric/nn/conv/cugraph/base.py\ntorch_geometric/nn/conv/rgcn_conv.py\ntorch_geometric/nn/dense/linear.py\ntorch_geometric/sampler/utils.py\ntorch_geometric/transforms/gdc.py\ntorch_geometric/utils/sparse.py\n\na73043736\ntorch_geometric/data/graph_store.py\ntorch_geometric/nn/conv/rgcn_conv.py\ntorch_geometric/nn/dense/linear.py\n\nd29154558\ntorch_geometric/nn/conv/cugraph/base.py\n\n886352bd6\ntorch_geometric/transforms/gdc.py\n\nce2a84f4d\ntorch_geometric/sampler/utils.py\n\n0b3f8e98a\ntorch_geometric/sampler/utils.py\n\n\n\n\nCode\n%%bash\ngit log -G \"convert_coo\" --pretty=format:\"%h\" --name-only\n\n\n\n\nCode\n%%bash\ngit log -G \"csr_indices\" --pretty=format:\"%h\" --name-only\n\n\n\n\nCode\n%%bash\ngit log master -G\"test_sparse\" --pretty=format:\"%h\" --name-only\n\n\ndba9659f6\ntest/test_edge_index.py\n\n123e38ef6\ntest/test_edge_index.py\n\n23bbc128d\ntest/test_edge_index.py\n\ned9698d0b\ntorch_geometric/testing/asserts.py\n\n1725f1436\ntest/utils/test_cross_entropy.py\n\n801723efa\ntest/utils/test_cross_entropy.py\n\n1dadc0705\ntorch_geometric/testing/asserts.py\n\n7b4892781\ntest/nn/conv/test_gcn_conv.py\n\n72e8ef33d\ntest/nn/conv/test_gcn_conv.py\n\n93fab2e53\ntest/nn/conv/test_gcn_conv.py\n\nd01ea9dab\ntest/utils/test_sparse.py\n\n2c01aa22c\ntest/utils/test_sparse.py\n\neb4260ce0\ntorch_geometric/nn/functional/pool/voxel_pool_test.py\n\n544f4ad0e\ntorch_geometric/nn/functional/pool/voxel_pool_test.py\n\n\n\n\n\n\n\nCode\n%%bash\ngit log --author=\"ravkalia\"\n\n\ncommit f0e4c829662df9eb67fd5c0abda002c9b7cd0afb\nAuthor: Ravi Kalia &lt;ravkalia@gmail.com&gt;\nDate:   Sun Mar 24 08:05:12 2024 -0500\n\n    Replace `withCUDA` decorator: `withDevice` (#9082)\n    \n    Replace `withCUDA` for a `withDevice` decorator.\n    \n    Change variable name from devices to processors to reduce confusion\n    against pytorch api (backends/devices) and reflect the hardware choices.\n    \n    Note that at this time:\n    \n    ## Hardware\n    3 repertoires of hardware can be used to run pyTorch code:\n    \n    * CPU only\n    * CPU and GPU\n    * Unified Memory Single Chip\n    \n    ## Backend Software\n    The backend is the software framework used to process tensors by\n    pytorch. There are several. For example, the following are the backends\n    available today:\n    \n    \n    ```python\n    torch.backends.cpu\n    torch.backends.cuda\n    torch.backends.cudnn\n    torch.backends.mha\n    torch.backends.mps\n    torch.backends.mkl\n    torch.backends.mkldnn\n    torch.backends.nnpack\n    torch.backends.openmp\n    torch.backends.opt_einsum\n    torch.backends.xeon\n    ```\n    `mps` is the only backend which works with the unified memory single\n    chip - currently Apple's M series.\n    \n    Hardware is determined by `torch.cuda.is_available()` and\n    `torch.backends.mps.is_available()` which confuses when looking up\n    `backends` and hardaware types/architectures/vendors. Introducing\n    `processor/processors` variable name reduces cognitive load, while\n    allowing future developments from vendors/frameworks.\n    \n    \n    \n    ~~Modify `withCUDA` decorator to be single purpose. Downstream\n    comprehensive distributed device and backend testing can be developed\n    later.~~\n    \n    ~~Redundant code was removed, this doesn't affect other components.~~\n    \n    ---------\n    \n    Co-authored-by: pre-commit-ci[bot] &lt;66853113+pre-commit-ci[bot]@users.noreply.github.com&gt;\n    Co-authored-by: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 1675b019c7182dbdc4970561f0dbff6dec3ee299\nAuthor: Ravi Kalia &lt;ravkalia@gmail.com&gt;\nDate:   Fri Mar 8 14:44:56 2024 -0500\n\n    feat(pyproject.toml): update & sort (#9024)\n    \n    - Add 'ipython' and 'matplotlib-inline' to dev dependencies\n    - Reorder 'torch_geometric[test]' in dev dependencies list\n    \n    Introduced for developer install to pass tests on apple silicon\n    \n    \n    Notes here:\n    \n    \n    https://project-delphi.github.io/ml-blog/posts/developing-pytorch-geometric-on-m1/\n\ncommit 25b2f208e671eeec285bfafa2e246ea0a234b312\nAuthor: Ravi Kalia &lt;ravkalia@gmail.com&gt;\nDate:   Wed Feb 21 11:11:33 2024 -0500\n\n    docs: fix broken links to source of graph classification datasets (#8946)\n    \n    **Update Broken Dataset Links in Documentation**\n    \n    This PR addresses broken links in the documentation that pointed to the\n    common benchmark datasets. The links were updated to point to the\n    correct URL.\n    \n    Changes were made in the following files:\n    \n    1. `benchmark/kernel/README.md`\n    2. `docs/source/get_started/introduction.rst`\n    \n    The specific changes are as follows:\n    \n    In `benchmark/kernel/README.md`:\n    \n    ```diff\n    - Evaluation script for various methods on [common benchmark datasets](http://graphkernels.cs.tu-dortmund.de) via 10-fold cross validation, where a training fold is randomly sampled to serve as a validation set.\n    + Evaluation script for various methods on [common benchmark datasets](https://chrsmrrs.github.io/datasets/) via 10-fold cross validation, where a training fold is randomly sampled to serve as a validation set.Update Broken Dataset Links in Documentation\n    ```\n    \n    In `docs/source/get_started/introduction.rst`:\n    \n    ```diff\n    - :pyg:`PyG` contains a large number of common benchmark datasets, *e.g.*, all Planetoid datasets (Cora, Citeseer, Pubmed), all graph classification datasets from `http://graphkernels.cs.tu-dortmund.de &lt;http://graphkernels.cs.tu-dortmund.de/&gt;`_ and their `cleaned versions &lt;https://github.com/nd7141/graph_datasets&gt;`_, the QM7 and QM9 dataset, and a handful of 3D mesh/point cloud datasets like FAUST, ModelNet10/40 and ShapeNet.\n    + :pyg:`PyG` contains a large number of common benchmark datasets, *e.g.*, all Planetoid datasets (Cora, Citeseer, Pubmed), all graph classification datasets from `TUDatasets https://chrsmrrs.github.io/datasets/`_ and their `cleaned versions &lt;https://github.com/nd7141/graph_datasets&gt;`_, the QM7 and QM9 dataset, and a handful of 3D mesh/point cloud datasets like FAUST, ModelNet10/40 and ShapeNet.\n    ```\n    \n    Please review these changes and merge the PR if everything is in order\n    or let me know if there are any issues or further changes needed.\n    \n    ---------\n    \n    Co-authored-by: Ravi Kalia &lt;rkalia.consultant@kriyatx.com&gt;\n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n\n\n\n\nCode\n%%bash\ngit log --author=\"ravkalia\" --since=\"2022-01-01\" --until=\"2024-02-31\"\n\n\ncommit 25b2f208e671eeec285bfafa2e246ea0a234b312\nAuthor: Ravi Kalia &lt;ravkalia@gmail.com&gt;\nDate:   Wed Feb 21 11:11:33 2024 -0500\n\n    docs: fix broken links to source of graph classification datasets (#8946)\n    \n    **Update Broken Dataset Links in Documentation**\n    \n    This PR addresses broken links in the documentation that pointed to the\n    common benchmark datasets. The links were updated to point to the\n    correct URL.\n    \n    Changes were made in the following files:\n    \n    1. `benchmark/kernel/README.md`\n    2. `docs/source/get_started/introduction.rst`\n    \n    The specific changes are as follows:\n    \n    In `benchmark/kernel/README.md`:\n    \n    ```diff\n    - Evaluation script for various methods on [common benchmark datasets](http://graphkernels.cs.tu-dortmund.de) via 10-fold cross validation, where a training fold is randomly sampled to serve as a validation set.\n    + Evaluation script for various methods on [common benchmark datasets](https://chrsmrrs.github.io/datasets/) via 10-fold cross validation, where a training fold is randomly sampled to serve as a validation set.Update Broken Dataset Links in Documentation\n    ```\n    \n    In `docs/source/get_started/introduction.rst`:\n    \n    ```diff\n    - :pyg:`PyG` contains a large number of common benchmark datasets, *e.g.*, all Planetoid datasets (Cora, Citeseer, Pubmed), all graph classification datasets from `http://graphkernels.cs.tu-dortmund.de &lt;http://graphkernels.cs.tu-dortmund.de/&gt;`_ and their `cleaned versions &lt;https://github.com/nd7141/graph_datasets&gt;`_, the QM7 and QM9 dataset, and a handful of 3D mesh/point cloud datasets like FAUST, ModelNet10/40 and ShapeNet.\n    + :pyg:`PyG` contains a large number of common benchmark datasets, *e.g.*, all Planetoid datasets (Cora, Citeseer, Pubmed), all graph classification datasets from `TUDatasets https://chrsmrrs.github.io/datasets/`_ and their `cleaned versions &lt;https://github.com/nd7141/graph_datasets&gt;`_, the QM7 and QM9 dataset, and a handful of 3D mesh/point cloud datasets like FAUST, ModelNet10/40 and ShapeNet.\n    ```\n    \n    Please review these changes and merge the PR if everything is in order\n    or let me know if there are any issues or further changes needed.\n    \n    ---------\n    \n    Co-authored-by: Ravi Kalia &lt;rkalia.consultant@kriyatx.com&gt;\n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n\n\n\n\nCode\n%%bash\ngit log --grep=\"docs\"  --since=\"2022-01-01\" --until=\"2022-02-31\"\n\n\ncommit 24a185e7268f70ee549c7a424b9426b9a18b5706\nAuthor: Ramona Bendias &lt;ramona.bendias@gmail.com&gt;\nDate:   Mon Feb 21 13:03:52 2022 +0000\n\n    Add general `Explainer` Class (#4090)\n    \n    * Add base Explainer\n    \n    * Update Explainer\n    \n    * Fix test\n    \n    * Clean code\n    \n    * Update test/nn/models/test_explainer.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Add hints and add get_num_hops\n    \n    * Fix\n    \n    * Change docstring\n    \n    * Update torch_geometric/utils/subgraph.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Fix tests\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n    * some typos\n    \n    * typo\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 6002170a513107d66d3bc37cf39dc924f380f694\nAuthor: RBendias &lt;ramona.bendias@gmail.com&gt;\nDate:   Wed Feb 9 12:07:02 2022 +0100\n\n    Make models compatible to Captum (#3990)\n    \n    * Add PNA model\n    \n    * Add to_captum for edge_masks\n    \n    * Add node explainability with captum\n    \n    * Update\n    \n    * Add CaptumModel documentation\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Add Captum dependency\n    \n    * Update captum_explainability.py\n    \n    * Update docs\n    \n    * Update docs\n    \n    * update doc\n    \n    Co-authored-by: @ramonabendias &lt;ramona@workist.com&gt;\n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 14d588d4ceb2d9c0bd71efa4f1051fb041154f2c\nAuthor: Konstantinos Kogkalidis &lt;konstantinos@riseup.net&gt;\nDate:   Mon Feb 7 09:48:21 2022 +0100\n\n    Update attention.py (#4009)\n    \n    fix shape of output tensor in docstring for GlobalAttention\n\ncommit 50ff5e6d60dacad8482fe71351bfe48882e95f83\nAuthor: RBendias &lt;ramona.bendias@gmail.com&gt;\nDate:   Wed Feb 2 20:58:55 2022 +0100\n\n    Add `full` extras to install command in contribution docs (#3991)\n    \n    * Add PNA model\n    \n    * Add `full` extras to contribution install command\n\ncommit 1e24b3a16fa2e92f9ad558ec2544c29132f7129b\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Wed Jan 26 16:26:24 2022 +0100\n\n    Refactor: `MLP` initialization (#3957)\n    \n    * update MLP\n    \n    * docstring\n    \n    * fix test\n    \n    * typo\n    \n    * typo\n\ncommit 3e4891be64a54b01cd8aa00852db830bc5459784\nAuthor: Arun &lt;arun.palaniappan1999@gmail.com&gt;\nDate:   Thu Jan 20 13:09:50 2022 +0530\n\n    Doc improvements to set2set layers (#3889)\n    \n    * added type hints\n    \n    * doc updates\n    \n    * set2set parameter to accept None arg\n    \n    * added shapes of input and output [docs]\n    \n    * minor fix\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * small update\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit fac848c25f9c0181e11869a6bcd85ed242d982c5\nAuthor: Otávio Calaça &lt;otaviocx@gmail.com&gt;\nDate:   Tue Jan 18 05:23:21 2022 -0300\n\n    Let `TemporalData` inherit from `BaseData` and add docs (#3867)\n    \n    * Refactor TemporalData class to inherit from BaseData\n    \n    * Fixes to get TemporalData working\n    \n    * Small fixes in __delitem__ of TemporalData\n    \n    * Add batch, __cat_dim__ and __inc__ to TemporalData\n    \n    * Add Docs to TemporalData\n    \n    * [pre-commit.ci] auto fixes from pre-commit.com hooks\n    \n    for more information, see https://pre-commit.ci\n    \n    * Lint fixes\n    \n    * Add removed method TemporalData.seq_batches\n    \n    * Update torch_geometric/data/temporal.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Update torch_geometric/data/temporal.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Changes requested in review\n    \n    * Removing trailing whitespace\n    \n    * fix doc + some inheritance issues\n    \n    * fix iter\n    \n    Co-authored-by: pre-commit-ci[bot] &lt;66853113+pre-commit-ci[bot]@users.noreply.github.com&gt;\n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 0c29b0d5b5d3b23e981601367a47222bddd9c34b\nAuthor: Stefano Roncelli &lt;45285915+saiden89@users.noreply.github.com&gt;\nDate:   Sat Jan 8 08:46:49 2022 +0100\n\n    Updated docstring for shape info - part 2 (#3739)\n    \n    * update docstrings for shape\n    \n    * update docstrings for shape\n    fix out channels description\n    \n    * update docstrings for shape\n    \n    * fix linting\n    \n    * update\n    \n    * update\n    \n    * update\n    \n    * update\n    \n    * fix tests\n    \n    * typo\n    \n    * fix typos\n    \n    * fix typo\n    \n    * update docs\n    \n    * update docstring for shape\n    fix typos for cheb_conv\n    \n    * rollback gmm_conv\n    \n    Co-authored-by: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\n\n\n\n\nCode\n%%bash\ngit log --oneline --grep=\"docs\"  --since=\"2022-01-01\" --until=\"2022-02-31\"\n\n\n24a185e72 Add general `Explainer` Class (#4090)\n6002170a5 Make models compatible to Captum (#3990)\n14d588d4c Update attention.py (#4009)\n50ff5e6d6 Add `full` extras to install command in contribution docs (#3991)\n1e24b3a16 Refactor: `MLP` initialization (#3957)\n3e4891be6 Doc improvements to set2set layers (#3889)\nfac848c25 Let `TemporalData` inherit from `BaseData` and add docs (#3867)\n0c29b0d5b Updated docstring for shape info - part 2 (#3739)\n\n\n\n\n\nThe main differences between git grep and grep are:\ngit grep only searches through your tracked files, while grep can search through any files. git grep is aware of your Git repository structure and can search through old commits, branches, etc., while grep only searches through the current state of files.\ngit grep is faster than grep when searching through a Git repository because it takes advantage of Git’s index data structure.\n\n\nCode\n%%time\n%%bash\ngit grep \"test_sparse\" &gt; /dev/null\n\n\nCPU times: user 1.7 ms, sys: 3.9 ms, total: 5.6 ms\nWall time: 28.7 ms\n\n\n\n\nCode\n%%time\n%%bash\ngrep -r \"test_sparse\" . &gt; /dev/null\n\n\nCPU times: user 1.69 ms, sys: 4.69 ms, total: 6.38 ms\nWall time: 365 ms\n\n\n\n\n\nThere are many ways to search a repository, particularly a Git Repo. We outlined some use cases with examples for a “Unix-like” file directory and also a Git Repo.\nIn most cases use:\n\ngit grep for searching strings in the repository in the current working tree or a specific commit\ngit log for searching across commits.\n\nThere are many flags and options for these commands - some combinations which produce the same output. Be sure to check the documentation for more information."
  },
  {
    "objectID": "posts/git-search/index.html#filename-in-unix-like-directory",
    "href": "posts/git-search/index.html#filename-in-unix-like-directory",
    "title": "Git Repository Search",
    "section": "",
    "text": "We can look for the string test_sparse in filenames using the shell command line tool find.\n\n\nCode\n%%bash\nfind . -name \"*test_sparse*\" -o -name \"*convert_coo_to_csr_indices*\"\n\n\n./test/utils/test_sparse.py\n\n\ngreat, so we have a file to look at. Let’s look at the file test_sparse.py. It seems to be unit tests related to sparsity, possibly testing utility functions for converting between sparse tensor representations."
  },
  {
    "objectID": "posts/git-search/index.html#string-in-unix-like-directory",
    "href": "posts/git-search/index.html#string-in-unix-like-directory",
    "title": "Git Repository Search",
    "section": "",
    "text": "String search is a bit more complicated. grep is an awesome tool for this.\n\n\nCode\n%%bash\ngrep -rn . -e \"test_sparse\" -e \"convert_coo_to_csr_indices\"\n\n\n./test/utils/test_cross_entropy.py:9:def test_sparse_cross_entropy_multiclass(with_edge_label_weight):\n./test/utils/test_cross_entropy.py:32:def test_sparse_cross_entropy_multilabel(with_edge_label_weight):\n./test/test_edge_index.py:102:def test_sparse_tensor(dtype, device):\n./test/test_edge_index.py:992:def test_sparse_narrow(device):\n./test/test_edge_index.py:1026:def test_sparse_resize(device):\n./torch_geometric/testing/asserts.py:24:    test_sparse_layouts: Optional[List[Union[str, torch.layout]]] = None,\n./torch_geometric/testing/asserts.py:49:        test_sparse_layouts (List[str or int], optional): The sparse layouts to\n./torch_geometric/testing/asserts.py:62:    if test_sparse_layouts is None:\n./torch_geometric/testing/asserts.py:63:        test_sparse_layouts = SPARSE_LAYOUTS\n./torch_geometric/testing/asserts.py:74:    if len(test_sparse_layouts) &gt; 0 and sparse_size is None:\n./torch_geometric/testing/asserts.py:75:        raise ValueError(f\"Got sparse layouts {test_sparse_layouts}, but no \"\n./torch_geometric/testing/asserts.py:93:    for layout in (test_sparse_layouts or []):\nBinary file ./.git/index matches\n\n\nMany locations matched to 3 files. It’s possible they aren’t all relevant for testing purpose. The .git/index is a binary file, which is used by git to store information about the repository, it’s not relevant for our task."
  },
  {
    "objectID": "posts/git-search/index.html#what-is-git",
    "href": "posts/git-search/index.html#what-is-git",
    "title": "Git Repository Search",
    "section": "",
    "text": "Git is a distributed version control system. It is a tool that tracks changes in files and directories. At user-defined snapshots in time, called commits, it records the changes made to the files and directories. As a consequence it is possible to search for changes in the repository across snapshots.\nAlong with grep and find, there are git specific tools for searching snapshots of the repo, commit messages and filtering by date and author, such as:\n\ngit ls-files\ngit log\ngit grep"
  },
  {
    "objectID": "posts/git-search/index.html#filename-in-git-repository",
    "href": "posts/git-search/index.html#filename-in-git-repository",
    "title": "Git Repository Search",
    "section": "",
    "text": "The working tree is what you see when you list the files in your project’s directory that are being tracked. It’s the version of your project that you’re currently working on. The git checkout command is used to update the working directory with a specific commit, matching the snapshot recorded in the commit. Untracked files are not affected by git checkout.\nThe git ls-files command lists the files in the working tree that are being tracked by git. The filenames can be searched for a string using the grep command.\n\n\nCode\n%%bash\ngit ls-files | grep \"test_sparse\"\n\n\ntest/utils/test_sparse.py\n\n\nThere’s not much else that can be done with git ls-files for searching filenames over snapshot commits. Some ways to search for filenames in the git repository across commits are:\nlog commit messages with commit where the filename was changed.\n\n\nCode\n%%bash\ngit log --all -- *test_sparse*\n\n\ncommit 62fa51e0000913e1b3023b817485d2b248322539\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sun Dec 24 11:56:08 2023 +0100\n\n    Accelerate concatenation of `torch.sparse` tensors (#8670)\n    \n    Fixes #8664\n\ncommit 1c89e751804d1eb2fb626dabc677198a1878c34d\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Wed Oct 4 09:59:36 2023 +0200\n\n    Skip TorchScript bug for PyTorch &lt; 1.12 (#8123)\n\ncommit 51c50c2f9d3372de34f4ac3617f396384a36558c\nAuthor: filipekstrm &lt;filip.ekstrom@hotmail.com&gt;\nDate:   Tue Oct 3 20:39:04 2023 +0200\n\n    Added `mask` argument to `dense_to_sparse` (#8117)\n    \n    Added optional argument mask to dense_to_sparse so that it can correctly\n    invert a call to to_dense_adj by returning the correct edge_index in\n    case there are graphs with different number of nodes (and hence, the\n    dense adjacency matrix contains some padding)\n    \n    ---------\n    \n    Co-authored-by: Filip Ekström Kelvinius &lt;filek51@lnx00195.ad.liu.se&gt;\n    Co-authored-by: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 2e0e7e35ce0e5a3dbe951ced527aa436e591bccd\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Thu Aug 24 08:28:32 2023 +0200\n\n    [Test] PyTorch 2.1 supports gradients on CSR matrices (#7924)\n\ncommit f80afef6fc27b8534261659ebdfd10efed8f4540\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Fri Jul 7 06:27:56 2023 +0700\n\n    Replace `torch.Tensor`calls (3/4) (#7697)\n\ncommit c0eb49f95198183e8cf15007bbd286c9ced7829a\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Thu Jun 29 06:10:56 2023 +0200\n\n    Test PyG on PyTorch &gt;= 1.11 (#7656)\n\ncommit 1e57a6c69cf15868bccc10b79479fdc4e7468211\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sun Jun 4 11:27:47 2023 +0200\n\n    Update the documentation of `GraphMaskExplainer` (#7504)\n\ncommit 0d35d8d9f0ad73b05a0bfd02e86caa7131f64177\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Thu May 11 15:21:49 2023 +0200\n\n    Test PyTorch `coalesce` bug on `torch.load` (#7351)\n\ncommit 93f9f590bb62a848e3f57b8fcaee5fd48c768436\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sun Mar 26 16:14:07 2023 +0200\n\n    Accelerated sparse tensor conversions (#7042)\n\ncommit c78c5b217f9fb652555512405fb561c33ae0b439\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sat Mar 25 16:59:59 2023 +0100\n\n    Test against nightly PyTorch releases (#7036)\n\ncommit c78f358bb664a15650c68113f7108be89aca98da\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Thu Mar 23 07:05:11 2023 +0100\n\n    Fix full test (#7007)\n\ncommit 153cb5cc6bd014b8fb453c5457a86d91f4b4483d\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sat Mar 18 23:16:05 2023 +0100\n\n    Add PyTorch 1.13 CI tests (#6960)\n\ncommit 9bda455cf0527dad6ee872175fa1f9a8b036fc80\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sat Mar 18 11:25:40 2023 +0100\n\n    Added `to_torch_csc_tensor` functionality (#6951)\n\ncommit d31a275b2d688bc5c8e4df17736ff3b6bbcbe965\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Fri Mar 17 10:31:41 2023 +0100\n\n    Fix PyTorch 2.0 warnings and add `torch.sparse_csr` utilities (#6939)\n\ncommit f865ab56cb4a8360d428978d67bd77ebdf01a15c\nAuthor: Jintang Li &lt;cnljt@outlook.com&gt;\nDate:   Fri Feb 17 22:40:05 2023 +0800\n\n    Added `utils.to_edge_index` to convert sparse tensors to edge indices and edge attributes (#6728)\n    \n    This PR implements `to_edge_index` to simplify the conversion between\n    PyTorch Sparse Tensor and edge indices. Perhaps there is a better name\n    than `to_edge_index`?\n    \n    ---------\n    \n    Co-authored-by: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 0c8f35fa9458bc4eb8ce43c3694ba35481623dde\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Mon Jan 30 16:22:46 2023 +0100\n\n    Bipartite `dense_to_sparse` support (#6546)\n    \n    Fixes #6507\n\ncommit 01037dbbedd48b4bff665f66cae9a5f23dad2e0e\nAuthor: Jintang Li &lt;cnljt@outlook.com&gt;\nDate:   Tue Nov 22 03:09:14 2022 +0800\n\n    Add `is_sparse` and `to_torch_coo_tensor` (#6003)\n    \n    This PR aims to\n    + Add `is_sparse` to check if the input is either `torch.sparse.Tensor`\n    or `torch_sparse.SparseTensor`\n    + Add `to_torch_coo_tensor` to convert `edge_index` and `edge_weight` to\n    `torch.sparse.Tensor` (in COO format)\n    + Move `is_torch_sparse_tensor` from `torch_sparse_tensor.py` to\n    `sparse.py`. I think this makes the structure in `torch_geometric.utils`\n    clearer.\n    + Clean up duplicated code in `message_passing.py`.\n    \n    Co-authored-by: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 0be0eca73920ca19d8fc99c963e6b49ff86c75f6\nAuthor: Akash Vartak &lt;cg82493@umbc.edu&gt;\nDate:   Thu Oct 13 14:30:35 2022 -0400\n\n    [Type Hints] `utils.dense_to_sparse` (#5683)\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 66b17806b1f4a2008e8be766064d9ef9a883ff03\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Fri Jan 21 11:34:22 2022 +0100\n\n    sort imports via isort\n\ncommit f29f27a26a97304995a02f92716c8e2d858f2051\nAuthor: pre-commit-ci[bot] &lt;66853113+pre-commit-ci[bot]@users.noreply.github.com&gt;\nDate:   Fri Jan 21 10:31:10 2022 +0000\n\n    [pre-commit.ci] auto fixes from pre-commit.com hooks\n    \n    for more information, see https://pre-commit.ci\n\ncommit ff0ff6190b66a9a40720073b4eefc8fc92d726ca\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Mon May 24 11:27:18 2021 +0200\n\n    dense_to_sparse with batched adjacencies\n\ncommit 550c6319261a56b87112837aae392927f401aed1\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sun Jun 16 18:08:31 2019 +0200\n\n    dense to sparse impl\n\ncommit d01ea9dab80be0a698fbdac341d3f777013ac527\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Mon Jun 10 19:30:51 2019 +0200\n\n    to dense adj implementation + removal of unused convert methods\n\ncommit 1c8ea5cdac21e8dd35b1a2c43c9f644900d65937\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Thu Mar 28 17:33:44 2019 +0100\n\n    test sparse\n\ncommit 2c01aa22c697b4326db05b7729f85613ebb8a8a0\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Fri Dec 28 17:04:26 2018 +0100\n\n    test utils"
  },
  {
    "objectID": "posts/git-search/index.html#string-in-git-repository",
    "href": "posts/git-search/index.html#string-in-git-repository",
    "title": "Git Repository Search",
    "section": "",
    "text": "If we want to search for a string across commits, the main power of git comes from the git log and git grep commands.\n\n\nCode\n%%bash\ngit log -S \"test_sparse\" --all\n\n\ncommit dba9659f6c4f29fd2be1f50b5ea12a29a926082f\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Thu Feb 29 14:04:19 2024 +0100\n\n    Fix `EdgeIndex.resize_` linting issues (#8993)\n\ncommit 123e38ef6715f75ed9198d256cc2cb984b431630\nAuthor: Poovaiah Palangappa &lt;98763718+pmpalang@users.noreply.github.com&gt;\nDate:   Sun Feb 11 03:32:44 2024 -0800\n\n    Example of a recommender system (#8546)\n    \n    Hi Everyone,\n    \n    I'm adding a recommender system example with the following salient\n    features\n    \n    1. Dataset MovieLens – a heterogenous use case\n    2. Demonstrates the use of edge based temporal sampling\n    3. Visualization\n      t-SNE based visualization (--visualize_emb)\n    4. Uses torch_geometric.nn.pool.MIPSKNNIndex for getting recommedations\n    5. Integration of the LinkPred metrics -- precision@k and ndcg@k\n    \n    Thanks,\n    Poovaiah\n    \n    ---------\n    \n    Co-authored-by: pre-commit-ci[bot] &lt;66853113+pre-commit-ci[bot]@users.noreply.github.com&gt;\n    Co-authored-by: Akihiro Nitta &lt;nitta@akihironitta.com&gt;\n    Co-authored-by: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 23bbc128d3df42fb11f187d0abd1c10eb569b3a0\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sat Dec 30 17:10:10 2023 +0100\n\n    Allow `EdgeIndex` creation from `torch.sparse` tensors (#8690)\n\ncommit ed9698d0bff2049413473c87f26774bedd190867\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Mon Dec 4 11:07:31 2023 +0100\n\n    Include `mypy` in `pre-commit` (#8520)\n    \n    Co-authored-by: Akihiro Nitta &lt;nitta@akihironitta.com&gt;\n\ncommit 801723efacee5ad2597256ba0d9934600512e626\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sat May 27 13:20:34 2023 +0200\n\n    Sparse `cross_entropy` implementation (#7447)\n\ncommit 1dadc070565ae45ce3a00f150b2fe50809c49959\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Mon Apr 10 02:50:51 2023 +0100\n\n    Helper assert function to test for module equivalence and invariance (#7144)\n\ncommit 7b4892781e2198ad99a8655da03133505619040a\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Sun Jun 28 11:23:29 2020 +0200\n\n    arma conv jittable\n\ncommit 93fab2e53dbd812c8cb08fc7a789ec0ace9e8768\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Mon Jun 8 15:26:35 2020 +0200\n\n    gcn conv\n\ncommit d01ea9dab80be0a698fbdac341d3f777013ac527\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Mon Jun 10 19:30:51 2019 +0200\n\n    to dense adj implementation + removal of unused convert methods\n\ncommit 2c01aa22c697b4326db05b7729f85613ebb8a8a0\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Fri Dec 28 17:04:26 2018 +0100\n\n    test utils\n\ncommit eb4260ce0154d2804f1e552db5be1d607a21d87d\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Fri Apr 13 15:58:53 2018 +0200\n\n    outsourced spline_conv, new voxel grid implementation\n\ncommit 544f4ad0e254e8fd7c96570756ac68ea15e6fe1d\nAuthor: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Mon Apr 9 09:05:36 2018 +0200\n\n    fixed tests\n\n\nto be specific to a branch, replace –all with the branch name (master in this case)\n\n\nCode\n%%bash\ngit log master -S \"convert_coo_to_csr_indices\" \n\n\n\n\nCode\n%%bash\ngit log master -S \"test_sparse\" --pretty=format:\"%h\" --name-only --diff-filter=A\n\n\n801723efa\ntest/utils/test_cross_entropy.py\n\n1dadc0705\ntorch_geometric/testing/asserts.py\n\n2c01aa22c\ntest/utils/test_sparse.py\n\n\nWith regular expression search use G (* glob is not needed as it’s implied with regular expressions).\n\n\nCode\n%%bash\ngit log -G \"convert_coo_to_csr_indices\" --pretty=format:\"%h\" --name-only\n\n\n\n\nCode\n%%bash\ngit log -G \"coo_to_csr\" --pretty=format:\"%h\" --name-only\n\n\n390942fc4\ntorch_geometric/data/edge_index.py\n\n699120e25\ntorch_geometric/data/edge_index.py\n\na6f0f4947\ntorch_geometric/data/edge_index.py\n\ncf786b735\ntorch_geometric/data/edge_index.py\n\nb825dc637\ntorch_geometric/data/edge_index.py\n\nb5ecfd9b4\ntorch_geometric/data/graph_store.py\ntorch_geometric/nn/conv/cugraph/base.py\ntorch_geometric/nn/conv/rgcn_conv.py\ntorch_geometric/nn/dense/linear.py\ntorch_geometric/sampler/utils.py\ntorch_geometric/transforms/gdc.py\ntorch_geometric/utils/sparse.py\n\na73043736\ntorch_geometric/data/graph_store.py\ntorch_geometric/nn/conv/rgcn_conv.py\ntorch_geometric/nn/dense/linear.py\n\nd29154558\ntorch_geometric/nn/conv/cugraph/base.py\n\n886352bd6\ntorch_geometric/transforms/gdc.py\n\nce2a84f4d\ntorch_geometric/sampler/utils.py\n\n0b3f8e98a\ntorch_geometric/sampler/utils.py\n\n\n\n\nCode\n%%bash\ngit log -G \"convert_coo\" --pretty=format:\"%h\" --name-only\n\n\n\n\nCode\n%%bash\ngit log -G \"csr_indices\" --pretty=format:\"%h\" --name-only\n\n\n\n\nCode\n%%bash\ngit log master -G\"test_sparse\" --pretty=format:\"%h\" --name-only\n\n\ndba9659f6\ntest/test_edge_index.py\n\n123e38ef6\ntest/test_edge_index.py\n\n23bbc128d\ntest/test_edge_index.py\n\ned9698d0b\ntorch_geometric/testing/asserts.py\n\n1725f1436\ntest/utils/test_cross_entropy.py\n\n801723efa\ntest/utils/test_cross_entropy.py\n\n1dadc0705\ntorch_geometric/testing/asserts.py\n\n7b4892781\ntest/nn/conv/test_gcn_conv.py\n\n72e8ef33d\ntest/nn/conv/test_gcn_conv.py\n\n93fab2e53\ntest/nn/conv/test_gcn_conv.py\n\nd01ea9dab\ntest/utils/test_sparse.py\n\n2c01aa22c\ntest/utils/test_sparse.py\n\neb4260ce0\ntorch_geometric/nn/functional/pool/voxel_pool_test.py\n\n544f4ad0e\ntorch_geometric/nn/functional/pool/voxel_pool_test.py"
  },
  {
    "objectID": "posts/git-search/index.html#search-for-author-date-and-string-in-git-commit-messages",
    "href": "posts/git-search/index.html#search-for-author-date-and-string-in-git-commit-messages",
    "title": "Git Repository Search",
    "section": "",
    "text": "Code\n%%bash\ngit log --author=\"ravkalia\"\n\n\ncommit f0e4c829662df9eb67fd5c0abda002c9b7cd0afb\nAuthor: Ravi Kalia &lt;ravkalia@gmail.com&gt;\nDate:   Sun Mar 24 08:05:12 2024 -0500\n\n    Replace `withCUDA` decorator: `withDevice` (#9082)\n    \n    Replace `withCUDA` for a `withDevice` decorator.\n    \n    Change variable name from devices to processors to reduce confusion\n    against pytorch api (backends/devices) and reflect the hardware choices.\n    \n    Note that at this time:\n    \n    ## Hardware\n    3 repertoires of hardware can be used to run pyTorch code:\n    \n    * CPU only\n    * CPU and GPU\n    * Unified Memory Single Chip\n    \n    ## Backend Software\n    The backend is the software framework used to process tensors by\n    pytorch. There are several. For example, the following are the backends\n    available today:\n    \n    \n    ```python\n    torch.backends.cpu\n    torch.backends.cuda\n    torch.backends.cudnn\n    torch.backends.mha\n    torch.backends.mps\n    torch.backends.mkl\n    torch.backends.mkldnn\n    torch.backends.nnpack\n    torch.backends.openmp\n    torch.backends.opt_einsum\n    torch.backends.xeon\n    ```\n    `mps` is the only backend which works with the unified memory single\n    chip - currently Apple's M series.\n    \n    Hardware is determined by `torch.cuda.is_available()` and\n    `torch.backends.mps.is_available()` which confuses when looking up\n    `backends` and hardaware types/architectures/vendors. Introducing\n    `processor/processors` variable name reduces cognitive load, while\n    allowing future developments from vendors/frameworks.\n    \n    \n    \n    ~~Modify `withCUDA` decorator to be single purpose. Downstream\n    comprehensive distributed device and backend testing can be developed\n    later.~~\n    \n    ~~Redundant code was removed, this doesn't affect other components.~~\n    \n    ---------\n    \n    Co-authored-by: pre-commit-ci[bot] &lt;66853113+pre-commit-ci[bot]@users.noreply.github.com&gt;\n    Co-authored-by: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 1675b019c7182dbdc4970561f0dbff6dec3ee299\nAuthor: Ravi Kalia &lt;ravkalia@gmail.com&gt;\nDate:   Fri Mar 8 14:44:56 2024 -0500\n\n    feat(pyproject.toml): update & sort (#9024)\n    \n    - Add 'ipython' and 'matplotlib-inline' to dev dependencies\n    - Reorder 'torch_geometric[test]' in dev dependencies list\n    \n    Introduced for developer install to pass tests on apple silicon\n    \n    \n    Notes here:\n    \n    \n    https://project-delphi.github.io/ml-blog/posts/developing-pytorch-geometric-on-m1/\n\ncommit 25b2f208e671eeec285bfafa2e246ea0a234b312\nAuthor: Ravi Kalia &lt;ravkalia@gmail.com&gt;\nDate:   Wed Feb 21 11:11:33 2024 -0500\n\n    docs: fix broken links to source of graph classification datasets (#8946)\n    \n    **Update Broken Dataset Links in Documentation**\n    \n    This PR addresses broken links in the documentation that pointed to the\n    common benchmark datasets. The links were updated to point to the\n    correct URL.\n    \n    Changes were made in the following files:\n    \n    1. `benchmark/kernel/README.md`\n    2. `docs/source/get_started/introduction.rst`\n    \n    The specific changes are as follows:\n    \n    In `benchmark/kernel/README.md`:\n    \n    ```diff\n    - Evaluation script for various methods on [common benchmark datasets](http://graphkernels.cs.tu-dortmund.de) via 10-fold cross validation, where a training fold is randomly sampled to serve as a validation set.\n    + Evaluation script for various methods on [common benchmark datasets](https://chrsmrrs.github.io/datasets/) via 10-fold cross validation, where a training fold is randomly sampled to serve as a validation set.Update Broken Dataset Links in Documentation\n    ```\n    \n    In `docs/source/get_started/introduction.rst`:\n    \n    ```diff\n    - :pyg:`PyG` contains a large number of common benchmark datasets, *e.g.*, all Planetoid datasets (Cora, Citeseer, Pubmed), all graph classification datasets from `http://graphkernels.cs.tu-dortmund.de &lt;http://graphkernels.cs.tu-dortmund.de/&gt;`_ and their `cleaned versions &lt;https://github.com/nd7141/graph_datasets&gt;`_, the QM7 and QM9 dataset, and a handful of 3D mesh/point cloud datasets like FAUST, ModelNet10/40 and ShapeNet.\n    + :pyg:`PyG` contains a large number of common benchmark datasets, *e.g.*, all Planetoid datasets (Cora, Citeseer, Pubmed), all graph classification datasets from `TUDatasets https://chrsmrrs.github.io/datasets/`_ and their `cleaned versions &lt;https://github.com/nd7141/graph_datasets&gt;`_, the QM7 and QM9 dataset, and a handful of 3D mesh/point cloud datasets like FAUST, ModelNet10/40 and ShapeNet.\n    ```\n    \n    Please review these changes and merge the PR if everything is in order\n    or let me know if there are any issues or further changes needed.\n    \n    ---------\n    \n    Co-authored-by: Ravi Kalia &lt;rkalia.consultant@kriyatx.com&gt;\n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n\n\n\n\nCode\n%%bash\ngit log --author=\"ravkalia\" --since=\"2022-01-01\" --until=\"2024-02-31\"\n\n\ncommit 25b2f208e671eeec285bfafa2e246ea0a234b312\nAuthor: Ravi Kalia &lt;ravkalia@gmail.com&gt;\nDate:   Wed Feb 21 11:11:33 2024 -0500\n\n    docs: fix broken links to source of graph classification datasets (#8946)\n    \n    **Update Broken Dataset Links in Documentation**\n    \n    This PR addresses broken links in the documentation that pointed to the\n    common benchmark datasets. The links were updated to point to the\n    correct URL.\n    \n    Changes were made in the following files:\n    \n    1. `benchmark/kernel/README.md`\n    2. `docs/source/get_started/introduction.rst`\n    \n    The specific changes are as follows:\n    \n    In `benchmark/kernel/README.md`:\n    \n    ```diff\n    - Evaluation script for various methods on [common benchmark datasets](http://graphkernels.cs.tu-dortmund.de) via 10-fold cross validation, where a training fold is randomly sampled to serve as a validation set.\n    + Evaluation script for various methods on [common benchmark datasets](https://chrsmrrs.github.io/datasets/) via 10-fold cross validation, where a training fold is randomly sampled to serve as a validation set.Update Broken Dataset Links in Documentation\n    ```\n    \n    In `docs/source/get_started/introduction.rst`:\n    \n    ```diff\n    - :pyg:`PyG` contains a large number of common benchmark datasets, *e.g.*, all Planetoid datasets (Cora, Citeseer, Pubmed), all graph classification datasets from `http://graphkernels.cs.tu-dortmund.de &lt;http://graphkernels.cs.tu-dortmund.de/&gt;`_ and their `cleaned versions &lt;https://github.com/nd7141/graph_datasets&gt;`_, the QM7 and QM9 dataset, and a handful of 3D mesh/point cloud datasets like FAUST, ModelNet10/40 and ShapeNet.\n    + :pyg:`PyG` contains a large number of common benchmark datasets, *e.g.*, all Planetoid datasets (Cora, Citeseer, Pubmed), all graph classification datasets from `TUDatasets https://chrsmrrs.github.io/datasets/`_ and their `cleaned versions &lt;https://github.com/nd7141/graph_datasets&gt;`_, the QM7 and QM9 dataset, and a handful of 3D mesh/point cloud datasets like FAUST, ModelNet10/40 and ShapeNet.\n    ```\n    \n    Please review these changes and merge the PR if everything is in order\n    or let me know if there are any issues or further changes needed.\n    \n    ---------\n    \n    Co-authored-by: Ravi Kalia &lt;rkalia.consultant@kriyatx.com&gt;\n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n\n\n\n\nCode\n%%bash\ngit log --grep=\"docs\"  --since=\"2022-01-01\" --until=\"2022-02-31\"\n\n\ncommit 24a185e7268f70ee549c7a424b9426b9a18b5706\nAuthor: Ramona Bendias &lt;ramona.bendias@gmail.com&gt;\nDate:   Mon Feb 21 13:03:52 2022 +0000\n\n    Add general `Explainer` Class (#4090)\n    \n    * Add base Explainer\n    \n    * Update Explainer\n    \n    * Fix test\n    \n    * Clean code\n    \n    * Update test/nn/models/test_explainer.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Add hints and add get_num_hops\n    \n    * Fix\n    \n    * Change docstring\n    \n    * Update torch_geometric/utils/subgraph.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Fix tests\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n    * some typos\n    \n    * typo\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 6002170a513107d66d3bc37cf39dc924f380f694\nAuthor: RBendias &lt;ramona.bendias@gmail.com&gt;\nDate:   Wed Feb 9 12:07:02 2022 +0100\n\n    Make models compatible to Captum (#3990)\n    \n    * Add PNA model\n    \n    * Add to_captum for edge_masks\n    \n    * Add node explainability with captum\n    \n    * Update\n    \n    * Add CaptumModel documentation\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Update torch_geometric/nn/models/explainer.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Add Captum dependency\n    \n    * Update captum_explainability.py\n    \n    * Update docs\n    \n    * Update docs\n    \n    * update doc\n    \n    Co-authored-by: @ramonabendias &lt;ramona@workist.com&gt;\n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 14d588d4ceb2d9c0bd71efa4f1051fb041154f2c\nAuthor: Konstantinos Kogkalidis &lt;konstantinos@riseup.net&gt;\nDate:   Mon Feb 7 09:48:21 2022 +0100\n\n    Update attention.py (#4009)\n    \n    fix shape of output tensor in docstring for GlobalAttention\n\ncommit 50ff5e6d60dacad8482fe71351bfe48882e95f83\nAuthor: RBendias &lt;ramona.bendias@gmail.com&gt;\nDate:   Wed Feb 2 20:58:55 2022 +0100\n\n    Add `full` extras to install command in contribution docs (#3991)\n    \n    * Add PNA model\n    \n    * Add `full` extras to contribution install command\n\ncommit 1e24b3a16fa2e92f9ad558ec2544c29132f7129b\nAuthor: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\nDate:   Wed Jan 26 16:26:24 2022 +0100\n\n    Refactor: `MLP` initialization (#3957)\n    \n    * update MLP\n    \n    * docstring\n    \n    * fix test\n    \n    * typo\n    \n    * typo\n\ncommit 3e4891be64a54b01cd8aa00852db830bc5459784\nAuthor: Arun &lt;arun.palaniappan1999@gmail.com&gt;\nDate:   Thu Jan 20 13:09:50 2022 +0530\n\n    Doc improvements to set2set layers (#3889)\n    \n    * added type hints\n    \n    * doc updates\n    \n    * set2set parameter to accept None arg\n    \n    * added shapes of input and output [docs]\n    \n    * minor fix\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * small update\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit fac848c25f9c0181e11869a6bcd85ed242d982c5\nAuthor: Otávio Calaça &lt;otaviocx@gmail.com&gt;\nDate:   Tue Jan 18 05:23:21 2022 -0300\n\n    Let `TemporalData` inherit from `BaseData` and add docs (#3867)\n    \n    * Refactor TemporalData class to inherit from BaseData\n    \n    * Fixes to get TemporalData working\n    \n    * Small fixes in __delitem__ of TemporalData\n    \n    * Add batch, __cat_dim__ and __inc__ to TemporalData\n    \n    * Add Docs to TemporalData\n    \n    * [pre-commit.ci] auto fixes from pre-commit.com hooks\n    \n    for more information, see https://pre-commit.ci\n    \n    * Lint fixes\n    \n    * Add removed method TemporalData.seq_batches\n    \n    * Update torch_geometric/data/temporal.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Update torch_geometric/data/temporal.py\n    \n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n    \n    * Changes requested in review\n    \n    * Removing trailing whitespace\n    \n    * fix doc + some inheritance issues\n    \n    * fix iter\n    \n    Co-authored-by: pre-commit-ci[bot] &lt;66853113+pre-commit-ci[bot]@users.noreply.github.com&gt;\n    Co-authored-by: Matthias Fey &lt;matthias.fey@tu-dortmund.de&gt;\n\ncommit 0c29b0d5b5d3b23e981601367a47222bddd9c34b\nAuthor: Stefano Roncelli &lt;45285915+saiden89@users.noreply.github.com&gt;\nDate:   Sat Jan 8 08:46:49 2022 +0100\n\n    Updated docstring for shape info - part 2 (#3739)\n    \n    * update docstrings for shape\n    \n    * update docstrings for shape\n    fix out channels description\n    \n    * update docstrings for shape\n    \n    * fix linting\n    \n    * update\n    \n    * update\n    \n    * update\n    \n    * update\n    \n    * fix tests\n    \n    * typo\n    \n    * fix typos\n    \n    * fix typo\n    \n    * update docs\n    \n    * update docstring for shape\n    fix typos for cheb_conv\n    \n    * rollback gmm_conv\n    \n    Co-authored-by: rusty1s &lt;matthias.fey@tu-dortmund.de&gt;\n\n\n\n\nCode\n%%bash\ngit log --oneline --grep=\"docs\"  --since=\"2022-01-01\" --until=\"2022-02-31\"\n\n\n24a185e72 Add general `Explainer` Class (#4090)\n6002170a5 Make models compatible to Captum (#3990)\n14d588d4c Update attention.py (#4009)\n50ff5e6d6 Add `full` extras to install command in contribution docs (#3991)\n1e24b3a16 Refactor: `MLP` initialization (#3957)\n3e4891be6 Doc improvements to set2set layers (#3889)\nfac848c25 Let `TemporalData` inherit from `BaseData` and add docs (#3867)\n0c29b0d5b Updated docstring for shape info - part 2 (#3739)"
  },
  {
    "objectID": "posts/git-search/index.html#git-grep-vs-grep",
    "href": "posts/git-search/index.html#git-grep-vs-grep",
    "title": "Git Repository Search",
    "section": "",
    "text": "The main differences between git grep and grep are:\ngit grep only searches through your tracked files, while grep can search through any files. git grep is aware of your Git repository structure and can search through old commits, branches, etc., while grep only searches through the current state of files.\ngit grep is faster than grep when searching through a Git repository because it takes advantage of Git’s index data structure.\n\n\nCode\n%%time\n%%bash\ngit grep \"test_sparse\" &gt; /dev/null\n\n\nCPU times: user 1.7 ms, sys: 3.9 ms, total: 5.6 ms\nWall time: 28.7 ms\n\n\n\n\nCode\n%%time\n%%bash\ngrep -r \"test_sparse\" . &gt; /dev/null\n\n\nCPU times: user 1.69 ms, sys: 4.69 ms, total: 6.38 ms\nWall time: 365 ms"
  },
  {
    "objectID": "posts/git-search/index.html#takeaways",
    "href": "posts/git-search/index.html#takeaways",
    "title": "Git Repository Search",
    "section": "",
    "text": "There are many ways to search a repository, particularly a Git Repo. We outlined some use cases with examples for a “Unix-like” file directory and also a Git Repo.\nIn most cases use:\n\ngit grep for searching strings in the repository in the current working tree or a specific commit\ngit log for searching across commits.\n\nThere are many flags and options for these commands - some combinations which produce the same output. Be sure to check the documentation for more information."
  }
]
