---
title: "Hugging Face Datasets"
description: "A practical guide to loading, creating, and using datasets with the Hugging Face `datasets` library."
author: "Ravi Kalia"
date: "2025-04-04"
categories: [Machine Learning, NLP, PyTorch]
format:
    html:
        code-fold: true
---

## Introduction

The Hugging Face `datasets` library is a powerful tool for downloading, processing, and managing large-scale datasets for machine learning, particularly in NLP but also for vision and tabular data. It provides seamless integration with PyTorch and TensorFlow.

## Key Features

- **Easy Access to Large Datasets**: Preloaded datasets such as `IMDB`, `CIFAR-10`, and `SQuAD`.
- **Streaming for Large Datasets**: Memory-efficient loading of massive datasets.
- **Dataset Preprocessing and Transformations**: Apply tokenization, filtering, and mapping functions.
- **Multiple File Format Support**: Works with CSV, JSON, and Parquet.
- **Dataset Splitting**: Simple train-test splitting.
- **Efficient Storage**: Uses Apache Arrow for fast data processing.
- **Caching**: Avoids redundant downloads and computations.

## Installation

```{bash}
pip install datasets
```

## Loading and Exploring a Dataset

```{python}
from datasets import load_dataset

# Load the IMDB sentiment analysis dataset
dataset = load_dataset("imdb")
print(dataset)
```

## Creating a Dataset from Scratch

```{python}
from datasets import Dataset

data = {
    "text": ["I love this!", "This is bad!", "Absolutely amazing!", "Not good at all!"],
    "label": [1, 0, 1, 0],
}

dataset = Dataset.from_dict(data)
print(dataset)
```

## Loading a Dataset from a CSV File

```{python}
from datasets import load_dataset

dataset = load_dataset("csv", data_files="data.csv")
print(dataset)
```

## Splitting a Dataset

```{python}
train_test_split = dataset["train"].train_test_split(test_size=0.1)
print(train_test_split)
```



## Tokenization and Collation with Transformers

```{python}
from torch.utils.data import DataLoader
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

# Dummy data
raw_data = [{"text": "hello", "label": 0}, {"text": "world", "label": 1}]


def collate_fn(batch):
    return tokenizer(
        [ex["text"] for ex in batch], padding=True, truncation=True, return_tensors="pt"
    )


loader = DataLoader(raw_data, batch_size=2, collate_fn=collate_fn)

for batch in loader:
    print(batch)
    break
```

## Streaming Large Datasets

```{python}
streaming_dataset = load_dataset("imdb", split="train", streaming=True)

for sample in streaming_dataset:
    print(sample)
    break  # Stop after one example
```

## Conclusion

The `datasets` library simplifies handling large-scale datasets with powerful built-in functionalities for efficient data loading, transformation, and integration into ML pipelines. It is particularly useful for NLP, vision, and tabular machine learning workflows.

---

Let me know if you need more examples or modifications! ðŸš€
