---
title: "Evaluating NLP Models with the Hugging Face `evaluate` Library"
author: "Ravi Kalia"
date: "2025-03-12"
format:
  html:
    toc: true
    toc-depth: 2
    code-fold: true
    code-tools: true
    code-copy: true
categories: [NLP, Hugging Face, Evaluation]
---

The Hugging Face `evaluate` library provides a simple and flexible interface for computing metrics on machine learning predictions. It's especially well-suited for NLP tasks like classification, summarization, and translation, where standard metrics are critical for reliable benchmarking.

## Installation

```bash
pip install evaluate
```

To explore available metrics

```{python}
import evaluate
evaluate.list_evaluation_modules()
```

Quick Example


```{python}
from evaluate import load

accuracy = load("accuracy")
accuracy.compute(predictions=[0, 1, 1, 0], references=[0, 1, 0, 0])
# {'accuracy': 0.75}
```

### NLP Tasks and Typical Metrics

| Task                 | Metric Examples                        |
|----------------------|--------------------------------------|
| Text classification  | accuracy, f1, recall, precision    |
| Sequence labeling    | seqeval                             |
| Translation          | bleu, sacrebleu, chrf              |
| Summarization        | rouge, bertscore, bleurt           |
| Text generation      | rouge, bertscore, meteor, bleurt   |
| Speech recognition   | wer, cer                           |

These scores need some dependencies:

```bash
pip install nltk rouge_score bert_score
```

### Text Generation Metrics

#### ROUGE

Measures token overlap with a reference:
*	rouge-1: unigram overlap
*	rouge-2: bigram overlap

```{python}
rouge = evaluate.load("rouge")
rouge.compute(predictions=["The cat sat."], references=["A cat was sitting."])
```


#### BLEU

Precision-based n-gram overlap metric, often used in translation:
```{python}
bleu = evaluate.load("bleu")
bleu.compute(
    predictions=["The cat is on the mat."], references=[["The cat sits on the mat."]]
)
```


#### BERTScore

Uses a pretrained transformer model to measure semantic similarity in embedding space. There's a small bug here, I don't have time to fix it - however similar code should work.

```{python}
# bertscore = evaluate.load("bertscore")
# bertscore.compute(predictions=["The cat sat."],
#                   references=["A cat was sitting."],
#                   lang="en")
```


#### With Hugging Face Pipelines

```{python}
from transformers import pipeline
summarizer = pipeline("summarization")

summary = summarizer("This is a long text to summarize.")[0]["summary_text"]

rouge.compute(predictions=[summary], references=["Reference summary here."])
```


#### Custom Metrics

Define your own scoring function:

```{python}
import evaluate


# Define the custom metric function
def my_metric_fn(predictions, references):
    score = sum(p == r for p, r in zip(predictions, references)) / len(references)
    return {"custom_accuracy": score}


# Use the evaluate's simple interface
result = my_metric_fn(predictions=[1, 0, 1], references=[1, 1, 1])
print(result)
```


The evaluate library is model-agnostic and pairs well with the rest of the Hugging Face ecosystem. Itâ€™s lightweight enough for quick experiments but supports rich comparisons for production or publication.