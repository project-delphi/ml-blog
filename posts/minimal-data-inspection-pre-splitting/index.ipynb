{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Minimal Data Inspection Before Splitting The Dataset\"\n",
    "author: \"Ravi Kalia\"\n",
    "date: \"2024-09-21\"\n",
    "categories: [machine-learning, development, statistics, splitting, python, inspection]\n",
    "image: \"./magnifying_glass.jpg\"\n",
    "format:\n",
    "    html:\n",
    "        code-copy: true\n",
    "        code-fold: true \n",
    "        toc: true\n",
    "        toc-depth: 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "  <em>Made with <span style=\"color: red;\">❤️</span> and <a href=\"https://github.com/features/copilot\" target=\"_blank\">GitHub Copilot</a></em>\n",
    "  <br>\n",
    "  <img src=\"https://github.githubassets.com/images/icons/emoji/octocat.png\" alt=\"GitHub Copilot Logo\" width=\"50\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Data Inspection Before Splitting The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Magnifying](./magnifying_glass.jpg)\n",
    "Photo by <a href=\"https://unsplash.com/@olloweb?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Agence Olloweb</a> on <a href=\"https://unsplash.com/photos/magnifying-glass-near-gray-laptop-computer-d9ILr-dbEdg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling data in machine learning can feel as important as wielding a lightsaber in the Star Wars universe. In this guide, we'll explore key concepts around data splitting and discuss potential risks of early preprocessing. Think of it as a journey toward mastering the art of data preparation, where each decision shapes your path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preparing the Dataset\n",
    "\n",
    "Let's begin our adventure with the Star Wars dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RespondentID Have you seen any of the 6 films in the Star Wars franchise?  \\\n",
      "0           NaN                                           Response             \n",
      "1  3.292880e+09                                                Yes             \n",
      "2  3.292880e+09                                                 No             \n",
      "3  3.292765e+09                                                Yes             \n",
      "4  3.292763e+09                                                Yes             \n",
      "\n",
      "  Do you consider yourself to be a fan of the Star Wars film franchise?  \\\n",
      "0                                           Response                      \n",
      "1                                                Yes                      \n",
      "2                                                NaN                      \n",
      "3                                                 No                      \n",
      "4                                                Yes                      \n",
      "\n",
      "  Which of the following Star Wars films have you seen? Please select all that apply.  \\\n",
      "0           Star Wars: Episode I  The Phantom Menace                                    \n",
      "1           Star Wars: Episode I  The Phantom Menace                                    \n",
      "2                                                NaN                                    \n",
      "3           Star Wars: Episode I  The Phantom Menace                                    \n",
      "4           Star Wars: Episode I  The Phantom Menace                                    \n",
      "\n",
      "                                    Unnamed: 4  \\\n",
      "0  Star Wars: Episode II  Attack of the Clones   \n",
      "1  Star Wars: Episode II  Attack of the Clones   \n",
      "2                                          NaN   \n",
      "3  Star Wars: Episode II  Attack of the Clones   \n",
      "4  Star Wars: Episode II  Attack of the Clones   \n",
      "\n",
      "                                    Unnamed: 5  \\\n",
      "0  Star Wars: Episode III  Revenge of the Sith   \n",
      "1  Star Wars: Episode III  Revenge of the Sith   \n",
      "2                                          NaN   \n",
      "3  Star Wars: Episode III  Revenge of the Sith   \n",
      "4  Star Wars: Episode III  Revenge of the Sith   \n",
      "\n",
      "                          Unnamed: 6  \\\n",
      "0  Star Wars: Episode IV  A New Hope   \n",
      "1  Star Wars: Episode IV  A New Hope   \n",
      "2                                NaN   \n",
      "3                                NaN   \n",
      "4  Star Wars: Episode IV  A New Hope   \n",
      "\n",
      "                                     Unnamed: 7  \\\n",
      "0  Star Wars: Episode V The Empire Strikes Back   \n",
      "1  Star Wars: Episode V The Empire Strikes Back   \n",
      "2                                           NaN   \n",
      "3                                           NaN   \n",
      "4  Star Wars: Episode V The Empire Strikes Back   \n",
      "\n",
      "                                 Unnamed: 8  \\\n",
      "0  Star Wars: Episode VI Return of the Jedi   \n",
      "1  Star Wars: Episode VI Return of the Jedi   \n",
      "2                                       NaN   \n",
      "3                                       NaN   \n",
      "4  Star Wars: Episode VI Return of the Jedi   \n",
      "\n",
      "  Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.  \\\n",
      "0           Star Wars: Episode I  The Phantom Menace                                                                                              \n",
      "1                                                  3                                                                                              \n",
      "2                                                NaN                                                                                              \n",
      "3                                                  1                                                                                              \n",
      "4                                                  5                                                                                              \n",
      "\n",
      "   ...       Unnamed: 28       Which character shot first?  \\\n",
      "0  ...              Yoda                          Response   \n",
      "1  ...    Very favorably  I don't understand this question   \n",
      "2  ...               NaN                               NaN   \n",
      "3  ...  Unfamiliar (N/A)  I don't understand this question   \n",
      "4  ...    Very favorably  I don't understand this question   \n",
      "\n",
      "  Are you familiar with the Expanded Universe?  \\\n",
      "0                                     Response   \n",
      "1                                          Yes   \n",
      "2                                          NaN   \n",
      "3                                           No   \n",
      "4                                           No   \n",
      "\n",
      "  Do you consider yourself to be a fan of the Expanded Universe?æ  \\\n",
      "0                                           Response                 \n",
      "1                                                 No                 \n",
      "2                                                NaN                 \n",
      "3                                                NaN                 \n",
      "4                                                NaN                 \n",
      "\n",
      "  Do you consider yourself to be a fan of the Star Trek franchise?    Gender  \\\n",
      "0                                           Response                Response   \n",
      "1                                                 No                    Male   \n",
      "2                                                Yes                    Male   \n",
      "3                                                 No                    Male   \n",
      "4                                                Yes                    Male   \n",
      "\n",
      "        Age     Household Income                         Education  \\\n",
      "0  Response             Response                          Response   \n",
      "1     18-29                  NaN                High school degree   \n",
      "2     18-29         $0 - $24,999                   Bachelor degree   \n",
      "3     18-29         $0 - $24,999                High school degree   \n",
      "4     18-29  $100,000 - $149,999  Some college or Associate degree   \n",
      "\n",
      "  Location (Census Region)  \n",
      "0                 Response  \n",
      "1           South Atlantic  \n",
      "2       West South Central  \n",
      "3       West North Central  \n",
      "4       West North Central  \n",
      "\n",
      "[5 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Star Wars dataset\n",
    "url = \"https://raw.githubusercontent.com/fivethirtyeight/data/master/star-wars-survey/StarWars.csv\"\n",
    "df = pd.read_csv(url, encoding=\"latin1\")\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.replace(\"Which of the following Star Wars films have you seen? Please select all that apply.\", \"seen_\")\n",
    "df.columns = df.columns.str.replace(\"Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.\", \"rank_\")\n",
    "df.columns = df.columns.str.replace(\"Do you consider yourself to be a fan of the Star Wars film franchise?\", \"is_fan\")\n",
    "\n",
    "# Select a subset of columns for our analysis\n",
    "columns_to_use = [\n",
    "    'seen_1', 'seen_2', 'seen_3', 'seen_4', 'seen_5', 'seen_6',\n",
    "    'rank_1', 'rank_2', 'rank_3', 'rank_4', 'rank_5', 'rank_6',\n",
    "    'is_fan', 'Gender', 'Age', 'Household Income', 'Education'\n",
    "]\n",
    "\n",
    "df = df[columns_to_use]\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Features: {df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Importance of Proper Data Splitting\n",
    "\n",
    "Splitting your data is crucial for developing robust machine learning models. The main purposes of these splits are:\n",
    "\n",
    "1. Training set: Used to train the model\n",
    "2. Validation set: Used for hyperparameter tuning and model selection\n",
    "3. Test set: Used to evaluate the final model's performance on unseen data\n",
    "\n",
    "Proper data splitting helps prevent overfitting and provides a realistic estimate of how your model will perform on new, unseen data.\n",
    "\n",
    "## The Dangers of Premature Preprocessing\n",
    "\n",
    "While data preprocessing is essential, performing certain steps before splitting your data can lead to data leakage and biased models. Let's explore some common preprocessing steps and their associated risks:\n",
    "\n",
    "### 1. Handling Outliers\n",
    "\n",
    "Removing or modifying outliers based on the entire dataset before splitting can lead to data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_outlier_effect(df, column):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(121)\n",
    "    df[column].hist(bins=30)\n",
    "    plt.title(f\"Original {column} Distribution\")\n",
    "    \n",
    "    # Incorrect way: Remove outliers before splitting\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df_filtered = df[(df[column] >= Q1 - 1.5*IQR) & (df[column] <= Q3 + 1.5*IQR)]\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    df_filtered[column].hist(bins=30)\n",
    "    plt.title(f\"{column} Distribution After Removing Outliers\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_outlier_effect(df, 'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing outliers from the entire dataset before splitting would result in a test set that no longer represents the true data distribution. This can lead to overly optimistic performance estimates and poor generalization to new data.\n",
    "\n",
    "### 2. Bucketing Variables\n",
    "\n",
    "Creating categorical variables from continuous ones (bucketing) based on the entire dataset can also cause data leakage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bucketing_effect(df, column, n_bins=5):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(121)\n",
    "    df[column].hist(bins=30)\n",
    "    plt.title(f\"Original {column} Distribution\")\n",
    "    \n",
    "    # Incorrect way: Create bins based on the entire dataset\n",
    "    df['bucketed'] = pd.qcut(df[column], q=n_bins)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    df['bucketed'].value_counts().sort_index().plot(kind='bar')\n",
    "    plt.title(f\"Bucketed {column} Distribution\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_bucketing_effect(df, 'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucketing variables using information from the entire dataset can introduce bias, as the bin boundaries are influenced by the test set.\n",
    "\n",
    "### 3. Handling Missing Data\n",
    "\n",
    "Imputing missing values using information from the entire dataset can lead to data leakage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_missingness_effect(df, column):\n",
    "    # Introduce some missing values\n",
    "    df_missing = df.copy()\n",
    "    df_missing.loc[df_missing.sample(frac=0.2).index, column] = np.nan\n",
    "    \n",
    "    print(f\"Missing values in {column}: {df_missing[column].isnull().sum()}\")\n",
    "    \n",
    "    # Incorrect way: Impute missing values based on the entire dataset\n",
    "    df_imputed = df_missing.copy()\n",
    "    df_imputed[column].fillna(df_imputed[column].mean(), inplace=True)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(121)\n",
    "    df[column].hist(bins=30)\n",
    "    plt.title(f\"Original {column} Distribution\")\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    df_imputed[column].hist(bins=30)\n",
    "    plt.title(f\"{column} Distribution After Imputation\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_missingness_effect(df, 'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing missing values using statistics from the entire dataset allows information from the test set to influence the training data, potentially leading to overfitting and unreliable performance estimates.\n",
    "\n",
    "### 4. Dimensionality Reduction\n",
    "\n",
    "Applying dimensionality reduction techniques like PCA to the entire dataset before splitting can also cause data leakage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def show_pca_effect(df, n_components=2):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Incorrect way: Apply PCA to the entire dataset\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result = pca.fit_transform(df[numeric_cols])\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)\n",
    "    plt.title(\"PCA Result (Incorrectly Applied to Entire Dataset)\")\n",
    "    plt.xlabel(\"First Principal Component\")\n",
    "    plt.ylabel(\"Second Principal Component\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "show_pca_effect(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying PCA or other dimensionality reduction techniques to the entire dataset allows information from the test set to influence the feature space of the training data.\n",
    "\n",
    "## Impact on Model Performance\n",
    "\n",
    "When preprocessing steps are applied to the entire dataset before splitting, several problems can arise:\n",
    "\n",
    "1. Overfitting: The model may implicitly learn patterns from the test set, leading to overly optimistic performance estimates.\n",
    "2. Poor generalization: The model may not perform well on truly unseen data because it has been trained on a dataset that doesn't represent the real-world data distribution.\n",
    "3. Biased feature importance: The importance of features may be distorted due to information leakage from the test set.\n",
    "4. Unreliable model selection: When comparing different models, the selection process may be biased towards models that overfit the leaked information.\n",
    "\n",
    "## The Exception: Minimal Target Variable Processing\n",
    "\n",
    "While most preprocessing should be done after splitting the data, some minimal processing of the target variable can be acceptable and even beneficial when done carefully:\n",
    "\n",
    "### 1. Calculating the Empirical Distribution of the Target\n",
    "\n",
    "Understanding the distribution of your target variable can inform your sampling strategy and help identify potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_distribution(df, target_column):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df[target_column].value_counts(normalize=True).plot(kind='bar')\n",
    "    plt.title(f\"Distribution of {target_column}\")\n",
    "    plt.ylabel(\"Proportion\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.show()\n",
    "\n",
    "plot_target_distribution(df, 'is_fan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Handling Target Variable Outliers\n",
    "\n",
    "Addressing outliers in your target variable before splitting can sometimes be beneficial. Strategies include:\n",
    "\n",
    "1. Grouping levels: If certain classes have very few samples, consider grouping them.\n",
    "2. Winsorization: Cap extreme values at a specified percentile.\n",
    "3. Removing outliers: In some cases, removing extreme outliers might be appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_target_outliers(df, target_column, strategy='winsorize'):\n",
    "    if strategy == 'winsorize':\n",
    "        low = df[target_column].quantile(0.01)\n",
    "        high = df[target_column].quantile(0.99)\n",
    "        df[target_column] = df[target_column].clip(low, high)\n",
    "    elif strategy == 'remove':\n",
    "        Q1 = df[target_column].quantile(0.25)\n",
    "        Q3 = df[target_column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        df = df[(df[target_column] >= Q1 - 1.5*IQR) & (df[target_column] <= Q3 + 1.5*IQR)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage (if 'is_fan' were a continuous variable):\n",
    "# df = handle_target_outliers(df, 'is_fan', strategy='winsorize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stratified Sampling for Data Splits\n",
    "\n",
    "When your target variable is imbalanced, stratified sampling becomes crucial. This ensures that your train, validation, and test sets maintain the same proportion of classes as the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_imbalance(df, target_column, threshold=0.8):\n",
    "    distribution = df[target_column].value_counts(normalize=True)\n",
    "    if distribution.max() > threshold:\n",
    "        print(f\"Warning: The target variable is imbalanced. The majority class represents {distribution.max():.2%} of the data.\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "is_imbalanced = check_imbalance(df, 'is_fan')\n",
    "\n",
    "# If imbalanced, use stratified sampling\n",
    "if is_imbalanced:\n",
    "    X = df.drop('is_fan', axis=1)\n",
    "    y = df['is_fan']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('is_fan', axis=1), df['is_fan'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set class distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"\\nTest set class distribution:\\n{y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Data Preprocessing\n",
    "\n",
    "To avoid these issues and ensure robust models, follow these best practices:\n",
    "\n",
    "1. Split your data first: Always split your data into train, validation, and test sets before any preprocessing.\n",
    "2. Preprocess within cross-validation: Apply preprocessing steps only to the training data within each fold of cross-validation.\n",
    "3. Use pipelines: Scikit-learn's Pipeline class can help ensure that preprocessing steps are only applied to the training data.\n",
    "4. Preserve test set integrity: Never use information from the test set for preprocessing or model development.\n",
    "\n",
    "Here's an example of how to properly split the data and apply preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# First, split the data\n",
    "X = df.drop('is_fan', axis=1)\n",
    "y = df['is_fan']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a preprocessing and modeling pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Proper data splitting and careful preprocessing are crucial for developing robust and reliable machine learning models. By understanding the risks associated with premature preprocessing and following best practices, you can avoid data leakage, obtain realistic performance estimates, and build models that generalize well to new data.\n",
    "\n",
    "Remember, the goal is not just to have a model that performs well on your test set, but one that will perform well on truly unseen data in real-world applications. By maintaining the integrity of your data splitting process and applying preprocessing steps correctly, you'll be well on your way to mastering the art of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
