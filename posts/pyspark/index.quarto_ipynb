{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Why PySpark?\"\n",
        "author: \"Ravi Kalia\"\n",
        "date: \"2025-03-27\"\n",
        "categories: [Big Data, PySpark, Distributed Computing]\n",
        "tags: [PySpark, Spark, Data Engineering, Machine Learning]\n",
        "format: html\n",
        "---\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "PySpark brings the power of Apache Spark to Python, allowing data scientists and engineers to process **big data** efficiently. It is **fast**, **scalable**, and **fault-tolerant**, making it ideal for large-scale data processing, machine learning, and real-time analytics.\n",
        "\n",
        "In this post, we'll explore **why PySpark is great**, how it works under the hood, and include **code examples** to demonstrate its capabilities.\n",
        "\n",
        "Note, in many cases it's possible to do similar operations with Pandas, but PySpark is designed to handle **big data** that doesn't fit into memory on a single machine. \n",
        "\n",
        "Another possibility is to SQL cloud service such as Google BigQuery, AWS RedShift or Azure Synapse Analytics. However PySpark is more flexible and can be run on-premises or in the cloud in a bespoke and Pythonic way.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸš€ Why is PySpark So Great?\n",
        "\n",
        "### ðŸ”¥ **Scalability**\n",
        "PySpark distributes data across **multiple nodes**, allowing it to process terabytes or even petabytes efficiently.\n",
        "\n",
        "### âš¡ **Speed**\n",
        "- Uses **in-memory computation**, reducing disk I/O.\n",
        "- **Optimized DAG execution** minimizes redundant computations.\n",
        "- **Parallelism** speeds up operations across distributed clusters.\n",
        "\n",
        "### ðŸ **Pythonic & Flexible**\n",
        "- Works seamlessly with **Pandas**, **NumPy**, and **MLlib**.\n",
        "- Supports multiple data formats: **CSV, Parquet, JSON, Delta Lake, etc.**\n",
        "\n",
        "### ðŸ“Š **SQL + ML Support**\n",
        "- **Spark SQL** lets you query big data using SQL.\n",
        "- **MLlib** enables scalable machine learning.\n",
        "\n",
        "### â˜ï¸ **Cloud & On-Premise Compatibility**\n",
        "- Runs on **AWS, Azure, GCP, Kubernetes, and local clusters**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ” How Does PySpark Achieve This?\n",
        "\n",
        "### 1ï¸âƒ£ **Resilient Distributed Datasets (RDDs) â€“ The Core**\n",
        "PySparkâ€™s core data structure is the **RDD** (Resilient Distributed Dataset), which:\n",
        "- **Distributes** data across multiple nodes for parallelism.\n",
        "- **Supports fault tolerance** by tracking lineage.\n",
        "- **Uses lazy evaluation**, meaning computations are only executed when needed.\n",
        "\n",
        "#### Example: Creating an RDD\n"
      ],
      "id": "d7558510"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder.appName(\"RDD Example\").getOrCreate()\n",
        "\n",
        "# Create an RDD from a Python list\n",
        "data = [\"apple\", \"banana\", \"cherry\", \"date\"]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# Transform and collect results\n",
        "upper_rdd = rdd.map(lambda x: x.upper())\n",
        "print(upper_rdd.collect())"
      ],
      "id": "6e835b71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2ï¸âƒ£ **Directed Acyclic Graph (DAG) Execution**\n",
        "Instead of executing step-by-step like MapReduce, Spark builds a **DAG of transformations**, optimizing execution by:\n",
        "- Pipelining operations.\n",
        "- Reducing redundant computations.\n",
        "\n",
        "### 3ï¸âƒ£ **In-Memory Computation for Speed**\n",
        "Unlike Hadoop, which writes intermediate data to disk, Spark keeps it in **RAM** whenever possible.\n",
        "\n",
        "#### Example: DataFrame Operations\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(\"Alice\", 30), (\"Bob\", 25), (\"Charlie\", 35)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Perform a transformation\n",
        "df_filtered = df.filter(col(\"Age\") > 28)\n",
        "\n",
        "df_filtered.show()\n",
        "```\n",
        "\n",
        "### 4ï¸âƒ£ **Distributed Task Scheduling**\n",
        "- The **Driver** manages execution and breaks tasks into **stages**.\n",
        "- **Workers** execute tasks across a **cluster manager** (YARN, Kubernetes, Mesos, or Standalone).\n",
        "\n",
        "### 5ï¸âƒ£ **Fault Tolerance with Lineage**\n",
        "If a node fails, Spark **recomputes only the lost partitions** using the RDDâ€™s **lineage graph**, avoiding the need to restart the entire job.\n",
        "\n",
        "#### Example: Handling Fault Tolerance\n",
        "\n",
        "```python\n",
        "# Simulating failure recovery\n",
        "rdd_with_failure = rdd.map(lambda x: 1 / (len(x) - 5))  # Will cause division by zero\n",
        "try:\n",
        "    print(rdd_with_failure.collect())\n",
        "except Exception as e:\n",
        "    print(\"Error handled:\", e)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¥ PySpark in Action: End-to-End Example\n",
        "\n",
        "Hereâ€™s a real-world PySpark example where we:\n",
        "1. Read a CSV file.\n",
        "2. Perform transformations.\n",
        "3. Run SQL queries.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"ExampleApp\").getOrCreate()\n",
        "\n",
        "# Read CSV into DataFrame\n",
        "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Transform: Filter and Aggregate\n",
        "filtered_df = df.filter(col(\"age\") > 30)\n",
        "avg_salary = filtered_df.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg_salary\"))\n",
        "\n",
        "# Run SQL Query\n",
        "filtered_df.createOrReplaceTempView(\"employees\")\n",
        "sql_result = spark.sql(\n",
        "    \"SELECT department, AVG(salary) AS avg_salary FROM employees GROUP BY department\"\n",
        ")\n",
        "\n",
        "# Show results\n",
        "avg_salary.show()\n",
        "sql_result.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Conclusion\n",
        "PySpark is a **powerful, scalable, and fast** big data processing framework. It achieves this through:\n",
        "- **RDDs for distributed computing**.\n",
        "- **DAG execution for optimization**.\n",
        "- **In-memory computation for speed**.\n",
        "- **Fault tolerance via lineage tracking**.\n",
        "\n",
        "If you work with **big data**, PySpark is a game-changer! ðŸš€\n",
        "\n",
        "### ðŸ”— Further Reading\n",
        "- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
        "- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n"
      ],
      "id": "22e02f96"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/ravikalia/Code/local-playground/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}